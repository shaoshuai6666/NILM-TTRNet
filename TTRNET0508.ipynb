{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: torch in ./miniconda3/lib/python3.8/site-packages (1.9.0+cu111)\n",
      "Requirement already satisfied: torchvision in ./miniconda3/lib/python3.8/site-packages (0.10.0+cu111)\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in ./miniconda3/lib/python3.8/site-packages (1.1.1)\n",
      "Requirement already satisfied: matplotlib in ./miniconda3/lib/python3.8/site-packages (3.4.3)\n",
      "Requirement already satisfied: tables in ./miniconda3/lib/python3.8/site-packages (3.7.0)\n",
      "Requirement already satisfied: pyarrow in ./miniconda3/lib/python3.8/site-packages (8.0.0)\n",
      "Requirement already satisfied: typing-extensions in ./miniconda3/lib/python3.8/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in ./miniconda3/lib/python3.8/site-packages (from torchvision) (8.3.2)\n",
      "Requirement already satisfied: numpy in ./miniconda3/lib/python3.8/site-packages (from torchvision) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./miniconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./miniconda3/lib/python3.8/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./miniconda3/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: packaging in ./miniconda3/lib/python3.8/site-packages (from tables) (21.0)\n",
      "Requirement already satisfied: numexpr>=2.6.2 in ./miniconda3/lib/python3.8/site-packages (from tables) (2.8.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pandas scikit-learn matplotlib tables pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "from collections import defaultdict, OrderedDict\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, matthews_corrcoef\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPxd9q-PhAXB"
   },
   "source": [
    "# Build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9g6B69jPJkz"
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('combed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4y3vM6zQz5DC"
   },
   "outputs": [],
   "source": [
    "def resample_meter(store=None, building=1, meter=1, period='1min', cutoff=1000.):\n",
    "    key = '/building{}/elec/meter{}'.format(building,meter)\n",
    "    m = store[key]\n",
    "    v = m.iloc[:,2].values.flatten()\n",
    "    t = m.index\n",
    "    s = pd.Series(v, index=t).clip(0.,cutoff)\n",
    "    s[s<10.] = 0.\n",
    "    return s.resample('1s').ffill(limit=300).fillna(0.).resample(period).mean().tz_convert('UTC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_a-pqdClkRmO"
   },
   "outputs": [],
   "source": [
    "def get_series(datastore, house, label, cutoff):\n",
    "    filename = './house_%1d_labels.dat' %house\n",
    "    print(filename)\n",
    "    labels = pd.read_csv(filename, delimiter=' ', header=None, index_col=0).to_dict()[1]\n",
    "    \n",
    "    for i in labels:\n",
    "        if labels[i] == label:\n",
    "            print(i, labels[i])\n",
    "            s = resample_meter(store, house, i, '30s', cutoff)#修改采样频率\n",
    "            #s = resample_meter(store, house, i, '6s', cutoff)\n",
    "    \n",
    "    s.index.name = 'datetime'\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6lRVLSIzk6B5",
    "outputId": "bc980e76-1dca-4841-cddd-f10d6ee69d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "8 AHU0\n",
      "./house_1_labels.dat\n",
      "9 AHU1\n",
      "./house_1_labels.dat\n",
      "10 AHU2\n",
      "./house_1_labels.dat\n",
      "11 AHU5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117744/2710667190.py:16: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,6,28):]\n",
      "/tmp/ipykernel_117744/2710667190.py:16: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  ds_1_valid = ds_1[pd.datetime(2014,6,28):]\n"
     ]
    }
   ],
   "source": [
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 60000.)\n",
    "m.name = 'aggregate'\n",
    "a1 = get_series(store, house, 'AHU0', 5000.)\n",
    "a1.name = 'AHU0'\n",
    "a2 = get_series(store, house, 'AHU1', 4500.)\n",
    "a2.name = 'AHU1'\n",
    "a3 = get_series(store, house, 'AHU2', 4500.)\n",
    "a3.name = 'AHU2'\n",
    "a4 = get_series(store, house, 'AHU5', 12000.)\n",
    "a4.name = 'AHU5'\n",
    "ds_1 = pd.concat([m, a1, a2, a3, a4], axis=1)\n",
    "ds_1.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_1_train = ds_1.dropna(axis=0, how='any')#删除总表有缺失值的行\n",
    "ds_1_valid = ds_1[pd.datetime(2014,6,28):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "5_PW33ZLpwsK",
    "outputId": "776d64c0-fcfa-4637-f983-cf7207bd6ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "2 elevator\n",
      "./house_1_labels.dat\n",
      "8 AHU0\n",
      "./house_1_labels.dat\n",
      "9 AHU1\n",
      "./house_1_labels.dat\n",
      "10 AHU2\n",
      "./house_1_labels.dat\n",
      "11 AHU5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117744/1683166673.py:21: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_2_valid = ds_2[pd.datetime(2014,6,28):]\n",
      "/tmp/ipykernel_117744/1683166673.py:21: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  ds_2_valid = ds_2[pd.datetime(2014,6,28):]\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 60000.)\n",
    "m.name = 'aggregate'\n",
    "e1 = get_series(store, house, 'elevator', 60000.)\n",
    "e1.name = 'elevator'\n",
    "a1 = get_series(store, house, 'AHU0', 5000.)\n",
    "a1.name = 'AHU0'\n",
    "a2 = get_series(store, house, 'AHU1', 4500.)\n",
    "a2.name = 'AHU1'\n",
    "a3 = get_series(store, house, 'AHU2', 4500.)\n",
    "a3.name = 'AHU2'\n",
    "a4 = get_series(store, house, 'AHU5', 12000.)\n",
    "a4.name = 'AHU5'\n",
    "m = m-e1\n",
    "m.name = 'aggregate'\n",
    "ds_2 = pd.concat([m, a1, a2, a3, a4], axis=1)\n",
    "ds_2.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_2_train = ds_2.dropna(axis=0, how='any')#删除总表有缺失值的行\n",
    "ds_2_valid = ds_2[pd.datetime(2014,6,28):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WPRtFZqclByE",
    "outputId": "234de859-ec02-433f-8d75-86dbfe448a26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "12 light\n",
      "./house_1_labels.dat\n",
      "8 AHU0\n",
      "./house_1_labels.dat\n",
      "9 AHU1\n",
      "./house_1_labels.dat\n",
      "10 AHU2\n",
      "./house_1_labels.dat\n",
      "11 AHU5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117744/2391751185.py:21: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2014,6,28):]\n",
      "/tmp/ipykernel_117744/2391751185.py:21: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  ds_3_valid = ds_3[pd.datetime(2014,6,28):]\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 60000.)\n",
    "m.name = 'aggregate'\n",
    "e1 = get_series(store, house, 'light', 60000.)\n",
    "e1.name = 'light'\n",
    "a1 = get_series(store, house, 'AHU0', 5000.)\n",
    "a1.name = 'AHU0'\n",
    "a2 = get_series(store, house, 'AHU1', 4500.)\n",
    "a2.name = 'AHU1'\n",
    "a3 = get_series(store, house, 'AHU2', 4500.)\n",
    "a3.name = 'AHU2'\n",
    "a4 = get_series(store, house, 'AHU5', 12000.)\n",
    "a4.name = 'AHU5'\n",
    "m = m-e1\n",
    "m.name = 'aggregate'\n",
    "ds_3 = pd.concat([m, a1, a2, a3, a4], axis=1)\n",
    "ds_3.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_3_train = ds_3.dropna(axis=0, how='any')#删除总表有缺失值的行\n",
    "ds_3_valid = ds_3[pd.datetime(2014,6,28):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "q0xA9KgDqt1q",
    "outputId": "e5eb5263-7e1c-4fbf-c46c-3a2c3e55858a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "13 sockets1\n",
      "./house_1_labels.dat\n",
      "8 AHU0\n",
      "./house_1_labels.dat\n",
      "9 AHU1\n",
      "./house_1_labels.dat\n",
      "10 AHU2\n",
      "./house_1_labels.dat\n",
      "11 AHU5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117744/2551802685.py:21: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_4_valid = ds_4[pd.datetime(2014,6,28):]\n",
      "/tmp/ipykernel_117744/2551802685.py:21: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  ds_4_valid = ds_4[pd.datetime(2014,6,28):]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 60000.)\n",
    "m.name = 'aggregate'\n",
    "e1 = get_series(store, house, 'sockets1', 60000.)\n",
    "e1.name = 'sockets1'\n",
    "a1 = get_series(store, house, 'AHU0', 5000.)\n",
    "a1.name = 'AHU0'\n",
    "a2 = get_series(store, house, 'AHU1', 4500.)\n",
    "a2.name = 'AHU1'\n",
    "a3 = get_series(store, house, 'AHU2', 4500.)\n",
    "a3.name = 'AHU2'\n",
    "a4 = get_series(store, house, 'AHU5', 12000.)\n",
    "a4.name = 'AHU5'\n",
    "m = m-e1\n",
    "m.name = 'aggregate'\n",
    "ds_4 = pd.concat([m, a1, a2, a3, a4], axis=1)\n",
    "ds_4.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_4_train = ds_4.dropna(axis=0, how='any')#删除总表有缺失值的行\n",
    "ds_4_valid = ds_4[pd.datetime(2014,6,28):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "-G2EBJcWrM5V",
    "outputId": "088066b8-e349-475e-fdf0-0bd14abcc57c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./house_1_labels.dat\n",
      "1 aggregate\n",
      "./house_1_labels.dat\n",
      "14 sockets2\n",
      "./house_1_labels.dat\n",
      "8 AHU0\n",
      "./house_1_labels.dat\n",
      "9 AHU1\n",
      "./house_1_labels.dat\n",
      "10 AHU2\n",
      "./house_1_labels.dat\n",
      "11 AHU5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_117744/4060306444.py:21: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,6,28):]\n",
      "/tmp/ipykernel_117744/4060306444.py:21: FutureWarning: Indexing a timezone-aware DatetimeIndex with a timezone-naive datetime is deprecated and will raise KeyError in a future version. Use a timezone-aware object instead.\n",
      "  ds_5_valid = ds_5[pd.datetime(2014,6,28):]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "house = 1\n",
    "m = get_series(store, house, 'aggregate', 60000.)\n",
    "m.name = 'aggregate'\n",
    "e1 = get_series(store, house, 'sockets2', 60000.)\n",
    "e1.name = 'sockets2'\n",
    "a1 = get_series(store, house, 'AHU0', 5000.)\n",
    "a1.name = 'AHU0'\n",
    "a2 = get_series(store, house, 'AHU1', 4500.)\n",
    "a2.name = 'AHU1'\n",
    "a3 = get_series(store, house, 'AHU2', 4500.)\n",
    "a3.name = 'AHU2'\n",
    "a4 = get_series(store, house, 'AHU5', 12000.)\n",
    "a4.name = 'AHU5'\n",
    "m = m-e1\n",
    "m.name = 'aggregate'\n",
    "ds_5 = pd.concat([m, a1, a2, a3, a4], axis=1)\n",
    "ds_5.fillna(method='pad', inplace=True)\n",
    "\n",
    "ds_5_train = ds_5.dropna(axis=0, how='any')#删除总表有缺失值的行\n",
    "ds_5_valid = ds_5[pd.datetime(2014,6,28):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69C1JT0kdJ_W"
   },
   "outputs": [],
   "source": [
    "ds_1_train.reset_index().to_feather('./feather/UKDALE_1_train.feather')\n",
    "ds_2_train.reset_index().to_feather('./feather/UKDALE_2_train.feather')\n",
    "ds_3_train.reset_index().to_feather('./feather/UKDALE_3_train.feather')\n",
    "ds_4_train.reset_index().to_feather('./feather/UKDALE_4_train.feather')\n",
    "ds_5_train.reset_index().to_feather('./feather/UKDALE_5_train.feather')\n",
    "\n",
    "ds_1_valid.reset_index().to_feather('./feather/UKDALE_1_valid.feather')\n",
    "ds_2_valid.reset_index().to_feather('./feather/UKDALE_2_valid.feather')\n",
    "ds_3_valid.reset_index().to_feather('./feather/UKDALE_3_valid.feather')\n",
    "ds_4_valid.reset_index().to_feather('./feather/UKDALE_4_valid.feather')\n",
    "ds_5_valid.reset_index().to_feather('./feather/UKDALE_5_valid.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PvY0BjXDdKXR"
   },
   "source": [
    "# Read the feather dataframe resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5vzLJT4LwVl"
   },
   "outputs": [],
   "source": [
    "def get_status(app, threshold, min_off, min_on):\n",
    "    condition = app > threshold\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    on_events = idx[:,0].copy()\n",
    "    off_events = idx[:,1].copy()\n",
    "    assert len(on_events) == len(off_events)\n",
    "\n",
    "    if len(on_events) > 0:\n",
    "        off_duration = on_events[1:] - off_events[:-1]\n",
    "        off_duration = np.insert(off_duration, 0, 1000.)\n",
    "        on_events = on_events[off_duration > min_off]\n",
    "        off_events = off_events[np.roll(off_duration, -1) > min_off]\n",
    "        assert len(on_events) == len(off_events)\n",
    "\n",
    "        on_duration = off_events - on_events\n",
    "        on_events = on_events[on_duration > min_on]\n",
    "        off_events = off_events[on_duration > min_on]\n",
    "\n",
    "    s = app.copy()\n",
    "    #s.iloc[:] = 0.\n",
    "    s[:] = 0.\n",
    "\n",
    "    for on, off in zip(on_events, off_events):\n",
    "        #s.iloc[on:off] = 1.\n",
    "        s[on:off] = 1.\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iR9d91y1ktT"
   },
   "outputs": [],
   "source": [
    "class Power(data.Dataset):\n",
    "    def __init__(self, meter=None, appliance=None, status=None, \n",
    "                 length=256, border=680, max_power=1., train=False):\n",
    "        self.length = length\n",
    "        self.border = border\n",
    "        self.max_power = max_power\n",
    "        self.train = train\n",
    "\n",
    "        self.meter = meter.copy()/self.max_power\n",
    "        self.appliance = appliance.copy()/self.max_power\n",
    "        self.status = status.copy()\n",
    "\n",
    "        self.epochs = (len(self.meter) - 2*self.border) // self.length\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        i = index * self.length + self.border\n",
    "        if self.train:\n",
    "            i = np.random.randint(self.border, len(self.meter) - self.length - self.border)\n",
    "\n",
    "        x = self.meter.iloc[i-self.border:i+self.length+self.border].values.astype('float32')\n",
    "        y = self.appliance.iloc[i:i+self.length].values.astype('float32')\n",
    "        s = self.status.iloc[i:i+self.length].values.astype('float32')\n",
    "#         x -= x.mean()\n",
    "        \n",
    "        return x, y, s\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__>='1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, \n",
    "                                    kernel_size=3, padding=padding, padding_mode='circular')\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_in',nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4; hour_size = 24\n",
    "        weekday_size = 7; day_size = 32; month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type=='fixed' else nn.Embedding\n",
    "        if freq=='t':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        \n",
    "        minute_x = self.minute_embed(x[:,:,4]) if hasattr(self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:,:,3])\n",
    "        weekday_x = self.weekday_embed(x[:,:,2])\n",
    "        day_x = self.day_embed(x[:,:,1])\n",
    "        month_x = self.month_embed(x[:,:,0])\n",
    "        \n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h':4, 't':5, 's':6, 'm':1, 'a':1, 'w':2, 'd':3, 'b':3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type, freq=freq) if embed_type!='timeF' else TimeFeatureEmbedding(d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(2,1)\n",
    "        x = self.value_embedding(x) + self.position_embedding(x) \n",
    "        x = x.transpose(2,1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4, 480])\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from torch.autograd import Variable\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import math\n",
    "# import copy\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import sklearn\n",
    "# import sklearn.metrics as metrics\n",
    "# from skmultilearn import dataset\n",
    "\n",
    "\n",
    "######################## RethinkNet ########################\n",
    "\n",
    "def arch_001(input_size, output_size, dropout=0.25, activation=nn.Sigmoid, rnn_unit='lstm'):\n",
    "    embed_size = 128\n",
    "\n",
    "    input_layer = nn.Linear(input_size, embed_size)\n",
    "    input_size = embed_size\n",
    "\n",
    "    if rnn_unit == 'rnn':\n",
    "        rnn_unit = nn.RNN(input_size, embed_size, 1)  # , dropout = dropout )\n",
    "    elif rnn_unit == 'lstm':\n",
    "        rnn_unit = nn.LSTM(input_size, embed_size, 1)  # , dropout = dropout)\n",
    "    elif rnn_unit == 'gru':\n",
    "        rnn_unit = nn.GRU(input_size, embed_size, 1)  # , dropout = dropout)\n",
    "    else:\n",
    "        NotImplementedError()\n",
    "\n",
    "    RNN = rnn_unit\n",
    "\n",
    "    dec = nn.Sequential(\n",
    "        nn.Linear(embed_size, output_size),\n",
    "        #activation()\n",
    "    )\n",
    "    return input_layer, RNN, dec, embed_size\n",
    "\n",
    "\n",
    "class RethinkNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, architecture=\"arch_001\", rethink_time=2, rnn_unit='lstm',\n",
    "                 reweight='None', device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "        super(RethinkNet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.rnn_unit = rnn_unit\n",
    "        self.input_layer, self.rnn, self.dec, self.embed_size = globals()[architecture](self.input_size,\n",
    "                                                                                        self.output_size,\n",
    "                                                                                        rnn_unit=rnn_unit)\n",
    "        self.b = rethink_time + 1\n",
    "        self.reweight = reweight\n",
    "        self.device = device\n",
    "\n",
    "    def prep_Y(self, Y):\n",
    "        return torch.cat([Y for _ in range(self.b)], axis=0)\n",
    "\n",
    "    def prep_X(self, X):\n",
    "        return X.view(480, X.shape[1], -1)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(1, batch_size, self.embed_size).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')), torch.zeros(1, batch_size,\n",
    "                                                                                                 self.embed_size).to(device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        hist = [0 for _ in range(self.b)]\n",
    "\n",
    "        h_0, c_0 = self.init_hidden(X.shape[0])\n",
    "        hidden = (h_0, c_0)\n",
    "        if (self.rnn_unit == 'rnn'): hidden = h_0\n",
    "        \n",
    "        X_embed = self.input_layer(X)\n",
    "        X_embed = self.prep_X(X_embed)\n",
    "\n",
    "        for i in range(self.b):\n",
    "            embed, hidden = self.rnn(X_embed, hidden)\n",
    "            out = self.dec(torch.squeeze(embed))\n",
    "            \n",
    "            hist[i] = out\n",
    "\n",
    "        return hist\n",
    "\n",
    "    def predict(self, X):\n",
    "        hist = self.predict_proba(X)\n",
    "        hist = [(i > Variable(torch.Tensor([0.5]))) * 1 for i in hist]\n",
    "        return hist\n",
    "\n",
    "    def forward(self, X):\n",
    "        output = [0 for _ in range(self.b)]\n",
    "        \n",
    "        h_0, c_0 = self.init_hidden(X.shape[0])\n",
    "        hidden = (h_0, c_0)\n",
    "        if (self.rnn_unit == 'rnn'): hidden = h_0\n",
    "        X_embed = self.input_layer(X.permute(2, 0, 1))\n",
    "        X_embed = self.prep_X(X_embed)\n",
    "        \n",
    "\n",
    "        for i in range(self.b):\n",
    "            embed, hidden = self.rnn(X_embed, hidden)\n",
    "            out = self.dec(torch.squeeze(embed))\n",
    "            out =torch.reshape(out, (out.shape[0],-1,out.shape[-1]))\n",
    "            out=out.permute(1, 2, 0)\n",
    "            output[i] = out\n",
    "        output = torch.cat(output, axis=0)\n",
    "        # for prediction\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "                                      \n",
    "    model = RethinkNet(4, 4, rethink_time=2).cuda()\n",
    "    x = torch.randn(2,4,480).cuda()\n",
    "    print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mirkSD9a1qTQ",
    "outputId": "df7353f4-93f8-4583-da76-3d999bb2d248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 480])\n",
      "5725638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=3, padding=1, stride=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #return self.bn(F.relu(self.conv(x)))\n",
    "        return self.drop(self.bn(F.relu(self.conv(x))))\n",
    "\n",
    "class TemporalPooling(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2):\n",
    "        super(TemporalPooling, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pool = nn.AvgPool1d(kernel_size=self.kernel_size, stride=self.kernel_size)\n",
    "        self.conv = nn.Conv1d(in_features, out_features, kernel_size=1, padding=0)\n",
    "        #self.upsample = nn.Upsample( scale_factor=kernel_size, mode='linear', align_corners=True)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(F.relu(x))\n",
    "        #return self.upsample(x)\n",
    "        #return self.drop(self.upsample(x))\n",
    "        return self.drop(F.interpolate(x, scale_factor=self.kernel_size, mode='linear', align_corners=True))\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_features=3, out_features=1, kernel_size=2, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv = nn.ConvTranspose1d(in_features, out_features, kernel_size=kernel_size, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "\n",
    "class TTRNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
    "        super(TTRNet, self).__init__()\n",
    "        p = 2\n",
    "        k = 1\n",
    "        features = init_features\n",
    "        self.embed  = DataEmbedding(1,256)\n",
    "        self.encoder_layer1 = nn.TransformerEncoderLayer(d_model=256, nhead=8,batch_first=True)\n",
    "        self.encoder_layer2 = nn.TransformerEncoderLayer(d_model=256, nhead=8,batch_first=True)\n",
    "        self.encoder_layer3 = nn.TransformerEncoderLayer(d_model=256, nhead=8,batch_first=True)\n",
    "        self.encoder_layer4 = nn.TransformerEncoderLayer(d_model=256, nhead=8,batch_first=True)\n",
    "        self.tpool1 = TemporalPooling(features*8**k, features*2**k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features*8**k, features*2**k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features*8**k, features*2**k, kernel_size=20)\n",
    "        self.tpool4 = TemporalPooling(features*8**k, features*2**k, kernel_size=30)\n",
    "\n",
    "        self.encoder1 = Encoder(in_channels, features, kernel_size=3, padding=0)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder2 = Encoder(features * 1**k, features * 2**k, kernel_size=3, padding=0)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder3 = Encoder(features * 2**k, features * 4**k, kernel_size=3, padding=0)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=p, stride=p)\n",
    "        self.encoder4 = Encoder(features * 4**k, features * 8**k, kernel_size=3, padding=0)\n",
    "        \n",
    "        self.tpool1 = TemporalPooling(features*8**k, features*2**k, kernel_size=5)\n",
    "        self.tpool2 = TemporalPooling(features*8**k, features*2**k, kernel_size=10)\n",
    "        self.tpool3 = TemporalPooling(features*8**k, features*2**k, kernel_size=20)\n",
    "        self.tpool4 = TemporalPooling(features*8**k, features*2**k, kernel_size=30)\n",
    "\n",
    "        self.decoder = Decoder(2*features * 8**k, features * 1**k, kernel_size=p**3, stride=p**3)\n",
    "\n",
    "        self.activation = nn.Conv1d(features * 1**k, out_channels, kernel_size=1, padding=0)\n",
    "        self.re = RethinkNet(features * 1**k,out_channels,rethink_time=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.transpose(2,1)\n",
    "        enc1 = self.encoder_layer1(x)\n",
    "#         print(\"enc1.shape\",enc1.shape)\n",
    "        enc1 = enc1.transpose(2,1)\n",
    "        enc1 = self.pool1(enc1)\n",
    "#         print(\"enc1.shape\",enc1.shape)\n",
    "        \n",
    "        enc2 = enc1.transpose(2,1)\n",
    "        enc2 = self.encoder_layer2(enc2)\n",
    "#         print(\"enc2.shape\",enc2.shape)\n",
    "        enc2 = enc2.transpose(2,1)\n",
    "        enc2 = self.pool2(enc2)\n",
    "#         print(\"enc2.shape\",enc2.shape)\n",
    "        \n",
    "        enc3 = enc2.transpose(2,1)\n",
    "        enc3 = self.encoder_layer3(enc3)\n",
    "#         print(\"enc3.shape\",enc3.shape)\n",
    "        enc3 = enc3.transpose(2,1)\n",
    "        enc4 = self.pool3(enc3)\n",
    "#         print(\"enc4.shape\",enc4.shape)\n",
    "\n",
    "        tp1 = self.tpool1(enc4)\n",
    "        tp2 = self.tpool2(enc4)\n",
    "        tp3 = self.tpool3(enc4)\n",
    "        tp4 = self.tpool4(enc4)\n",
    "\n",
    "        dec = self.decoder(torch.cat([enc4, tp1, tp2, tp3, tp4], dim=1))\n",
    "\n",
    "#         act = self.activation(dec)\n",
    "        act = self.re(dec)\n",
    "        return act\n",
    "\n",
    "x = torch.randn(32,1,60*8).cuda()\n",
    "model = TTRNet(1,3,32).cuda()\n",
    "print(model(x).shape)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMsGSOln1wS3"
   },
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, filename):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the test loss as the model trains\n",
    "    test_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    # to track the average test loss per epoch as the model trains\n",
    "    avg_test_losses = [] \n",
    "    \n",
    "    min_loss = np.inf\n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    #patience = 10\n",
    "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target_power, target_status) in enumerate(train_loader, 1):\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            target_status=torch.cat([target_status for _ in range (4)], axis=0)\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in valid_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            target_status=torch.cat([target_status for _ in range (4)], axis=0)\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        ##################    \n",
    "        # test the model #\n",
    "        ##################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data, target_power, target_status in test_loader:\n",
    "            data = data.unsqueeze(1).cuda()\n",
    "            target_power = target_power.cuda()\n",
    "            target_status = target_status.cuda()\n",
    "            \n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output_status = model(data).permute(0,2,1)\n",
    "            # calculate the loss\n",
    "            target_status=torch.cat([target_status for _ in range (4)], axis=0)\n",
    "            loss = criterion(output_status, target_status)\n",
    "            # record validation loss\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        test_loss = np.average(test_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'test_loss: {test_loss:.5f} ')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        #early_stopping(valid_loss, model)\n",
    "        #if (early_stopping.early_stop and (epoch > 80)):\n",
    "        #    break\n",
    "        \n",
    "        if valid_loss < min_loss:\n",
    "            print(f'Validation loss decreased ({min_loss:.6f} --> {valid_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            min_loss = valid_loss\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    \n",
    "    return  model, avg_train_losses, avg_valid_losses, avg_test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pP9W8d_K12mm"
   },
   "outputs": [],
   "source": [
    "def evaluate_activation(model, loader, a):\n",
    "    x_true = []\n",
    "    s_true = []\n",
    "    p_true = []\n",
    "    s_hat = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, p, s in loader:\n",
    "            x = x.unsqueeze(1).cuda()\n",
    "            p = p.permute(0,2,1)[:,a,:]\n",
    "            s = s.permute(0,2,1)[:,a,:]\n",
    "            \n",
    "            sh = model(x)\n",
    "            _,sh = torch.split(sh,3,dim=0)\n",
    "            sh = torch.sigmoid(sh[:,a,:])\n",
    "            \n",
    "            s_hat.append(sh.contiguous().view(-1).detach().cpu().numpy())\n",
    "            \n",
    "            x_true.append(x[:,:,BORDER:-BORDER].contiguous().view(-1).detach().cpu().numpy())\n",
    "            s_true.append(s.contiguous().view(-1).detach().cpu().numpy())\n",
    "            p_true.append(p.contiguous().view(-1).detach().cpu().numpy())\n",
    "    x_true = np.hstack(x_true)\n",
    "    s_true = np.hstack(s_true)\n",
    "    p_true = np.hstack(p_true)\n",
    "    s_hat = np.hstack(s_hat)\n",
    "\n",
    "    return x_true, p_true, s_true, s_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OeLT8jHC1-mA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSypXakc14qb"
   },
   "outputs": [],
   "source": [
    "APPLIANCE = ['AHU0', 'AHU1', 'AHU2','AHU5']\n",
    "THRESHOLD = [500., 450., 450., 1200.]\n",
    "MIN_ON = [30, 30., 30., 30.]\n",
    "MIN_OFF = [30, 30.,30, 30]\n",
    "\n",
    "METER = 'aggregate'\n",
    "SEQ_LEN = 60*8\n",
    "BORDER = 0\n",
    "BATCH_SIZE = 32\n",
    "Epochs = 100\n",
    "jb=20#重复几遍\n",
    "MAX_POWER = 60000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mO2wSprQfV6x"
   },
   "outputs": [],
   "source": [
    "ds_meter = []\n",
    "ds_appliance = []\n",
    "ds_status = []\n",
    "for i in range(5):\n",
    "    ds = pd.read_feather('./feather/UKDALE_%d_train.feather' %(i+1))\n",
    "    ds.set_index('datetime', inplace=True)\n",
    "    \n",
    "    meter = ds[METER]\n",
    "    appliances = ds[APPLIANCE]\n",
    "    \n",
    "    status = pd.DataFrame()\n",
    "    for a in range(len(APPLIANCE)):\n",
    "        status = pd.concat([status, get_status(ds[APPLIANCE[a]], THRESHOLD[a], MIN_OFF[a], MIN_ON[a])], axis=1)\n",
    "    \n",
    "    ds_meter.append(meter)\n",
    "    ds_appliance.append(appliances)\n",
    "    ds_status.append(status)\n",
    "\n",
    "ds_len = [len(ds_meter[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "sGfWAsgWbVtt",
    "outputId": "2c141854-5a34-469f-e034-c5f535f83c52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AHU0    33\n",
       "AHU1    21\n",
       "AHU2    42\n",
       "AHU5    41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ds_status[1].diff()==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "Tq5YJh7lzqvS",
    "outputId": "494fd4da-e7ad-485e-9da1-4ffee3d198a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AHU0</th>\n",
       "      <th>AHU1</th>\n",
       "      <th>AHU2</th>\n",
       "      <th>AHU5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>83046.000000</td>\n",
       "      <td>83046.000000</td>\n",
       "      <td>83046.000000</td>\n",
       "      <td>83046.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.451774</td>\n",
       "      <td>0.404234</td>\n",
       "      <td>0.498627</td>\n",
       "      <td>0.494617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497672</td>\n",
       "      <td>0.490746</td>\n",
       "      <td>0.500001</td>\n",
       "      <td>0.499974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               AHU0          AHU1          AHU2          AHU5\n",
       "count  83046.000000  83046.000000  83046.000000  83046.000000\n",
       "mean       0.451774      0.404234      0.498627      0.494617\n",
       "std        0.497672      0.490746      0.500001      0.499974\n",
       "min        0.000000      0.000000      0.000000      0.000000\n",
       "25%        0.000000      0.000000      0.000000      0.000000\n",
       "50%        0.000000      0.000000      0.000000      0.000000\n",
       "75%        1.000000      1.000000      1.000000      1.000000\n",
       "max        1.000000      1.000000      1.000000      1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_status[1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iLZ7PmzG7jHS"
   },
   "outputs": [],
   "source": [
    "ds_house_train = [Power(ds_meter[i][:int(0.7*ds_len[i])], \n",
    "                        ds_appliance[i][:int(0.7*ds_len[i])], \n",
    "                        ds_status[i][:int(0.7*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, True) for i in range(5+0)]\n",
    "\n",
    "ds_house_valid = [Power(ds_meter[i][int(0.7*ds_len[i]):int(0.85*ds_len[i])], \n",
    "                        ds_appliance[i][int(0.7*ds_len[i]):int(0.85*ds_len[i])],\n",
    "                        ds_status[i][int(0.7*ds_len[i]):int(0.85*ds_len[i])], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_test  = [Power(ds_meter[i][int(0.85*ds_len[i]):], \n",
    "                        ds_appliance[i][int(0.85*ds_len[i]):],\n",
    "                        ds_status[i][int(0.85*ds_len[i]):], \n",
    "                        SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_house_total  = [Power(ds_meter[i], ds_appliance[i], ds_status[i], \n",
    "                         SEQ_LEN, BORDER, MAX_POWER, False) for i in range(5+0)]\n",
    "\n",
    "ds_train_seen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                ds_house_train[1], \n",
    "                                                ds_house_train[2], \n",
    "                                                ds_house_train[3],\n",
    "                                                ds_house_train[4]\n",
    "                                                ])\n",
    "ds_valid_seen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                #ds_house_valid[1], \n",
    "                                                #ds_house_valid[2], \n",
    "                                                #ds_house_valid[3], \n",
    "                                                #ds_house_valid[4]\n",
    "                                                ])\n",
    "\n",
    "dl_train_seen = DataLoader(dataset = ds_train_seen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_seen = DataLoader(dataset = ds_valid_seen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_seen = DataLoader(dataset = ds_house_test[0], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "ds_train_unseen = torch.utils.data.ConcatDataset([ds_house_train[0], \n",
    "                                                  #ds_house_train[1], \n",
    "                                                  #ds_house_train[2], \n",
    "                                                  #ds_house_train[3], \n",
    "                                                  ds_house_train[4]\n",
    "                                                  ])\n",
    "ds_valid_unseen = torch.utils.data.ConcatDataset([ds_house_valid[0], \n",
    "                                                  #ds_house_valid[1], \n",
    "                                                  #ds_house_valid[2], \n",
    "                                                  #ds_house_valid[3], \n",
    "                                                  ds_house_valid[4]\n",
    "                                                  ])\n",
    "dl_train_unseen = DataLoader(dataset = ds_train_unseen, batch_size = BATCH_SIZE, shuffle=True)\n",
    "dl_valid_unseen = DataLoader(dataset = ds_valid_unseen, batch_size = BATCH_SIZE, shuffle=False)\n",
    "dl_test_unseen = DataLoader(dataset = ds_house_total[1], batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "dl_house_test = [DataLoader(dataset = ds_house_test[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_valid = [DataLoader(dataset = ds_house_valid[i], batch_size = 1, shuffle=False) for i in range(5)]\n",
    "dl_house_total = [DataLoader(dataset = ds_house_total[i], batch_size = 1, shuffle=False) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC-IwqzoL2D5"
   },
   "outputs": [],
   "source": [
    "dataiter = iter(dl_house_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "colab_type": "code",
    "id": "rB6NlwrYYE3D",
    "outputId": "1a9d8d38-b2b9-45cd-f1fd-0b0604f464a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 1.5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "#x, y, s = dataiter.next()\n",
    "a = 1\n",
    "for i in range(100):\n",
    "    x, y, s = dataiter.next()\n",
    "    if y[0,:,a].sum() > 0:\n",
    "        break\n",
    "    if s[0,:,a].sum() > 0:\n",
    "        break\n",
    "plt.plot(np.arange(-BORDER, SEQ_LEN + BORDER), x[0,:].detach().numpy(), 'k-')\n",
    "plt.plot(y[0,:,a].detach().numpy())\n",
    "plt.plot(s[0,:,a].detach().numpy())\n",
    "plt.ylim([-0.5,1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, pos_weight, gamma):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.pos_weight  = pos_weight\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "    # y_pred is the logits without Sigmoid\n",
    "        assert y_pred.shape == y_true.shape\n",
    "        pt = torch.exp(-F.binary_cross_entropy_with_logits(y_pred, y_true, reduction='none')).detach().cuda()\n",
    "        sample_weight = (1 - pt) **  self.gamma\n",
    "        return F.binary_cross_entropy_with_logits(y_pred, y_true, weight= sample_weight.cuda(), pos_weight=self.pos_weight.cuda()).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zaxR7GIAnffI",
    "outputId": "5951ba49-d959-4cc3-d8af-edf6aea88a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/100] train_loss: 0.18462 valid_loss: 0.19294 test_loss: 0.18069 \n",
      "Validation loss decreased (inf --> 0.192941).  Saving model ...\n",
      "[  2/100] train_loss: 0.17950 valid_loss: 0.18674 test_loss: 0.17432 \n",
      "Validation loss decreased (0.192941 --> 0.186742).  Saving model ...\n",
      "[  3/100] train_loss: 0.17382 valid_loss: 0.17877 test_loss: 0.16605 \n",
      "Validation loss decreased (0.186742 --> 0.178774).  Saving model ...\n",
      "[  4/100] train_loss: 0.16628 valid_loss: 0.17004 test_loss: 0.15801 \n",
      "Validation loss decreased (0.178774 --> 0.170043).  Saving model ...\n",
      "[  5/100] train_loss: 0.15721 valid_loss: 0.16413 test_loss: 0.14916 \n",
      "Validation loss decreased (0.170043 --> 0.164128).  Saving model ...\n",
      "[  6/100] train_loss: 0.14933 valid_loss: 0.15228 test_loss: 0.13932 \n",
      "Validation loss decreased (0.164128 --> 0.152283).  Saving model ...\n",
      "[  7/100] train_loss: 0.13827 valid_loss: 0.14063 test_loss: 0.12729 \n",
      "Validation loss decreased (0.152283 --> 0.140628).  Saving model ...\n",
      "[  8/100] train_loss: 0.13007 valid_loss: 0.13694 test_loss: 0.11791 \n",
      "Validation loss decreased (0.140628 --> 0.136938).  Saving model ...\n",
      "[  9/100] train_loss: 0.12120 valid_loss: 0.11783 test_loss: 0.10622 \n",
      "Validation loss decreased (0.136938 --> 0.117831).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11668 valid_loss: 0.11797 test_loss: 0.09830 \n",
      "[ 11/100] train_loss: 0.10537 valid_loss: 0.10946 test_loss: 0.08929 \n",
      "Validation loss decreased (0.117831 --> 0.109458).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10086 valid_loss: 0.10345 test_loss: 0.08214 \n",
      "Validation loss decreased (0.109458 --> 0.103450).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09723 valid_loss: 0.09975 test_loss: 0.07632 \n",
      "Validation loss decreased (0.103450 --> 0.099748).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09282 valid_loss: 0.09326 test_loss: 0.07047 \n",
      "Validation loss decreased (0.099748 --> 0.093263).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08863 valid_loss: 0.09474 test_loss: 0.06667 \n",
      "[ 16/100] train_loss: 0.08540 valid_loss: 0.08700 test_loss: 0.06481 \n",
      "Validation loss decreased (0.093263 --> 0.086995).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08297 valid_loss: 0.09000 test_loss: 0.05875 \n",
      "[ 18/100] train_loss: 0.08009 valid_loss: 0.08756 test_loss: 0.05602 \n",
      "[ 19/100] train_loss: 0.08609 valid_loss: 0.08516 test_loss: 0.05734 \n",
      "Validation loss decreased (0.086995 --> 0.085160).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08042 valid_loss: 0.08885 test_loss: 0.05347 \n",
      "[ 21/100] train_loss: 0.07576 valid_loss: 0.08674 test_loss: 0.05139 \n",
      "[ 22/100] train_loss: 0.07843 valid_loss: 0.08598 test_loss: 0.05242 \n",
      "[ 23/100] train_loss: 0.07658 valid_loss: 0.08346 test_loss: 0.05020 \n",
      "Validation loss decreased (0.085160 --> 0.083456).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07642 valid_loss: 0.08468 test_loss: 0.04980 \n",
      "[ 25/100] train_loss: 0.07515 valid_loss: 0.08350 test_loss: 0.04771 \n",
      "[ 26/100] train_loss: 0.07972 valid_loss: 0.08357 test_loss: 0.04847 \n",
      "[ 27/100] train_loss: 0.07584 valid_loss: 0.08105 test_loss: 0.04928 \n",
      "Validation loss decreased (0.083456 --> 0.081047).  Saving model ...\n",
      "[ 28/100] train_loss: 0.07265 valid_loss: 0.08631 test_loss: 0.04677 \n",
      "[ 29/100] train_loss: 0.07221 valid_loss: 0.08223 test_loss: 0.04642 \n",
      "[ 30/100] train_loss: 0.07312 valid_loss: 0.08479 test_loss: 0.04716 \n",
      "[ 31/100] train_loss: 0.07008 valid_loss: 0.08442 test_loss: 0.04334 \n",
      "[ 32/100] train_loss: 0.07615 valid_loss: 0.09229 test_loss: 0.04592 \n",
      "[ 33/100] train_loss: 0.07469 valid_loss: 0.08574 test_loss: 0.05008 \n",
      "[ 34/100] train_loss: 0.06757 valid_loss: 0.07973 test_loss: 0.04234 \n",
      "Validation loss decreased (0.081047 --> 0.079731).  Saving model ...\n",
      "[ 35/100] train_loss: 0.07261 valid_loss: 0.08862 test_loss: 0.04834 \n",
      "[ 36/100] train_loss: 0.06799 valid_loss: 0.08289 test_loss: 0.04154 \n",
      "[ 37/100] train_loss: 0.06822 valid_loss: 0.08425 test_loss: 0.04400 \n",
      "[ 38/100] train_loss: 0.06740 valid_loss: 0.08059 test_loss: 0.04026 \n",
      "[ 39/100] train_loss: 0.07225 valid_loss: 0.08076 test_loss: 0.04398 \n",
      "[ 40/100] train_loss: 0.06905 valid_loss: 0.08291 test_loss: 0.04181 \n",
      "[ 41/100] train_loss: 0.06549 valid_loss: 0.08099 test_loss: 0.04036 \n",
      "[ 42/100] train_loss: 0.06840 valid_loss: 0.07936 test_loss: 0.04897 \n",
      "Validation loss decreased (0.079731 --> 0.079363).  Saving model ...\n",
      "[ 43/100] train_loss: 0.06580 valid_loss: 0.08742 test_loss: 0.04273 \n",
      "[ 44/100] train_loss: 0.06478 valid_loss: 0.08585 test_loss: 0.04236 \n",
      "[ 45/100] train_loss: 0.06492 valid_loss: 0.07965 test_loss: 0.04308 \n",
      "[ 46/100] train_loss: 0.06688 valid_loss: 0.08402 test_loss: 0.04084 \n",
      "[ 47/100] train_loss: 0.06382 valid_loss: 0.08502 test_loss: 0.04197 \n",
      "[ 48/100] train_loss: 0.06235 valid_loss: 0.09450 test_loss: 0.04461 \n",
      "[ 49/100] train_loss: 0.05998 valid_loss: 0.08094 test_loss: 0.03828 \n",
      "[ 50/100] train_loss: 0.06521 valid_loss: 0.08480 test_loss: 0.04401 \n",
      "[ 51/100] train_loss: 0.06643 valid_loss: 0.08718 test_loss: 0.04531 \n",
      "[ 52/100] train_loss: 0.06436 valid_loss: 0.09072 test_loss: 0.04635 \n",
      "[ 53/100] train_loss: 0.06410 valid_loss: 0.08266 test_loss: 0.03999 \n",
      "[ 54/100] train_loss: 0.06283 valid_loss: 0.08088 test_loss: 0.04049 \n",
      "[ 55/100] train_loss: 0.06044 valid_loss: 0.08224 test_loss: 0.04304 \n",
      "[ 56/100] train_loss: 0.06309 valid_loss: 0.09074 test_loss: 0.04459 \n",
      "[ 57/100] train_loss: 0.06055 valid_loss: 0.08588 test_loss: 0.04496 \n",
      "[ 58/100] train_loss: 0.06571 valid_loss: 0.07523 test_loss: 0.03826 \n",
      "Validation loss decreased (0.079363 --> 0.075228).  Saving model ...\n",
      "[ 59/100] train_loss: 0.06179 valid_loss: 0.09150 test_loss: 0.04783 \n",
      "[ 60/100] train_loss: 0.05883 valid_loss: 0.08637 test_loss: 0.03890 \n",
      "[ 61/100] train_loss: 0.06189 valid_loss: 0.08824 test_loss: 0.04371 \n",
      "[ 62/100] train_loss: 0.06094 valid_loss: 0.08622 test_loss: 0.04035 \n",
      "[ 63/100] train_loss: 0.06014 valid_loss: 0.08856 test_loss: 0.04592 \n",
      "[ 64/100] train_loss: 0.06126 valid_loss: 0.08276 test_loss: 0.03709 \n",
      "[ 65/100] train_loss: 0.06294 valid_loss: 0.08561 test_loss: 0.04168 \n",
      "[ 66/100] train_loss: 0.05807 valid_loss: 0.08051 test_loss: 0.04106 \n",
      "[ 67/100] train_loss: 0.05715 valid_loss: 0.08192 test_loss: 0.04404 \n",
      "[ 68/100] train_loss: 0.06142 valid_loss: 0.07914 test_loss: 0.03856 \n",
      "[ 69/100] train_loss: 0.05551 valid_loss: 0.07745 test_loss: 0.03730 \n",
      "[ 70/100] train_loss: 0.05974 valid_loss: 0.07891 test_loss: 0.04154 \n",
      "[ 71/100] train_loss: 0.06451 valid_loss: 0.08497 test_loss: 0.04188 \n",
      "[ 72/100] train_loss: 0.06127 valid_loss: 0.08135 test_loss: 0.04073 \n",
      "[ 73/100] train_loss: 0.05617 valid_loss: 0.07886 test_loss: 0.04022 \n",
      "[ 74/100] train_loss: 0.06035 valid_loss: 0.07771 test_loss: 0.03761 \n",
      "[ 75/100] train_loss: 0.06398 valid_loss: 0.08103 test_loss: 0.04057 \n",
      "[ 76/100] train_loss: 0.05659 valid_loss: 0.07989 test_loss: 0.03901 \n",
      "[ 77/100] train_loss: 0.05972 valid_loss: 0.08960 test_loss: 0.04426 \n",
      "[ 78/100] train_loss: 0.05859 valid_loss: 0.07995 test_loss: 0.03605 \n",
      "[ 79/100] train_loss: 0.05861 valid_loss: 0.07799 test_loss: 0.03944 \n",
      "[ 80/100] train_loss: 0.05894 valid_loss: 0.08926 test_loss: 0.04627 \n",
      "[ 81/100] train_loss: 0.05883 valid_loss: 0.07785 test_loss: 0.03680 \n",
      "[ 82/100] train_loss: 0.05483 valid_loss: 0.08220 test_loss: 0.03775 \n",
      "[ 83/100] train_loss: 0.06017 valid_loss: 0.07917 test_loss: 0.03689 \n",
      "[ 84/100] train_loss: 0.05334 valid_loss: 0.07224 test_loss: 0.03703 \n",
      "Validation loss decreased (0.075228 --> 0.072241).  Saving model ...\n",
      "[ 85/100] train_loss: 0.05791 valid_loss: 0.08579 test_loss: 0.04460 \n",
      "[ 86/100] train_loss: 0.05599 valid_loss: 0.07941 test_loss: 0.03815 \n",
      "[ 87/100] train_loss: 0.05059 valid_loss: 0.08339 test_loss: 0.03518 \n",
      "[ 88/100] train_loss: 0.05872 valid_loss: 0.07777 test_loss: 0.04337 \n",
      "[ 89/100] train_loss: 0.05817 valid_loss: 0.09222 test_loss: 0.04364 \n",
      "[ 90/100] train_loss: 0.05553 valid_loss: 0.07867 test_loss: 0.03745 \n",
      "[ 91/100] train_loss: 0.05570 valid_loss: 0.07987 test_loss: 0.03415 \n",
      "[ 92/100] train_loss: 0.05936 valid_loss: 0.09701 test_loss: 0.05102 \n",
      "[ 93/100] train_loss: 0.05478 valid_loss: 0.07740 test_loss: 0.03315 \n",
      "[ 94/100] train_loss: 0.05399 valid_loss: 0.08459 test_loss: 0.04261 \n",
      "[ 95/100] train_loss: 0.05871 valid_loss: 0.08480 test_loss: 0.04103 \n",
      "[ 96/100] train_loss: 0.05775 valid_loss: 0.07842 test_loss: 0.03854 \n",
      "[ 97/100] train_loss: 0.05789 valid_loss: 0.08272 test_loss: 0.03941 \n",
      "[ 98/100] train_loss: 0.05798 valid_loss: 0.07638 test_loss: 0.03824 \n",
      "[ 99/100] train_loss: 0.05346 valid_loss: 0.08350 test_loss: 0.03547 \n",
      "[100/100] train_loss: 0.05339 valid_loss: 0.07735 test_loss: 0.03981 \n",
      "TRAINING MODEL 1\n",
      "[  1/100] train_loss: 0.18346 valid_loss: 0.19617 test_loss: 0.17556 \n",
      "Validation loss decreased (inf --> 0.196170).  Saving model ...\n",
      "[  2/100] train_loss: 0.17861 valid_loss: 0.19161 test_loss: 0.16713 \n",
      "Validation loss decreased (0.196170 --> 0.191610).  Saving model ...\n",
      "[  3/100] train_loss: 0.17261 valid_loss: 0.18638 test_loss: 0.15734 \n",
      "Validation loss decreased (0.191610 --> 0.186380).  Saving model ...\n",
      "[  4/100] train_loss: 0.16794 valid_loss: 0.18095 test_loss: 0.14938 \n",
      "Validation loss decreased (0.186380 --> 0.180946).  Saving model ...\n",
      "[  5/100] train_loss: 0.15930 valid_loss: 0.17459 test_loss: 0.13996 \n",
      "Validation loss decreased (0.180946 --> 0.174586).  Saving model ...\n",
      "[  6/100] train_loss: 0.15253 valid_loss: 0.17025 test_loss: 0.13064 \n",
      "Validation loss decreased (0.174586 --> 0.170249).  Saving model ...\n",
      "[  7/100] train_loss: 0.14374 valid_loss: 0.16108 test_loss: 0.12208 \n",
      "Validation loss decreased (0.170249 --> 0.161082).  Saving model ...\n",
      "[  8/100] train_loss: 0.13469 valid_loss: 0.15269 test_loss: 0.11340 \n",
      "Validation loss decreased (0.161082 --> 0.152690).  Saving model ...\n",
      "[  9/100] train_loss: 0.12389 valid_loss: 0.14615 test_loss: 0.10391 \n",
      "Validation loss decreased (0.152690 --> 0.146148).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11865 valid_loss: 0.13018 test_loss: 0.09181 \n",
      "Validation loss decreased (0.146148 --> 0.130184).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10749 valid_loss: 0.12895 test_loss: 0.08551 \n",
      "Validation loss decreased (0.130184 --> 0.128953).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10959 valid_loss: 0.11851 test_loss: 0.07943 \n",
      "Validation loss decreased (0.128953 --> 0.118506).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10248 valid_loss: 0.11355 test_loss: 0.07448 \n",
      "Validation loss decreased (0.118506 --> 0.113551).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09536 valid_loss: 0.10619 test_loss: 0.06961 \n",
      "Validation loss decreased (0.113551 --> 0.106190).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09538 valid_loss: 0.10839 test_loss: 0.06662 \n",
      "[ 16/100] train_loss: 0.09104 valid_loss: 0.10397 test_loss: 0.06324 \n",
      "Validation loss decreased (0.106190 --> 0.103966).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08842 valid_loss: 0.10483 test_loss: 0.05993 \n",
      "[ 18/100] train_loss: 0.08527 valid_loss: 0.10240 test_loss: 0.05737 \n",
      "Validation loss decreased (0.103966 --> 0.102400).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08121 valid_loss: 0.09494 test_loss: 0.05606 \n",
      "Validation loss decreased (0.102400 --> 0.094941).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08339 valid_loss: 0.09440 test_loss: 0.05383 \n",
      "Validation loss decreased (0.094941 --> 0.094399).  Saving model ...\n",
      "[ 21/100] train_loss: 0.08416 valid_loss: 0.09815 test_loss: 0.05227 \n",
      "[ 22/100] train_loss: 0.08282 valid_loss: 0.09357 test_loss: 0.05367 \n",
      "Validation loss decreased (0.094399 --> 0.093567).  Saving model ...\n",
      "[ 23/100] train_loss: 0.07993 valid_loss: 0.09338 test_loss: 0.05120 \n",
      "Validation loss decreased (0.093567 --> 0.093381).  Saving model ...\n",
      "[ 24/100] train_loss: 0.08064 valid_loss: 0.09039 test_loss: 0.05101 \n",
      "Validation loss decreased (0.093381 --> 0.090386).  Saving model ...\n",
      "[ 25/100] train_loss: 0.08196 valid_loss: 0.09309 test_loss: 0.05060 \n",
      "[ 26/100] train_loss: 0.08055 valid_loss: 0.09421 test_loss: 0.04927 \n",
      "[ 27/100] train_loss: 0.07598 valid_loss: 0.08972 test_loss: 0.04872 \n",
      "Validation loss decreased (0.090386 --> 0.089721).  Saving model ...\n",
      "[ 28/100] train_loss: 0.07413 valid_loss: 0.09205 test_loss: 0.04624 \n",
      "[ 29/100] train_loss: 0.06877 valid_loss: 0.09911 test_loss: 0.04589 \n",
      "[ 30/100] train_loss: 0.07736 valid_loss: 0.08667 test_loss: 0.04802 \n",
      "Validation loss decreased (0.089721 --> 0.086670).  Saving model ...\n",
      "[ 31/100] train_loss: 0.07512 valid_loss: 0.08605 test_loss: 0.04626 \n",
      "Validation loss decreased (0.086670 --> 0.086048).  Saving model ...\n",
      "[ 32/100] train_loss: 0.07148 valid_loss: 0.09012 test_loss: 0.04242 \n",
      "[ 33/100] train_loss: 0.06849 valid_loss: 0.09353 test_loss: 0.04453 \n",
      "[ 34/100] train_loss: 0.06841 valid_loss: 0.08467 test_loss: 0.04095 \n",
      "Validation loss decreased (0.086048 --> 0.084674).  Saving model ...\n",
      "[ 35/100] train_loss: 0.07294 valid_loss: 0.08998 test_loss: 0.04261 \n",
      "[ 36/100] train_loss: 0.07179 valid_loss: 0.08962 test_loss: 0.04248 \n",
      "[ 37/100] train_loss: 0.07236 valid_loss: 0.08727 test_loss: 0.04246 \n",
      "[ 38/100] train_loss: 0.06600 valid_loss: 0.08516 test_loss: 0.03908 \n",
      "[ 39/100] train_loss: 0.07049 valid_loss: 0.08405 test_loss: 0.04004 \n",
      "Validation loss decreased (0.084674 --> 0.084048).  Saving model ...\n",
      "[ 40/100] train_loss: 0.07151 valid_loss: 0.08500 test_loss: 0.03980 \n",
      "[ 41/100] train_loss: 0.06463 valid_loss: 0.09243 test_loss: 0.04057 \n",
      "[ 42/100] train_loss: 0.06606 valid_loss: 0.09046 test_loss: 0.04054 \n",
      "[ 43/100] train_loss: 0.06673 valid_loss: 0.08304 test_loss: 0.03999 \n",
      "Validation loss decreased (0.084048 --> 0.083041).  Saving model ...\n",
      "[ 44/100] train_loss: 0.06614 valid_loss: 0.08600 test_loss: 0.03915 \n",
      "[ 45/100] train_loss: 0.06293 valid_loss: 0.09228 test_loss: 0.03801 \n",
      "[ 46/100] train_loss: 0.06514 valid_loss: 0.08974 test_loss: 0.04048 \n",
      "[ 47/100] train_loss: 0.06678 valid_loss: 0.09144 test_loss: 0.04079 \n",
      "[ 48/100] train_loss: 0.06652 valid_loss: 0.08576 test_loss: 0.03764 \n",
      "[ 49/100] train_loss: 0.06353 valid_loss: 0.08951 test_loss: 0.03830 \n",
      "[ 50/100] train_loss: 0.06239 valid_loss: 0.08240 test_loss: 0.03613 \n",
      "Validation loss decreased (0.083041 --> 0.082404).  Saving model ...\n",
      "[ 51/100] train_loss: 0.06285 valid_loss: 0.10125 test_loss: 0.04066 \n",
      "[ 52/100] train_loss: 0.06301 valid_loss: 0.08670 test_loss: 0.03654 \n",
      "[ 53/100] train_loss: 0.06363 valid_loss: 0.09047 test_loss: 0.03658 \n",
      "[ 54/100] train_loss: 0.06193 valid_loss: 0.08925 test_loss: 0.03669 \n",
      "[ 55/100] train_loss: 0.06290 valid_loss: 0.08282 test_loss: 0.03758 \n",
      "[ 56/100] train_loss: 0.05933 valid_loss: 0.09369 test_loss: 0.03645 \n",
      "[ 57/100] train_loss: 0.06321 valid_loss: 0.08791 test_loss: 0.03754 \n",
      "[ 58/100] train_loss: 0.06176 valid_loss: 0.09098 test_loss: 0.03757 \n",
      "[ 59/100] train_loss: 0.06348 valid_loss: 0.09953 test_loss: 0.04104 \n",
      "[ 60/100] train_loss: 0.06028 valid_loss: 0.08378 test_loss: 0.03473 \n",
      "[ 61/100] train_loss: 0.06271 valid_loss: 0.09070 test_loss: 0.03751 \n",
      "[ 62/100] train_loss: 0.06236 valid_loss: 0.09877 test_loss: 0.04123 \n",
      "[ 63/100] train_loss: 0.06039 valid_loss: 0.08594 test_loss: 0.03452 \n",
      "[ 64/100] train_loss: 0.05949 valid_loss: 0.08375 test_loss: 0.03396 \n",
      "[ 65/100] train_loss: 0.06274 valid_loss: 0.09532 test_loss: 0.04273 \n",
      "[ 66/100] train_loss: 0.06482 valid_loss: 0.08814 test_loss: 0.03452 \n",
      "[ 67/100] train_loss: 0.05651 valid_loss: 0.09024 test_loss: 0.03407 \n",
      "[ 68/100] train_loss: 0.05960 valid_loss: 0.10496 test_loss: 0.04761 \n",
      "[ 69/100] train_loss: 0.05892 valid_loss: 0.08477 test_loss: 0.03296 \n",
      "[ 70/100] train_loss: 0.06047 valid_loss: 0.09368 test_loss: 0.03803 \n",
      "[ 71/100] train_loss: 0.05821 valid_loss: 0.09373 test_loss: 0.03871 \n",
      "[ 72/100] train_loss: 0.05753 valid_loss: 0.08614 test_loss: 0.03715 \n",
      "[ 73/100] train_loss: 0.06104 valid_loss: 0.09687 test_loss: 0.04129 \n",
      "[ 74/100] train_loss: 0.05680 valid_loss: 0.10062 test_loss: 0.04131 \n",
      "[ 75/100] train_loss: 0.06313 valid_loss: 0.08726 test_loss: 0.03554 \n",
      "[ 76/100] train_loss: 0.05991 valid_loss: 0.08575 test_loss: 0.03401 \n",
      "[ 77/100] train_loss: 0.05949 valid_loss: 0.09331 test_loss: 0.03849 \n",
      "[ 78/100] train_loss: 0.05788 valid_loss: 0.09060 test_loss: 0.03473 \n",
      "[ 79/100] train_loss: 0.06198 valid_loss: 0.08304 test_loss: 0.03556 \n",
      "[ 80/100] train_loss: 0.05405 valid_loss: 0.09140 test_loss: 0.03279 \n",
      "[ 81/100] train_loss: 0.05732 valid_loss: 0.09131 test_loss: 0.03872 \n",
      "[ 82/100] train_loss: 0.06213 valid_loss: 0.09186 test_loss: 0.03797 \n",
      "[ 83/100] train_loss: 0.05979 valid_loss: 0.08660 test_loss: 0.03341 \n",
      "[ 84/100] train_loss: 0.05804 valid_loss: 0.08241 test_loss: 0.03416 \n",
      "[ 85/100] train_loss: 0.05887 valid_loss: 0.09902 test_loss: 0.03964 \n",
      "[ 86/100] train_loss: 0.05485 valid_loss: 0.09318 test_loss: 0.03877 \n",
      "[ 87/100] train_loss: 0.05893 valid_loss: 0.09856 test_loss: 0.04073 \n",
      "[ 88/100] train_loss: 0.05807 valid_loss: 0.08428 test_loss: 0.03202 \n",
      "[ 89/100] train_loss: 0.05883 valid_loss: 0.08299 test_loss: 0.03314 \n",
      "[ 90/100] train_loss: 0.05587 valid_loss: 0.08952 test_loss: 0.03461 \n",
      "[ 91/100] train_loss: 0.05275 valid_loss: 0.10401 test_loss: 0.03917 \n",
      "[ 92/100] train_loss: 0.06017 valid_loss: 0.09514 test_loss: 0.04158 \n",
      "[ 93/100] train_loss: 0.05864 valid_loss: 0.10221 test_loss: 0.03940 \n",
      "[ 94/100] train_loss: 0.05916 valid_loss: 0.09311 test_loss: 0.03818 \n",
      "[ 95/100] train_loss: 0.05579 valid_loss: 0.09064 test_loss: 0.03785 \n",
      "[ 96/100] train_loss: 0.05677 valid_loss: 0.09651 test_loss: 0.03660 \n",
      "[ 97/100] train_loss: 0.05865 valid_loss: 0.08707 test_loss: 0.03400 \n",
      "[ 98/100] train_loss: 0.05365 valid_loss: 0.08690 test_loss: 0.03165 \n",
      "[ 99/100] train_loss: 0.05615 valid_loss: 0.09028 test_loss: 0.03362 \n",
      "[100/100] train_loss: 0.05458 valid_loss: 0.09619 test_loss: 0.03394 \n",
      "TRAINING MODEL 2\n",
      "[  1/100] train_loss: 0.18551 valid_loss: 0.19229 test_loss: 0.18032 \n",
      "Validation loss decreased (inf --> 0.192291).  Saving model ...\n",
      "[  2/100] train_loss: 0.18132 valid_loss: 0.18776 test_loss: 0.17554 \n",
      "Validation loss decreased (0.192291 --> 0.187762).  Saving model ...\n",
      "[  3/100] train_loss: 0.17706 valid_loss: 0.18209 test_loss: 0.16772 \n",
      "Validation loss decreased (0.187762 --> 0.182091).  Saving model ...\n",
      "[  4/100] train_loss: 0.17067 valid_loss: 0.17494 test_loss: 0.15977 \n",
      "Validation loss decreased (0.182091 --> 0.174938).  Saving model ...\n",
      "[  5/100] train_loss: 0.16488 valid_loss: 0.16740 test_loss: 0.15164 \n",
      "Validation loss decreased (0.174938 --> 0.167405).  Saving model ...\n",
      "[  6/100] train_loss: 0.15388 valid_loss: 0.15882 test_loss: 0.14150 \n",
      "Validation loss decreased (0.167405 --> 0.158823).  Saving model ...\n",
      "[  7/100] train_loss: 0.14646 valid_loss: 0.15202 test_loss: 0.13211 \n",
      "Validation loss decreased (0.158823 --> 0.152024).  Saving model ...\n",
      "[  8/100] train_loss: 0.13502 valid_loss: 0.13830 test_loss: 0.11819 \n",
      "Validation loss decreased (0.152024 --> 0.138299).  Saving model ...\n",
      "[  9/100] train_loss: 0.12646 valid_loss: 0.12927 test_loss: 0.10921 \n",
      "Validation loss decreased (0.138299 --> 0.129271).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11800 valid_loss: 0.11973 test_loss: 0.09889 \n",
      "Validation loss decreased (0.129271 --> 0.119726).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11386 valid_loss: 0.11501 test_loss: 0.08874 \n",
      "Validation loss decreased (0.119726 --> 0.115014).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10462 valid_loss: 0.10636 test_loss: 0.08272 \n",
      "Validation loss decreased (0.115014 --> 0.106362).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09904 valid_loss: 0.10056 test_loss: 0.07779 \n",
      "Validation loss decreased (0.106362 --> 0.100564).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09460 valid_loss: 0.09870 test_loss: 0.07067 \n",
      "Validation loss decreased (0.100564 --> 0.098704).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09328 valid_loss: 0.09618 test_loss: 0.06799 \n",
      "Validation loss decreased (0.098704 --> 0.096184).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08408 valid_loss: 0.09415 test_loss: 0.06291 \n",
      "Validation loss decreased (0.096184 --> 0.094149).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08497 valid_loss: 0.09554 test_loss: 0.05991 \n",
      "[ 18/100] train_loss: 0.07893 valid_loss: 0.08724 test_loss: 0.05792 \n",
      "Validation loss decreased (0.094149 --> 0.087237).  Saving model ...\n",
      "[ 19/100] train_loss: 0.07909 valid_loss: 0.08682 test_loss: 0.05803 \n",
      "Validation loss decreased (0.087237 --> 0.086821).  Saving model ...\n",
      "[ 20/100] train_loss: 0.07948 valid_loss: 0.09535 test_loss: 0.05398 \n",
      "[ 21/100] train_loss: 0.07936 valid_loss: 0.09224 test_loss: 0.05165 \n",
      "[ 22/100] train_loss: 0.07926 valid_loss: 0.08309 test_loss: 0.05234 \n",
      "Validation loss decreased (0.086821 --> 0.083087).  Saving model ...\n",
      "[ 23/100] train_loss: 0.07949 valid_loss: 0.08313 test_loss: 0.05376 \n",
      "[ 24/100] train_loss: 0.07782 valid_loss: 0.08979 test_loss: 0.04931 \n",
      "[ 25/100] train_loss: 0.07528 valid_loss: 0.08899 test_loss: 0.04807 \n",
      "[ 26/100] train_loss: 0.07720 valid_loss: 0.08302 test_loss: 0.04939 \n",
      "Validation loss decreased (0.083087 --> 0.083017).  Saving model ...\n",
      "[ 27/100] train_loss: 0.07083 valid_loss: 0.08713 test_loss: 0.04678 \n",
      "[ 28/100] train_loss: 0.07189 valid_loss: 0.08323 test_loss: 0.04532 \n",
      "[ 29/100] train_loss: 0.07127 valid_loss: 0.08648 test_loss: 0.04620 \n",
      "[ 30/100] train_loss: 0.07533 valid_loss: 0.08334 test_loss: 0.04940 \n",
      "[ 31/100] train_loss: 0.07007 valid_loss: 0.08546 test_loss: 0.04323 \n",
      "[ 32/100] train_loss: 0.06650 valid_loss: 0.08982 test_loss: 0.04406 \n",
      "[ 33/100] train_loss: 0.07390 valid_loss: 0.08316 test_loss: 0.04701 \n",
      "[ 34/100] train_loss: 0.07561 valid_loss: 0.07926 test_loss: 0.04413 \n",
      "Validation loss decreased (0.083017 --> 0.079260).  Saving model ...\n",
      "[ 35/100] train_loss: 0.07495 valid_loss: 0.09193 test_loss: 0.04541 \n",
      "[ 36/100] train_loss: 0.07453 valid_loss: 0.08129 test_loss: 0.04247 \n",
      "[ 37/100] train_loss: 0.06752 valid_loss: 0.08090 test_loss: 0.04244 \n",
      "[ 38/100] train_loss: 0.07301 valid_loss: 0.08670 test_loss: 0.04548 \n",
      "[ 39/100] train_loss: 0.06842 valid_loss: 0.08573 test_loss: 0.04120 \n",
      "[ 40/100] train_loss: 0.06881 valid_loss: 0.08943 test_loss: 0.04260 \n",
      "[ 41/100] train_loss: 0.06649 valid_loss: 0.08364 test_loss: 0.04120 \n",
      "[ 42/100] train_loss: 0.06788 valid_loss: 0.08712 test_loss: 0.04245 \n",
      "[ 43/100] train_loss: 0.06566 valid_loss: 0.08868 test_loss: 0.04145 \n",
      "[ 44/100] train_loss: 0.06373 valid_loss: 0.08045 test_loss: 0.03916 \n",
      "[ 45/100] train_loss: 0.06411 valid_loss: 0.08076 test_loss: 0.04102 \n",
      "[ 46/100] train_loss: 0.06736 valid_loss: 0.07840 test_loss: 0.04053 \n",
      "Validation loss decreased (0.079260 --> 0.078402).  Saving model ...\n",
      "[ 47/100] train_loss: 0.06956 valid_loss: 0.08930 test_loss: 0.04468 \n",
      "[ 48/100] train_loss: 0.06638 valid_loss: 0.07916 test_loss: 0.04053 \n",
      "[ 49/100] train_loss: 0.06360 valid_loss: 0.08097 test_loss: 0.03806 \n",
      "[ 50/100] train_loss: 0.06428 valid_loss: 0.08990 test_loss: 0.04338 \n",
      "[ 51/100] train_loss: 0.06286 valid_loss: 0.07946 test_loss: 0.03820 \n",
      "[ 52/100] train_loss: 0.06355 valid_loss: 0.08604 test_loss: 0.03841 \n",
      "[ 53/100] train_loss: 0.06410 valid_loss: 0.08409 test_loss: 0.04079 \n",
      "[ 54/100] train_loss: 0.06605 valid_loss: 0.07931 test_loss: 0.03831 \n",
      "[ 55/100] train_loss: 0.06503 valid_loss: 0.08365 test_loss: 0.04215 \n",
      "[ 56/100] train_loss: 0.06354 valid_loss: 0.08767 test_loss: 0.04137 \n",
      "[ 57/100] train_loss: 0.06620 valid_loss: 0.08448 test_loss: 0.04147 \n",
      "[ 58/100] train_loss: 0.06407 valid_loss: 0.08308 test_loss: 0.03604 \n",
      "[ 59/100] train_loss: 0.05981 valid_loss: 0.08215 test_loss: 0.03852 \n",
      "[ 60/100] train_loss: 0.06072 valid_loss: 0.08339 test_loss: 0.03807 \n",
      "[ 61/100] train_loss: 0.06092 valid_loss: 0.08693 test_loss: 0.03833 \n",
      "[ 62/100] train_loss: 0.05607 valid_loss: 0.08546 test_loss: 0.03905 \n",
      "[ 63/100] train_loss: 0.06338 valid_loss: 0.08959 test_loss: 0.03850 \n",
      "[ 64/100] train_loss: 0.06000 valid_loss: 0.08337 test_loss: 0.03773 \n",
      "[ 65/100] train_loss: 0.06329 valid_loss: 0.08430 test_loss: 0.04086 \n",
      "[ 66/100] train_loss: 0.06406 valid_loss: 0.09292 test_loss: 0.03806 \n",
      "[ 67/100] train_loss: 0.06299 valid_loss: 0.08327 test_loss: 0.04233 \n",
      "[ 68/100] train_loss: 0.06038 valid_loss: 0.07574 test_loss: 0.03346 \n",
      "Validation loss decreased (0.078402 --> 0.075739).  Saving model ...\n",
      "[ 69/100] train_loss: 0.06083 valid_loss: 0.08949 test_loss: 0.04312 \n",
      "[ 70/100] train_loss: 0.06004 valid_loss: 0.07903 test_loss: 0.03829 \n",
      "[ 71/100] train_loss: 0.05679 valid_loss: 0.08527 test_loss: 0.03483 \n",
      "[ 72/100] train_loss: 0.05959 valid_loss: 0.07771 test_loss: 0.03808 \n",
      "[ 73/100] train_loss: 0.05468 valid_loss: 0.08657 test_loss: 0.03569 \n",
      "[ 74/100] train_loss: 0.05812 valid_loss: 0.08588 test_loss: 0.03962 \n",
      "[ 75/100] train_loss: 0.05645 valid_loss: 0.07925 test_loss: 0.03413 \n",
      "[ 76/100] train_loss: 0.05701 valid_loss: 0.07942 test_loss: 0.03600 \n",
      "[ 77/100] train_loss: 0.06000 valid_loss: 0.08156 test_loss: 0.03498 \n",
      "[ 78/100] train_loss: 0.05607 valid_loss: 0.07987 test_loss: 0.03500 \n",
      "[ 79/100] train_loss: 0.05835 valid_loss: 0.08360 test_loss: 0.03576 \n",
      "[ 80/100] train_loss: 0.05821 valid_loss: 0.08376 test_loss: 0.03693 \n",
      "[ 81/100] train_loss: 0.05698 valid_loss: 0.08026 test_loss: 0.03815 \n",
      "[ 82/100] train_loss: 0.05339 valid_loss: 0.09998 test_loss: 0.04212 \n",
      "[ 83/100] train_loss: 0.06107 valid_loss: 0.07665 test_loss: 0.03337 \n",
      "[ 84/100] train_loss: 0.05894 valid_loss: 0.08190 test_loss: 0.04195 \n",
      "[ 85/100] train_loss: 0.05357 valid_loss: 0.08873 test_loss: 0.03606 \n",
      "[ 86/100] train_loss: 0.05615 valid_loss: 0.07534 test_loss: 0.03633 \n",
      "Validation loss decreased (0.075739 --> 0.075341).  Saving model ...\n",
      "[ 87/100] train_loss: 0.05724 valid_loss: 0.07276 test_loss: 0.03318 \n",
      "Validation loss decreased (0.075341 --> 0.072761).  Saving model ...\n",
      "[ 88/100] train_loss: 0.05570 valid_loss: 0.08960 test_loss: 0.03565 \n",
      "[ 89/100] train_loss: 0.05243 valid_loss: 0.08360 test_loss: 0.03819 \n",
      "[ 90/100] train_loss: 0.05936 valid_loss: 0.08112 test_loss: 0.03479 \n",
      "[ 91/100] train_loss: 0.05858 valid_loss: 0.07915 test_loss: 0.03893 \n",
      "[ 92/100] train_loss: 0.05377 valid_loss: 0.08681 test_loss: 0.03592 \n",
      "[ 93/100] train_loss: 0.05416 valid_loss: 0.07656 test_loss: 0.03439 \n",
      "[ 94/100] train_loss: 0.05512 valid_loss: 0.07195 test_loss: 0.03185 \n",
      "Validation loss decreased (0.072761 --> 0.071945).  Saving model ...\n",
      "[ 95/100] train_loss: 0.05825 valid_loss: 0.08046 test_loss: 0.03861 \n",
      "[ 96/100] train_loss: 0.05648 valid_loss: 0.08106 test_loss: 0.03631 \n",
      "[ 97/100] train_loss: 0.05578 valid_loss: 0.07942 test_loss: 0.03653 \n",
      "[ 98/100] train_loss: 0.05595 valid_loss: 0.07730 test_loss: 0.03471 \n",
      "[ 99/100] train_loss: 0.05163 valid_loss: 0.07910 test_loss: 0.03074 \n",
      "[100/100] train_loss: 0.05506 valid_loss: 0.07887 test_loss: 0.04141 \n",
      "TRAINING MODEL 3\n",
      "[  1/100] train_loss: 0.18477 valid_loss: 0.18479 test_loss: 0.18335 \n",
      "Validation loss decreased (inf --> 0.184791).  Saving model ...\n",
      "[  2/100] train_loss: 0.17850 valid_loss: 0.17890 test_loss: 0.17493 \n",
      "Validation loss decreased (0.184791 --> 0.178903).  Saving model ...\n",
      "[  3/100] train_loss: 0.17197 valid_loss: 0.17234 test_loss: 0.16470 \n",
      "Validation loss decreased (0.178903 --> 0.172338).  Saving model ...\n",
      "[  4/100] train_loss: 0.16538 valid_loss: 0.16608 test_loss: 0.15537 \n",
      "Validation loss decreased (0.172338 --> 0.166076).  Saving model ...\n",
      "[  5/100] train_loss: 0.15814 valid_loss: 0.15924 test_loss: 0.14706 \n",
      "Validation loss decreased (0.166076 --> 0.159239).  Saving model ...\n",
      "[  6/100] train_loss: 0.15048 valid_loss: 0.15135 test_loss: 0.13709 \n",
      "Validation loss decreased (0.159239 --> 0.151348).  Saving model ...\n",
      "[  7/100] train_loss: 0.13987 valid_loss: 0.14442 test_loss: 0.12754 \n",
      "Validation loss decreased (0.151348 --> 0.144419).  Saving model ...\n",
      "[  8/100] train_loss: 0.13141 valid_loss: 0.13664 test_loss: 0.11760 \n",
      "Validation loss decreased (0.144419 --> 0.136645).  Saving model ...\n",
      "[  9/100] train_loss: 0.12492 valid_loss: 0.12839 test_loss: 0.10936 \n",
      "Validation loss decreased (0.136645 --> 0.128386).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11711 valid_loss: 0.12167 test_loss: 0.10232 \n",
      "Validation loss decreased (0.128386 --> 0.121669).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11244 valid_loss: 0.11204 test_loss: 0.09350 \n",
      "Validation loss decreased (0.121669 --> 0.112041).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10638 valid_loss: 0.10589 test_loss: 0.08427 \n",
      "Validation loss decreased (0.112041 --> 0.105886).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10306 valid_loss: 0.10574 test_loss: 0.07862 \n",
      "Validation loss decreased (0.105886 --> 0.105743).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09866 valid_loss: 0.09992 test_loss: 0.07415 \n",
      "Validation loss decreased (0.105743 --> 0.099922).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09161 valid_loss: 0.09793 test_loss: 0.06944 \n",
      "Validation loss decreased (0.099922 --> 0.097931).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08957 valid_loss: 0.09233 test_loss: 0.06727 \n",
      "Validation loss decreased (0.097931 --> 0.092333).  Saving model ...\n",
      "[ 17/100] train_loss: 0.09159 valid_loss: 0.09534 test_loss: 0.06443 \n",
      "[ 18/100] train_loss: 0.08947 valid_loss: 0.09279 test_loss: 0.06191 \n",
      "[ 19/100] train_loss: 0.08443 valid_loss: 0.09043 test_loss: 0.05963 \n",
      "Validation loss decreased (0.092333 --> 0.090432).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08624 valid_loss: 0.09209 test_loss: 0.05816 \n",
      "[ 21/100] train_loss: 0.08192 valid_loss: 0.09133 test_loss: 0.05585 \n",
      "[ 22/100] train_loss: 0.07703 valid_loss: 0.08661 test_loss: 0.05335 \n",
      "Validation loss decreased (0.090432 --> 0.086612).  Saving model ...\n",
      "[ 23/100] train_loss: 0.07850 valid_loss: 0.08466 test_loss: 0.05440 \n",
      "Validation loss decreased (0.086612 --> 0.084664).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07861 valid_loss: 0.09155 test_loss: 0.05188 \n",
      "[ 25/100] train_loss: 0.07390 valid_loss: 0.08624 test_loss: 0.05017 \n",
      "[ 26/100] train_loss: 0.07879 valid_loss: 0.08440 test_loss: 0.05298 \n",
      "Validation loss decreased (0.084664 --> 0.084405).  Saving model ...\n",
      "[ 27/100] train_loss: 0.07396 valid_loss: 0.09114 test_loss: 0.04886 \n",
      "[ 28/100] train_loss: 0.07747 valid_loss: 0.08529 test_loss: 0.04875 \n",
      "[ 29/100] train_loss: 0.07114 valid_loss: 0.08295 test_loss: 0.04743 \n",
      "Validation loss decreased (0.084405 --> 0.082951).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07077 valid_loss: 0.08604 test_loss: 0.04666 \n",
      "[ 31/100] train_loss: 0.07387 valid_loss: 0.08621 test_loss: 0.04607 \n",
      "[ 32/100] train_loss: 0.06951 valid_loss: 0.08457 test_loss: 0.04305 \n",
      "[ 33/100] train_loss: 0.07167 valid_loss: 0.09147 test_loss: 0.04600 \n",
      "[ 34/100] train_loss: 0.07102 valid_loss: 0.08387 test_loss: 0.04436 \n",
      "[ 35/100] train_loss: 0.07050 valid_loss: 0.08504 test_loss: 0.04505 \n",
      "[ 36/100] train_loss: 0.07013 valid_loss: 0.09056 test_loss: 0.04376 \n",
      "[ 37/100] train_loss: 0.07216 valid_loss: 0.08977 test_loss: 0.04352 \n",
      "[ 38/100] train_loss: 0.07156 valid_loss: 0.08944 test_loss: 0.04233 \n",
      "[ 39/100] train_loss: 0.06847 valid_loss: 0.08755 test_loss: 0.04286 \n",
      "[ 40/100] train_loss: 0.06921 valid_loss: 0.08565 test_loss: 0.04170 \n",
      "[ 41/100] train_loss: 0.06838 valid_loss: 0.08874 test_loss: 0.04206 \n",
      "[ 42/100] train_loss: 0.06637 valid_loss: 0.08819 test_loss: 0.04310 \n",
      "[ 43/100] train_loss: 0.06366 valid_loss: 0.09117 test_loss: 0.04133 \n",
      "[ 44/100] train_loss: 0.06432 valid_loss: 0.09136 test_loss: 0.04184 \n",
      "[ 45/100] train_loss: 0.06841 valid_loss: 0.09224 test_loss: 0.04180 \n",
      "[ 46/100] train_loss: 0.06860 valid_loss: 0.08654 test_loss: 0.03986 \n",
      "[ 47/100] train_loss: 0.06116 valid_loss: 0.09598 test_loss: 0.04164 \n",
      "[ 48/100] train_loss: 0.06227 valid_loss: 0.08711 test_loss: 0.03860 \n",
      "[ 49/100] train_loss: 0.06152 valid_loss: 0.08751 test_loss: 0.04002 \n",
      "[ 50/100] train_loss: 0.06417 valid_loss: 0.09237 test_loss: 0.04098 \n",
      "[ 51/100] train_loss: 0.06319 valid_loss: 0.09167 test_loss: 0.04030 \n",
      "[ 52/100] train_loss: 0.06218 valid_loss: 0.08640 test_loss: 0.03555 \n",
      "[ 53/100] train_loss: 0.06354 valid_loss: 0.09057 test_loss: 0.03971 \n",
      "[ 54/100] train_loss: 0.06096 valid_loss: 0.10603 test_loss: 0.04636 \n",
      "[ 55/100] train_loss: 0.06458 valid_loss: 0.09050 test_loss: 0.04021 \n",
      "[ 56/100] train_loss: 0.06070 valid_loss: 0.09632 test_loss: 0.03936 \n",
      "[ 57/100] train_loss: 0.05941 valid_loss: 0.09823 test_loss: 0.04244 \n",
      "[ 58/100] train_loss: 0.06309 valid_loss: 0.09145 test_loss: 0.04129 \n",
      "[ 59/100] train_loss: 0.06637 valid_loss: 0.08735 test_loss: 0.03852 \n",
      "[ 60/100] train_loss: 0.06277 valid_loss: 0.09460 test_loss: 0.04095 \n",
      "[ 61/100] train_loss: 0.06072 valid_loss: 0.09232 test_loss: 0.04395 \n",
      "[ 62/100] train_loss: 0.06106 valid_loss: 0.08790 test_loss: 0.03393 \n",
      "[ 63/100] train_loss: 0.06126 valid_loss: 0.09728 test_loss: 0.04460 \n",
      "[ 64/100] train_loss: 0.06320 valid_loss: 0.09392 test_loss: 0.04091 \n",
      "[ 65/100] train_loss: 0.06355 valid_loss: 0.09459 test_loss: 0.04176 \n",
      "[ 66/100] train_loss: 0.06102 valid_loss: 0.08968 test_loss: 0.03751 \n",
      "[ 67/100] train_loss: 0.06624 valid_loss: 0.09049 test_loss: 0.04396 \n",
      "[ 68/100] train_loss: 0.06010 valid_loss: 0.08715 test_loss: 0.03385 \n",
      "[ 69/100] train_loss: 0.05758 valid_loss: 0.08919 test_loss: 0.03648 \n",
      "[ 70/100] train_loss: 0.05937 valid_loss: 0.08966 test_loss: 0.03783 \n",
      "[ 71/100] train_loss: 0.05597 valid_loss: 0.09433 test_loss: 0.03580 \n",
      "[ 72/100] train_loss: 0.05845 valid_loss: 0.10134 test_loss: 0.04175 \n",
      "[ 73/100] train_loss: 0.05968 valid_loss: 0.08583 test_loss: 0.03830 \n",
      "[ 74/100] train_loss: 0.05893 valid_loss: 0.08974 test_loss: 0.03592 \n",
      "[ 75/100] train_loss: 0.05614 valid_loss: 0.09990 test_loss: 0.04475 \n",
      "[ 76/100] train_loss: 0.06020 valid_loss: 0.08770 test_loss: 0.03476 \n",
      "[ 77/100] train_loss: 0.05624 valid_loss: 0.10281 test_loss: 0.04171 \n",
      "[ 78/100] train_loss: 0.05724 valid_loss: 0.10413 test_loss: 0.04489 \n",
      "[ 79/100] train_loss: 0.05899 valid_loss: 0.08824 test_loss: 0.03214 \n",
      "[ 80/100] train_loss: 0.05574 valid_loss: 0.09116 test_loss: 0.03868 \n",
      "[ 81/100] train_loss: 0.05747 valid_loss: 0.10503 test_loss: 0.04162 \n",
      "[ 82/100] train_loss: 0.05977 valid_loss: 0.10678 test_loss: 0.04340 \n",
      "[ 83/100] train_loss: 0.06171 valid_loss: 0.08848 test_loss: 0.03710 \n",
      "[ 84/100] train_loss: 0.05529 valid_loss: 0.09417 test_loss: 0.03582 \n",
      "[ 85/100] train_loss: 0.05370 valid_loss: 0.10652 test_loss: 0.04402 \n",
      "[ 86/100] train_loss: 0.05538 valid_loss: 0.08383 test_loss: 0.03348 \n",
      "[ 87/100] train_loss: 0.05455 valid_loss: 0.09396 test_loss: 0.03470 \n",
      "[ 88/100] train_loss: 0.05631 valid_loss: 0.09770 test_loss: 0.03877 \n",
      "[ 89/100] train_loss: 0.06059 valid_loss: 0.09349 test_loss: 0.04176 \n",
      "[ 90/100] train_loss: 0.05299 valid_loss: 0.09543 test_loss: 0.03767 \n",
      "[ 91/100] train_loss: 0.05540 valid_loss: 0.09174 test_loss: 0.03655 \n",
      "[ 92/100] train_loss: 0.05757 valid_loss: 0.09447 test_loss: 0.03560 \n",
      "[ 93/100] train_loss: 0.05445 valid_loss: 0.09088 test_loss: 0.03635 \n",
      "[ 94/100] train_loss: 0.05210 valid_loss: 0.10032 test_loss: 0.04150 \n",
      "[ 95/100] train_loss: 0.05382 valid_loss: 0.09210 test_loss: 0.03591 \n",
      "[ 96/100] train_loss: 0.05213 valid_loss: 0.09507 test_loss: 0.03358 \n",
      "[ 97/100] train_loss: 0.05586 valid_loss: 0.09175 test_loss: 0.03707 \n",
      "[ 98/100] train_loss: 0.05515 valid_loss: 0.09050 test_loss: 0.03356 \n",
      "[ 99/100] train_loss: 0.05246 valid_loss: 0.09653 test_loss: 0.03592 \n",
      "[100/100] train_loss: 0.05675 valid_loss: 0.09789 test_loss: 0.04116 \n",
      "TRAINING MODEL 4\n",
      "[  1/100] train_loss: 0.18505 valid_loss: 0.18649 test_loss: 0.18343 \n",
      "Validation loss decreased (inf --> 0.186488).  Saving model ...\n",
      "[  2/100] train_loss: 0.17946 valid_loss: 0.18156 test_loss: 0.17601 \n",
      "Validation loss decreased (0.186488 --> 0.181555).  Saving model ...\n",
      "[  3/100] train_loss: 0.17342 valid_loss: 0.17459 test_loss: 0.16618 \n",
      "Validation loss decreased (0.181555 --> 0.174585).  Saving model ...\n",
      "[  4/100] train_loss: 0.16515 valid_loss: 0.16731 test_loss: 0.15660 \n",
      "Validation loss decreased (0.174585 --> 0.167312).  Saving model ...\n",
      "[  5/100] train_loss: 0.15606 valid_loss: 0.15801 test_loss: 0.14511 \n",
      "Validation loss decreased (0.167312 --> 0.158013).  Saving model ...\n",
      "[  6/100] train_loss: 0.14670 valid_loss: 0.14872 test_loss: 0.13290 \n",
      "Validation loss decreased (0.158013 --> 0.148723).  Saving model ...\n",
      "[  7/100] train_loss: 0.13679 valid_loss: 0.14078 test_loss: 0.12103 \n",
      "Validation loss decreased (0.148723 --> 0.140783).  Saving model ...\n",
      "[  8/100] train_loss: 0.12518 valid_loss: 0.12965 test_loss: 0.10968 \n",
      "Validation loss decreased (0.140783 --> 0.129654).  Saving model ...\n",
      "[  9/100] train_loss: 0.11916 valid_loss: 0.12526 test_loss: 0.10071 \n",
      "Validation loss decreased (0.129654 --> 0.125256).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11140 valid_loss: 0.11487 test_loss: 0.09090 \n",
      "Validation loss decreased (0.125256 --> 0.114870).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10242 valid_loss: 0.10459 test_loss: 0.08385 \n",
      "Validation loss decreased (0.114870 --> 0.104591).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09774 valid_loss: 0.10415 test_loss: 0.07634 \n",
      "Validation loss decreased (0.104591 --> 0.104152).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09451 valid_loss: 0.09622 test_loss: 0.07188 \n",
      "Validation loss decreased (0.104152 --> 0.096222).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09010 valid_loss: 0.09384 test_loss: 0.06705 \n",
      "Validation loss decreased (0.096222 --> 0.093837).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08942 valid_loss: 0.09502 test_loss: 0.06373 \n",
      "[ 16/100] train_loss: 0.08831 valid_loss: 0.09559 test_loss: 0.06087 \n",
      "[ 17/100] train_loss: 0.08644 valid_loss: 0.08930 test_loss: 0.05993 \n",
      "Validation loss decreased (0.093837 --> 0.089297).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08141 valid_loss: 0.09117 test_loss: 0.05629 \n",
      "[ 19/100] train_loss: 0.08311 valid_loss: 0.08711 test_loss: 0.05489 \n",
      "Validation loss decreased (0.089297 --> 0.087115).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08077 valid_loss: 0.09098 test_loss: 0.05300 \n",
      "[ 21/100] train_loss: 0.07556 valid_loss: 0.08473 test_loss: 0.05204 \n",
      "Validation loss decreased (0.087115 --> 0.084733).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07736 valid_loss: 0.08616 test_loss: 0.04913 \n",
      "[ 23/100] train_loss: 0.07705 valid_loss: 0.09000 test_loss: 0.05145 \n",
      "[ 24/100] train_loss: 0.07888 valid_loss: 0.08518 test_loss: 0.05121 \n",
      "[ 25/100] train_loss: 0.07664 valid_loss: 0.08724 test_loss: 0.04868 \n",
      "[ 26/100] train_loss: 0.07144 valid_loss: 0.08881 test_loss: 0.04679 \n",
      "[ 27/100] train_loss: 0.06703 valid_loss: 0.08311 test_loss: 0.04402 \n",
      "Validation loss decreased (0.084733 --> 0.083105).  Saving model ...\n",
      "[ 28/100] train_loss: 0.07242 valid_loss: 0.08733 test_loss: 0.04587 \n",
      "[ 29/100] train_loss: 0.07737 valid_loss: 0.08823 test_loss: 0.04643 \n",
      "[ 30/100] train_loss: 0.07493 valid_loss: 0.08969 test_loss: 0.04474 \n",
      "[ 31/100] train_loss: 0.07235 valid_loss: 0.08212 test_loss: 0.04697 \n",
      "Validation loss decreased (0.083105 --> 0.082122).  Saving model ...\n",
      "[ 32/100] train_loss: 0.06935 valid_loss: 0.08630 test_loss: 0.04303 \n",
      "[ 33/100] train_loss: 0.06817 valid_loss: 0.08077 test_loss: 0.04417 \n",
      "Validation loss decreased (0.082122 --> 0.080773).  Saving model ...\n",
      "[ 34/100] train_loss: 0.07113 valid_loss: 0.08340 test_loss: 0.04133 \n",
      "[ 35/100] train_loss: 0.06691 valid_loss: 0.08306 test_loss: 0.04260 \n",
      "[ 36/100] train_loss: 0.07133 valid_loss: 0.08287 test_loss: 0.04444 \n",
      "[ 37/100] train_loss: 0.07158 valid_loss: 0.08508 test_loss: 0.04206 \n",
      "[ 38/100] train_loss: 0.06641 valid_loss: 0.08927 test_loss: 0.04108 \n",
      "[ 39/100] train_loss: 0.06727 valid_loss: 0.09143 test_loss: 0.04245 \n",
      "[ 40/100] train_loss: 0.06849 valid_loss: 0.08326 test_loss: 0.04111 \n",
      "[ 41/100] train_loss: 0.06902 valid_loss: 0.08587 test_loss: 0.04222 \n",
      "[ 42/100] train_loss: 0.07480 valid_loss: 0.08551 test_loss: 0.04199 \n",
      "[ 43/100] train_loss: 0.06766 valid_loss: 0.08329 test_loss: 0.03823 \n",
      "[ 44/100] train_loss: 0.06335 valid_loss: 0.08505 test_loss: 0.04111 \n",
      "[ 45/100] train_loss: 0.06804 valid_loss: 0.08623 test_loss: 0.04154 \n",
      "[ 46/100] train_loss: 0.06463 valid_loss: 0.08513 test_loss: 0.03852 \n",
      "[ 47/100] train_loss: 0.07017 valid_loss: 0.08612 test_loss: 0.04351 \n",
      "[ 48/100] train_loss: 0.06433 valid_loss: 0.08671 test_loss: 0.03874 \n",
      "[ 49/100] train_loss: 0.06533 valid_loss: 0.08094 test_loss: 0.04142 \n",
      "[ 50/100] train_loss: 0.06552 valid_loss: 0.07845 test_loss: 0.03674 \n",
      "Validation loss decreased (0.080773 --> 0.078453).  Saving model ...\n",
      "[ 51/100] train_loss: 0.06596 valid_loss: 0.08367 test_loss: 0.04033 \n",
      "[ 52/100] train_loss: 0.06666 valid_loss: 0.09418 test_loss: 0.04377 \n",
      "[ 53/100] train_loss: 0.06549 valid_loss: 0.08795 test_loss: 0.03917 \n",
      "[ 54/100] train_loss: 0.05750 valid_loss: 0.08741 test_loss: 0.03777 \n",
      "[ 55/100] train_loss: 0.06018 valid_loss: 0.09378 test_loss: 0.04001 \n",
      "[ 56/100] train_loss: 0.06220 valid_loss: 0.08684 test_loss: 0.04057 \n",
      "[ 57/100] train_loss: 0.06292 valid_loss: 0.08208 test_loss: 0.03843 \n",
      "[ 58/100] train_loss: 0.06654 valid_loss: 0.08522 test_loss: 0.04044 \n",
      "[ 59/100] train_loss: 0.06207 valid_loss: 0.08540 test_loss: 0.03747 \n",
      "[ 60/100] train_loss: 0.06034 valid_loss: 0.07821 test_loss: 0.03753 \n",
      "Validation loss decreased (0.078453 --> 0.078212).  Saving model ...\n",
      "[ 61/100] train_loss: 0.06154 valid_loss: 0.08414 test_loss: 0.03939 \n",
      "[ 62/100] train_loss: 0.05724 valid_loss: 0.09916 test_loss: 0.04005 \n",
      "[ 63/100] train_loss: 0.06166 valid_loss: 0.08944 test_loss: 0.04480 \n",
      "[ 64/100] train_loss: 0.06184 valid_loss: 0.07876 test_loss: 0.03462 \n",
      "[ 65/100] train_loss: 0.05961 valid_loss: 0.09941 test_loss: 0.04716 \n",
      "[ 66/100] train_loss: 0.06114 valid_loss: 0.08579 test_loss: 0.03785 \n",
      "[ 67/100] train_loss: 0.05560 valid_loss: 0.07619 test_loss: 0.03607 \n",
      "Validation loss decreased (0.078212 --> 0.076186).  Saving model ...\n",
      "[ 68/100] train_loss: 0.06605 valid_loss: 0.08380 test_loss: 0.04124 \n",
      "[ 69/100] train_loss: 0.06339 valid_loss: 0.09419 test_loss: 0.04089 \n",
      "[ 70/100] train_loss: 0.06050 valid_loss: 0.08613 test_loss: 0.04040 \n",
      "[ 71/100] train_loss: 0.06142 valid_loss: 0.08867 test_loss: 0.03745 \n",
      "[ 72/100] train_loss: 0.06349 valid_loss: 0.07807 test_loss: 0.03668 \n",
      "[ 73/100] train_loss: 0.05628 valid_loss: 0.07983 test_loss: 0.03173 \n",
      "[ 74/100] train_loss: 0.06134 valid_loss: 0.10172 test_loss: 0.05080 \n",
      "[ 75/100] train_loss: 0.06128 valid_loss: 0.08329 test_loss: 0.03421 \n",
      "[ 76/100] train_loss: 0.05720 valid_loss: 0.08591 test_loss: 0.03733 \n",
      "[ 77/100] train_loss: 0.06019 valid_loss: 0.08233 test_loss: 0.03769 \n",
      "[ 78/100] train_loss: 0.06360 valid_loss: 0.07996 test_loss: 0.03580 \n",
      "[ 79/100] train_loss: 0.06323 valid_loss: 0.09290 test_loss: 0.04303 \n",
      "[ 80/100] train_loss: 0.06012 valid_loss: 0.09037 test_loss: 0.03930 \n",
      "[ 81/100] train_loss: 0.05692 valid_loss: 0.08074 test_loss: 0.03443 \n",
      "[ 82/100] train_loss: 0.06393 valid_loss: 0.09912 test_loss: 0.04744 \n",
      "[ 83/100] train_loss: 0.05677 valid_loss: 0.09801 test_loss: 0.04104 \n",
      "[ 84/100] train_loss: 0.05901 valid_loss: 0.08332 test_loss: 0.03772 \n",
      "[ 85/100] train_loss: 0.05957 valid_loss: 0.08854 test_loss: 0.03805 \n",
      "[ 86/100] train_loss: 0.05457 valid_loss: 0.07847 test_loss: 0.03266 \n",
      "[ 87/100] train_loss: 0.05797 valid_loss: 0.09208 test_loss: 0.04241 \n",
      "[ 88/100] train_loss: 0.05692 valid_loss: 0.09597 test_loss: 0.03762 \n",
      "[ 89/100] train_loss: 0.05834 valid_loss: 0.08184 test_loss: 0.03740 \n",
      "[ 90/100] train_loss: 0.05942 valid_loss: 0.08384 test_loss: 0.03815 \n",
      "[ 91/100] train_loss: 0.05723 valid_loss: 0.08609 test_loss: 0.03744 \n",
      "[ 92/100] train_loss: 0.06011 valid_loss: 0.08485 test_loss: 0.03862 \n",
      "[ 93/100] train_loss: 0.05497 valid_loss: 0.08084 test_loss: 0.03683 \n",
      "[ 94/100] train_loss: 0.05750 valid_loss: 0.09275 test_loss: 0.03666 \n",
      "[ 95/100] train_loss: 0.05278 valid_loss: 0.08582 test_loss: 0.03222 \n",
      "[ 96/100] train_loss: 0.05745 valid_loss: 0.09538 test_loss: 0.04209 \n",
      "[ 97/100] train_loss: 0.05580 valid_loss: 0.09421 test_loss: 0.04110 \n",
      "[ 98/100] train_loss: 0.05614 valid_loss: 0.08542 test_loss: 0.03492 \n",
      "[ 99/100] train_loss: 0.05601 valid_loss: 0.09500 test_loss: 0.04245 \n",
      "[100/100] train_loss: 0.05362 valid_loss: 0.08519 test_loss: 0.03374 \n",
      "TRAINING MODEL 5\n",
      "[  1/100] train_loss: 0.18571 valid_loss: 0.18697 test_loss: 0.18492 \n",
      "Validation loss decreased (inf --> 0.186970).  Saving model ...\n",
      "[  2/100] train_loss: 0.17998 valid_loss: 0.18212 test_loss: 0.17794 \n",
      "Validation loss decreased (0.186970 --> 0.182124).  Saving model ...\n",
      "[  3/100] train_loss: 0.17475 valid_loss: 0.17649 test_loss: 0.16894 \n",
      "Validation loss decreased (0.182124 --> 0.176491).  Saving model ...\n",
      "[  4/100] train_loss: 0.16829 valid_loss: 0.16979 test_loss: 0.16012 \n",
      "Validation loss decreased (0.176491 --> 0.169790).  Saving model ...\n",
      "[  5/100] train_loss: 0.16170 valid_loss: 0.16265 test_loss: 0.15065 \n",
      "Validation loss decreased (0.169790 --> 0.162650).  Saving model ...\n",
      "[  6/100] train_loss: 0.15371 valid_loss: 0.15482 test_loss: 0.14113 \n",
      "Validation loss decreased (0.162650 --> 0.154817).  Saving model ...\n",
      "[  7/100] train_loss: 0.14585 valid_loss: 0.14643 test_loss: 0.13134 \n",
      "Validation loss decreased (0.154817 --> 0.146433).  Saving model ...\n",
      "[  8/100] train_loss: 0.13604 valid_loss: 0.13702 test_loss: 0.12036 \n",
      "Validation loss decreased (0.146433 --> 0.137018).  Saving model ...\n",
      "[  9/100] train_loss: 0.12786 valid_loss: 0.12966 test_loss: 0.11004 \n",
      "Validation loss decreased (0.137018 --> 0.129663).  Saving model ...\n",
      "[ 10/100] train_loss: 0.12063 valid_loss: 0.12143 test_loss: 0.10061 \n",
      "Validation loss decreased (0.129663 --> 0.121433).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11569 valid_loss: 0.11404 test_loss: 0.09191 \n",
      "Validation loss decreased (0.121433 --> 0.114040).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10578 valid_loss: 0.10888 test_loss: 0.08420 \n",
      "Validation loss decreased (0.114040 --> 0.108885).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09653 valid_loss: 0.10261 test_loss: 0.07667 \n",
      "Validation loss decreased (0.108885 --> 0.102608).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09661 valid_loss: 0.09756 test_loss: 0.07200 \n",
      "Validation loss decreased (0.102608 --> 0.097558).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09239 valid_loss: 0.09756 test_loss: 0.06775 \n",
      "[ 16/100] train_loss: 0.08948 valid_loss: 0.09325 test_loss: 0.06351 \n",
      "Validation loss decreased (0.097558 --> 0.093250).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08702 valid_loss: 0.09000 test_loss: 0.06122 \n",
      "Validation loss decreased (0.093250 --> 0.090000).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08562 valid_loss: 0.08730 test_loss: 0.06063 \n",
      "Validation loss decreased (0.090000 --> 0.087304).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08350 valid_loss: 0.09290 test_loss: 0.05513 \n",
      "[ 20/100] train_loss: 0.08413 valid_loss: 0.08722 test_loss: 0.05545 \n",
      "Validation loss decreased (0.087304 --> 0.087216).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07898 valid_loss: 0.08442 test_loss: 0.05472 \n",
      "Validation loss decreased (0.087216 --> 0.084416).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07740 valid_loss: 0.09379 test_loss: 0.05019 \n",
      "[ 23/100] train_loss: 0.07804 valid_loss: 0.08403 test_loss: 0.05458 \n",
      "Validation loss decreased (0.084416 --> 0.084033).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07508 valid_loss: 0.08476 test_loss: 0.04786 \n",
      "[ 25/100] train_loss: 0.07485 valid_loss: 0.08371 test_loss: 0.04801 \n",
      "Validation loss decreased (0.084033 --> 0.083710).  Saving model ...\n",
      "[ 26/100] train_loss: 0.07530 valid_loss: 0.08594 test_loss: 0.04744 \n",
      "[ 27/100] train_loss: 0.07619 valid_loss: 0.08137 test_loss: 0.04750 \n",
      "Validation loss decreased (0.083710 --> 0.081369).  Saving model ...\n",
      "[ 28/100] train_loss: 0.07605 valid_loss: 0.08411 test_loss: 0.04687 \n",
      "[ 29/100] train_loss: 0.06856 valid_loss: 0.07918 test_loss: 0.04653 \n",
      "Validation loss decreased (0.081369 --> 0.079180).  Saving model ...\n",
      "[ 30/100] train_loss: 0.06871 valid_loss: 0.07791 test_loss: 0.04592 \n",
      "Validation loss decreased (0.079180 --> 0.077912).  Saving model ...\n",
      "[ 31/100] train_loss: 0.07268 valid_loss: 0.08892 test_loss: 0.04390 \n",
      "[ 32/100] train_loss: 0.07231 valid_loss: 0.08146 test_loss: 0.04338 \n",
      "[ 33/100] train_loss: 0.07501 valid_loss: 0.08145 test_loss: 0.04402 \n",
      "[ 34/100] train_loss: 0.07126 valid_loss: 0.07795 test_loss: 0.04488 \n",
      "[ 35/100] train_loss: 0.06662 valid_loss: 0.07698 test_loss: 0.04512 \n",
      "Validation loss decreased (0.077912 --> 0.076978).  Saving model ...\n",
      "[ 36/100] train_loss: 0.07328 valid_loss: 0.08594 test_loss: 0.04392 \n",
      "[ 37/100] train_loss: 0.06911 valid_loss: 0.07939 test_loss: 0.04156 \n",
      "[ 38/100] train_loss: 0.06804 valid_loss: 0.07628 test_loss: 0.04679 \n",
      "Validation loss decreased (0.076978 --> 0.076277).  Saving model ...\n",
      "[ 39/100] train_loss: 0.06742 valid_loss: 0.07949 test_loss: 0.04105 \n",
      "[ 40/100] train_loss: 0.06560 valid_loss: 0.08664 test_loss: 0.04124 \n",
      "[ 41/100] train_loss: 0.06569 valid_loss: 0.07837 test_loss: 0.04118 \n",
      "[ 42/100] train_loss: 0.06748 valid_loss: 0.07753 test_loss: 0.03898 \n",
      "[ 43/100] train_loss: 0.06777 valid_loss: 0.07896 test_loss: 0.03894 \n",
      "[ 44/100] train_loss: 0.06371 valid_loss: 0.07590 test_loss: 0.03852 \n",
      "Validation loss decreased (0.076277 --> 0.075901).  Saving model ...\n",
      "[ 45/100] train_loss: 0.06692 valid_loss: 0.07996 test_loss: 0.04269 \n",
      "[ 46/100] train_loss: 0.06524 valid_loss: 0.07700 test_loss: 0.03865 \n",
      "[ 47/100] train_loss: 0.06906 valid_loss: 0.07809 test_loss: 0.03922 \n",
      "[ 48/100] train_loss: 0.06053 valid_loss: 0.07996 test_loss: 0.03760 \n",
      "[ 49/100] train_loss: 0.06567 valid_loss: 0.07745 test_loss: 0.03854 \n",
      "[ 50/100] train_loss: 0.06701 valid_loss: 0.07843 test_loss: 0.03960 \n",
      "[ 51/100] train_loss: 0.06399 valid_loss: 0.08623 test_loss: 0.03702 \n",
      "[ 52/100] train_loss: 0.06114 valid_loss: 0.07950 test_loss: 0.03804 \n",
      "[ 53/100] train_loss: 0.06440 valid_loss: 0.07890 test_loss: 0.03864 \n",
      "[ 54/100] train_loss: 0.06306 valid_loss: 0.07347 test_loss: 0.03823 \n",
      "Validation loss decreased (0.075901 --> 0.073465).  Saving model ...\n",
      "[ 55/100] train_loss: 0.06241 valid_loss: 0.07727 test_loss: 0.03789 \n",
      "[ 56/100] train_loss: 0.06044 valid_loss: 0.08119 test_loss: 0.03720 \n",
      "[ 57/100] train_loss: 0.06438 valid_loss: 0.07854 test_loss: 0.03747 \n",
      "[ 58/100] train_loss: 0.06447 valid_loss: 0.07536 test_loss: 0.03865 \n",
      "[ 59/100] train_loss: 0.06196 valid_loss: 0.07894 test_loss: 0.03501 \n",
      "[ 60/100] train_loss: 0.06082 valid_loss: 0.07722 test_loss: 0.03612 \n",
      "[ 61/100] train_loss: 0.06584 valid_loss: 0.08422 test_loss: 0.04022 \n",
      "[ 62/100] train_loss: 0.06506 valid_loss: 0.07944 test_loss: 0.03805 \n",
      "[ 63/100] train_loss: 0.05504 valid_loss: 0.07858 test_loss: 0.03491 \n",
      "[ 64/100] train_loss: 0.05929 valid_loss: 0.08701 test_loss: 0.03937 \n",
      "[ 65/100] train_loss: 0.06513 valid_loss: 0.07732 test_loss: 0.04000 \n",
      "[ 66/100] train_loss: 0.05714 valid_loss: 0.07825 test_loss: 0.03317 \n",
      "[ 67/100] train_loss: 0.06466 valid_loss: 0.08200 test_loss: 0.04242 \n",
      "[ 68/100] train_loss: 0.06301 valid_loss: 0.07336 test_loss: 0.03554 \n",
      "Validation loss decreased (0.073465 --> 0.073364).  Saving model ...\n",
      "[ 69/100] train_loss: 0.05401 valid_loss: 0.07491 test_loss: 0.03060 \n",
      "[ 70/100] train_loss: 0.05772 valid_loss: 0.07823 test_loss: 0.03597 \n",
      "[ 71/100] train_loss: 0.06181 valid_loss: 0.07357 test_loss: 0.03268 \n",
      "[ 72/100] train_loss: 0.06123 valid_loss: 0.07532 test_loss: 0.03383 \n",
      "[ 73/100] train_loss: 0.05792 valid_loss: 0.07757 test_loss: 0.03457 \n",
      "[ 74/100] train_loss: 0.05539 valid_loss: 0.07796 test_loss: 0.03277 \n",
      "[ 75/100] train_loss: 0.06056 valid_loss: 0.07597 test_loss: 0.03653 \n",
      "[ 76/100] train_loss: 0.05656 valid_loss: 0.07791 test_loss: 0.03275 \n",
      "[ 77/100] train_loss: 0.05776 valid_loss: 0.08135 test_loss: 0.03527 \n",
      "[ 78/100] train_loss: 0.05541 valid_loss: 0.08357 test_loss: 0.03593 \n",
      "[ 79/100] train_loss: 0.05727 valid_loss: 0.07551 test_loss: 0.03744 \n",
      "[ 80/100] train_loss: 0.05722 valid_loss: 0.07649 test_loss: 0.03164 \n",
      "[ 81/100] train_loss: 0.05899 valid_loss: 0.08277 test_loss: 0.03437 \n",
      "[ 82/100] train_loss: 0.05993 valid_loss: 0.08304 test_loss: 0.03353 \n",
      "[ 83/100] train_loss: 0.06017 valid_loss: 0.08045 test_loss: 0.03301 \n",
      "[ 84/100] train_loss: 0.05928 valid_loss: 0.07506 test_loss: 0.03196 \n",
      "[ 85/100] train_loss: 0.05976 valid_loss: 0.07897 test_loss: 0.03721 \n",
      "[ 86/100] train_loss: 0.05910 valid_loss: 0.07562 test_loss: 0.03188 \n",
      "[ 87/100] train_loss: 0.05580 valid_loss: 0.08156 test_loss: 0.03143 \n",
      "[ 88/100] train_loss: 0.05279 valid_loss: 0.08090 test_loss: 0.03334 \n",
      "[ 89/100] train_loss: 0.05368 valid_loss: 0.07793 test_loss: 0.03259 \n",
      "[ 90/100] train_loss: 0.05515 valid_loss: 0.07616 test_loss: 0.03051 \n",
      "[ 91/100] train_loss: 0.05912 valid_loss: 0.07738 test_loss: 0.03635 \n",
      "[ 92/100] train_loss: 0.05879 valid_loss: 0.07971 test_loss: 0.03573 \n",
      "[ 93/100] train_loss: 0.05528 valid_loss: 0.07667 test_loss: 0.03238 \n",
      "[ 94/100] train_loss: 0.05560 valid_loss: 0.07312 test_loss: 0.03365 \n",
      "Validation loss decreased (0.073364 --> 0.073119).  Saving model ...\n",
      "[ 95/100] train_loss: 0.05820 valid_loss: 0.07730 test_loss: 0.03535 \n",
      "[ 96/100] train_loss: 0.05455 valid_loss: 0.07771 test_loss: 0.03094 \n",
      "[ 97/100] train_loss: 0.05483 valid_loss: 0.07435 test_loss: 0.03260 \n",
      "[ 98/100] train_loss: 0.05662 valid_loss: 0.07651 test_loss: 0.03453 \n",
      "[ 99/100] train_loss: 0.05244 valid_loss: 0.07628 test_loss: 0.02784 \n",
      "[100/100] train_loss: 0.05776 valid_loss: 0.07748 test_loss: 0.03473 \n",
      "TRAINING MODEL 6\n",
      "[  1/100] train_loss: 0.18378 valid_loss: 0.19306 test_loss: 0.17811 \n",
      "Validation loss decreased (inf --> 0.193061).  Saving model ...\n",
      "[  2/100] train_loss: 0.17918 valid_loss: 0.18863 test_loss: 0.17104 \n",
      "Validation loss decreased (0.193061 --> 0.188632).  Saving model ...\n",
      "[  3/100] train_loss: 0.17304 valid_loss: 0.18276 test_loss: 0.16072 \n",
      "Validation loss decreased (0.188632 --> 0.182757).  Saving model ...\n",
      "[  4/100] train_loss: 0.16706 valid_loss: 0.17708 test_loss: 0.15088 \n",
      "Validation loss decreased (0.182757 --> 0.177079).  Saving model ...\n",
      "[  5/100] train_loss: 0.15892 valid_loss: 0.16988 test_loss: 0.13998 \n",
      "Validation loss decreased (0.177079 --> 0.169876).  Saving model ...\n",
      "[  6/100] train_loss: 0.15077 valid_loss: 0.16102 test_loss: 0.12983 \n",
      "Validation loss decreased (0.169876 --> 0.161017).  Saving model ...\n",
      "[  7/100] train_loss: 0.14227 valid_loss: 0.15504 test_loss: 0.12347 \n",
      "Validation loss decreased (0.161017 --> 0.155044).  Saving model ...\n",
      "[  8/100] train_loss: 0.13380 valid_loss: 0.14423 test_loss: 0.11059 \n",
      "Validation loss decreased (0.155044 --> 0.144225).  Saving model ...\n",
      "[  9/100] train_loss: 0.12531 valid_loss: 0.13563 test_loss: 0.09978 \n",
      "Validation loss decreased (0.144225 --> 0.135632).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11702 valid_loss: 0.12966 test_loss: 0.09183 \n",
      "Validation loss decreased (0.135632 --> 0.129657).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10888 valid_loss: 0.12263 test_loss: 0.08410 \n",
      "Validation loss decreased (0.129657 --> 0.122625).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10863 valid_loss: 0.11188 test_loss: 0.07766 \n",
      "Validation loss decreased (0.122625 --> 0.111878).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10170 valid_loss: 0.10785 test_loss: 0.07261 \n",
      "Validation loss decreased (0.111878 --> 0.107853).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09727 valid_loss: 0.10562 test_loss: 0.06904 \n",
      "Validation loss decreased (0.107853 --> 0.105625).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09241 valid_loss: 0.10552 test_loss: 0.06558 \n",
      "Validation loss decreased (0.105625 --> 0.105518).  Saving model ...\n",
      "[ 16/100] train_loss: 0.09239 valid_loss: 0.09953 test_loss: 0.06267 \n",
      "Validation loss decreased (0.105518 --> 0.099528).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08672 valid_loss: 0.09871 test_loss: 0.06035 \n",
      "Validation loss decreased (0.099528 --> 0.098712).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08534 valid_loss: 0.09378 test_loss: 0.05727 \n",
      "Validation loss decreased (0.098712 --> 0.093777).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08382 valid_loss: 0.08937 test_loss: 0.05790 \n",
      "Validation loss decreased (0.093777 --> 0.089373).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08374 valid_loss: 0.08987 test_loss: 0.05410 \n",
      "[ 21/100] train_loss: 0.08215 valid_loss: 0.09248 test_loss: 0.05267 \n",
      "[ 22/100] train_loss: 0.08182 valid_loss: 0.08989 test_loss: 0.05107 \n",
      "[ 23/100] train_loss: 0.07495 valid_loss: 0.08850 test_loss: 0.05036 \n",
      "Validation loss decreased (0.089373 --> 0.088497).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07129 valid_loss: 0.08572 test_loss: 0.04824 \n",
      "Validation loss decreased (0.088497 --> 0.085722).  Saving model ...\n",
      "[ 25/100] train_loss: 0.07346 valid_loss: 0.08433 test_loss: 0.04750 \n",
      "Validation loss decreased (0.085722 --> 0.084332).  Saving model ...\n",
      "[ 26/100] train_loss: 0.07413 valid_loss: 0.09619 test_loss: 0.04819 \n",
      "[ 27/100] train_loss: 0.07609 valid_loss: 0.08540 test_loss: 0.04567 \n",
      "[ 28/100] train_loss: 0.07203 valid_loss: 0.08828 test_loss: 0.04432 \n",
      "[ 29/100] train_loss: 0.07290 valid_loss: 0.08912 test_loss: 0.04417 \n",
      "[ 30/100] train_loss: 0.07391 valid_loss: 0.08670 test_loss: 0.04351 \n",
      "[ 31/100] train_loss: 0.06853 valid_loss: 0.08600 test_loss: 0.04229 \n",
      "[ 32/100] train_loss: 0.06836 valid_loss: 0.08538 test_loss: 0.04201 \n",
      "[ 33/100] train_loss: 0.06931 valid_loss: 0.08617 test_loss: 0.04220 \n",
      "[ 34/100] train_loss: 0.07000 valid_loss: 0.08193 test_loss: 0.04145 \n",
      "Validation loss decreased (0.084332 --> 0.081934).  Saving model ...\n",
      "[ 35/100] train_loss: 0.06667 valid_loss: 0.08841 test_loss: 0.04067 \n",
      "[ 36/100] train_loss: 0.07163 valid_loss: 0.07939 test_loss: 0.04083 \n",
      "Validation loss decreased (0.081934 --> 0.079394).  Saving model ...\n",
      "[ 37/100] train_loss: 0.06719 valid_loss: 0.08723 test_loss: 0.03993 \n",
      "[ 38/100] train_loss: 0.07074 valid_loss: 0.08589 test_loss: 0.04003 \n",
      "[ 39/100] train_loss: 0.06624 valid_loss: 0.08046 test_loss: 0.03936 \n",
      "[ 40/100] train_loss: 0.06284 valid_loss: 0.08750 test_loss: 0.03912 \n",
      "[ 41/100] train_loss: 0.06639 valid_loss: 0.09803 test_loss: 0.04335 \n",
      "[ 42/100] train_loss: 0.06819 valid_loss: 0.08061 test_loss: 0.03894 \n",
      "[ 43/100] train_loss: 0.06788 valid_loss: 0.08164 test_loss: 0.03904 \n",
      "[ 44/100] train_loss: 0.06416 valid_loss: 0.08592 test_loss: 0.03821 \n",
      "[ 45/100] train_loss: 0.06514 valid_loss: 0.09303 test_loss: 0.04039 \n",
      "[ 46/100] train_loss: 0.06951 valid_loss: 0.08932 test_loss: 0.03988 \n",
      "[ 47/100] train_loss: 0.06288 valid_loss: 0.08172 test_loss: 0.03591 \n",
      "[ 48/100] train_loss: 0.06389 valid_loss: 0.08534 test_loss: 0.03894 \n",
      "[ 49/100] train_loss: 0.05957 valid_loss: 0.09331 test_loss: 0.03810 \n",
      "[ 50/100] train_loss: 0.05840 valid_loss: 0.08404 test_loss: 0.03340 \n",
      "[ 51/100] train_loss: 0.06370 valid_loss: 0.09567 test_loss: 0.04243 \n",
      "[ 52/100] train_loss: 0.06327 valid_loss: 0.08481 test_loss: 0.03389 \n",
      "[ 53/100] train_loss: 0.06190 valid_loss: 0.08869 test_loss: 0.03861 \n",
      "[ 54/100] train_loss: 0.05831 valid_loss: 0.08935 test_loss: 0.03571 \n",
      "[ 55/100] train_loss: 0.06293 valid_loss: 0.09577 test_loss: 0.04212 \n",
      "[ 56/100] train_loss: 0.05839 valid_loss: 0.09024 test_loss: 0.03438 \n",
      "[ 57/100] train_loss: 0.05958 valid_loss: 0.08692 test_loss: 0.03749 \n",
      "[ 58/100] train_loss: 0.06083 valid_loss: 0.08235 test_loss: 0.03429 \n",
      "[ 59/100] train_loss: 0.06255 valid_loss: 0.09264 test_loss: 0.04014 \n",
      "[ 60/100] train_loss: 0.05992 valid_loss: 0.08542 test_loss: 0.03367 \n",
      "[ 61/100] train_loss: 0.06160 valid_loss: 0.09088 test_loss: 0.03955 \n",
      "[ 62/100] train_loss: 0.05950 valid_loss: 0.09298 test_loss: 0.03889 \n",
      "[ 63/100] train_loss: 0.06288 valid_loss: 0.08336 test_loss: 0.03647 \n",
      "[ 64/100] train_loss: 0.05709 valid_loss: 0.09161 test_loss: 0.03749 \n",
      "[ 65/100] train_loss: 0.05598 valid_loss: 0.08583 test_loss: 0.03316 \n",
      "[ 66/100] train_loss: 0.05483 valid_loss: 0.08842 test_loss: 0.03483 \n",
      "[ 67/100] train_loss: 0.05876 valid_loss: 0.08757 test_loss: 0.03592 \n",
      "[ 68/100] train_loss: 0.05978 valid_loss: 0.08461 test_loss: 0.03125 \n",
      "[ 69/100] train_loss: 0.05890 valid_loss: 0.08658 test_loss: 0.03510 \n",
      "[ 70/100] train_loss: 0.06020 valid_loss: 0.09624 test_loss: 0.03732 \n",
      "[ 71/100] train_loss: 0.06010 valid_loss: 0.08161 test_loss: 0.03352 \n",
      "[ 72/100] train_loss: 0.06173 valid_loss: 0.08665 test_loss: 0.03549 \n",
      "[ 73/100] train_loss: 0.05913 valid_loss: 0.09005 test_loss: 0.03456 \n",
      "[ 74/100] train_loss: 0.06048 valid_loss: 0.08708 test_loss: 0.03640 \n",
      "[ 75/100] train_loss: 0.05810 valid_loss: 0.09015 test_loss: 0.03370 \n",
      "[ 76/100] train_loss: 0.05812 valid_loss: 0.09799 test_loss: 0.03950 \n",
      "[ 77/100] train_loss: 0.05518 valid_loss: 0.08506 test_loss: 0.03098 \n",
      "[ 78/100] train_loss: 0.05652 valid_loss: 0.09754 test_loss: 0.03751 \n",
      "[ 79/100] train_loss: 0.05701 valid_loss: 0.10076 test_loss: 0.03675 \n",
      "[ 80/100] train_loss: 0.05768 valid_loss: 0.09056 test_loss: 0.03611 \n",
      "[ 81/100] train_loss: 0.05749 valid_loss: 0.09252 test_loss: 0.03419 \n",
      "[ 82/100] train_loss: 0.05392 valid_loss: 0.08568 test_loss: 0.03124 \n",
      "[ 83/100] train_loss: 0.05521 valid_loss: 0.09874 test_loss: 0.03860 \n",
      "[ 84/100] train_loss: 0.05584 valid_loss: 0.09569 test_loss: 0.03391 \n",
      "[ 85/100] train_loss: 0.05697 valid_loss: 0.08743 test_loss: 0.03312 \n",
      "[ 86/100] train_loss: 0.05297 valid_loss: 0.09643 test_loss: 0.03355 \n",
      "[ 87/100] train_loss: 0.05511 valid_loss: 0.08916 test_loss: 0.03459 \n",
      "[ 88/100] train_loss: 0.05969 valid_loss: 0.09119 test_loss: 0.03620 \n",
      "[ 89/100] train_loss: 0.05352 valid_loss: 0.09097 test_loss: 0.03214 \n",
      "[ 90/100] train_loss: 0.05318 valid_loss: 0.08914 test_loss: 0.03262 \n",
      "[ 91/100] train_loss: 0.05622 valid_loss: 0.09372 test_loss: 0.03643 \n",
      "[ 92/100] train_loss: 0.05394 valid_loss: 0.10946 test_loss: 0.04070 \n",
      "[ 93/100] train_loss: 0.05709 valid_loss: 0.09863 test_loss: 0.03700 \n",
      "[ 94/100] train_loss: 0.05309 valid_loss: 0.08560 test_loss: 0.03038 \n",
      "[ 95/100] train_loss: 0.05342 valid_loss: 0.09534 test_loss: 0.03415 \n",
      "[ 96/100] train_loss: 0.05752 valid_loss: 0.09492 test_loss: 0.03370 \n",
      "[ 97/100] train_loss: 0.05667 valid_loss: 0.09423 test_loss: 0.03800 \n",
      "[ 98/100] train_loss: 0.05394 valid_loss: 0.10020 test_loss: 0.03812 \n",
      "[ 99/100] train_loss: 0.05445 valid_loss: 0.09191 test_loss: 0.03396 \n",
      "[100/100] train_loss: 0.05754 valid_loss: 0.09737 test_loss: 0.03971 \n",
      "TRAINING MODEL 7\n",
      "[  1/100] train_loss: 0.18449 valid_loss: 0.18464 test_loss: 0.18397 \n",
      "Validation loss decreased (inf --> 0.184643).  Saving model ...\n",
      "[  2/100] train_loss: 0.17888 valid_loss: 0.17910 test_loss: 0.17699 \n",
      "Validation loss decreased (0.184643 --> 0.179100).  Saving model ...\n",
      "[  3/100] train_loss: 0.17319 valid_loss: 0.17126 test_loss: 0.16870 \n",
      "Validation loss decreased (0.179100 --> 0.171259).  Saving model ...\n",
      "[  4/100] train_loss: 0.16648 valid_loss: 0.16351 test_loss: 0.16053 \n",
      "Validation loss decreased (0.171259 --> 0.163506).  Saving model ...\n",
      "[  5/100] train_loss: 0.15842 valid_loss: 0.15602 test_loss: 0.15241 \n",
      "Validation loss decreased (0.163506 --> 0.156019).  Saving model ...\n",
      "[  6/100] train_loss: 0.15023 valid_loss: 0.14782 test_loss: 0.14307 \n",
      "Validation loss decreased (0.156019 --> 0.147816).  Saving model ...\n",
      "[  7/100] train_loss: 0.14237 valid_loss: 0.13832 test_loss: 0.13373 \n",
      "Validation loss decreased (0.147816 --> 0.138322).  Saving model ...\n",
      "[  8/100] train_loss: 0.13247 valid_loss: 0.12905 test_loss: 0.12401 \n",
      "Validation loss decreased (0.138322 --> 0.129051).  Saving model ...\n",
      "[  9/100] train_loss: 0.12465 valid_loss: 0.11886 test_loss: 0.11508 \n",
      "Validation loss decreased (0.129051 --> 0.118860).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11900 valid_loss: 0.11268 test_loss: 0.10737 \n",
      "Validation loss decreased (0.118860 --> 0.112675).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11315 valid_loss: 0.10409 test_loss: 0.09879 \n",
      "Validation loss decreased (0.112675 --> 0.104094).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10693 valid_loss: 0.10161 test_loss: 0.09207 \n",
      "Validation loss decreased (0.104094 --> 0.101609).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10194 valid_loss: 0.09619 test_loss: 0.08592 \n",
      "Validation loss decreased (0.101609 --> 0.096186).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09913 valid_loss: 0.09106 test_loss: 0.08059 \n",
      "Validation loss decreased (0.096186 --> 0.091065).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09302 valid_loss: 0.08939 test_loss: 0.07519 \n",
      "Validation loss decreased (0.091065 --> 0.089393).  Saving model ...\n",
      "[ 16/100] train_loss: 0.09628 valid_loss: 0.08879 test_loss: 0.07097 \n",
      "Validation loss decreased (0.089393 --> 0.088791).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08674 valid_loss: 0.08661 test_loss: 0.06741 \n",
      "Validation loss decreased (0.088791 --> 0.086611).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08421 valid_loss: 0.08550 test_loss: 0.06285 \n",
      "Validation loss decreased (0.086611 --> 0.085495).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08367 valid_loss: 0.08346 test_loss: 0.05998 \n",
      "Validation loss decreased (0.085495 --> 0.083463).  Saving model ...\n",
      "[ 20/100] train_loss: 0.07830 valid_loss: 0.08508 test_loss: 0.05825 \n",
      "[ 21/100] train_loss: 0.08141 valid_loss: 0.08095 test_loss: 0.05915 \n",
      "Validation loss decreased (0.083463 --> 0.080948).  Saving model ...\n",
      "[ 22/100] train_loss: 0.08245 valid_loss: 0.08206 test_loss: 0.05445 \n",
      "[ 23/100] train_loss: 0.08035 valid_loss: 0.07966 test_loss: 0.05525 \n",
      "Validation loss decreased (0.080948 --> 0.079656).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07835 valid_loss: 0.08009 test_loss: 0.05073 \n",
      "[ 25/100] train_loss: 0.07393 valid_loss: 0.08100 test_loss: 0.05055 \n",
      "[ 26/100] train_loss: 0.07728 valid_loss: 0.08487 test_loss: 0.04985 \n",
      "[ 27/100] train_loss: 0.07754 valid_loss: 0.08657 test_loss: 0.04911 \n",
      "[ 28/100] train_loss: 0.07703 valid_loss: 0.08161 test_loss: 0.04920 \n",
      "[ 29/100] train_loss: 0.07248 valid_loss: 0.07772 test_loss: 0.04986 \n",
      "Validation loss decreased (0.079656 --> 0.077715).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07357 valid_loss: 0.07961 test_loss: 0.04527 \n",
      "[ 31/100] train_loss: 0.07079 valid_loss: 0.08235 test_loss: 0.04463 \n",
      "[ 32/100] train_loss: 0.07078 valid_loss: 0.07769 test_loss: 0.04424 \n",
      "Validation loss decreased (0.077715 --> 0.077686).  Saving model ...\n",
      "[ 33/100] train_loss: 0.07071 valid_loss: 0.07769 test_loss: 0.04437 \n",
      "[ 34/100] train_loss: 0.07014 valid_loss: 0.08231 test_loss: 0.04344 \n",
      "[ 35/100] train_loss: 0.06972 valid_loss: 0.07911 test_loss: 0.04218 \n",
      "[ 36/100] train_loss: 0.06999 valid_loss: 0.07952 test_loss: 0.04303 \n",
      "[ 37/100] train_loss: 0.07046 valid_loss: 0.08030 test_loss: 0.04160 \n",
      "[ 38/100] train_loss: 0.06861 valid_loss: 0.08636 test_loss: 0.04175 \n",
      "[ 39/100] train_loss: 0.06898 valid_loss: 0.07611 test_loss: 0.04230 \n",
      "Validation loss decreased (0.077686 --> 0.076113).  Saving model ...\n",
      "[ 40/100] train_loss: 0.07065 valid_loss: 0.07869 test_loss: 0.03972 \n",
      "[ 41/100] train_loss: 0.06786 valid_loss: 0.07968 test_loss: 0.04130 \n",
      "[ 42/100] train_loss: 0.07008 valid_loss: 0.08149 test_loss: 0.03965 \n",
      "[ 43/100] train_loss: 0.06675 valid_loss: 0.08336 test_loss: 0.04128 \n",
      "[ 44/100] train_loss: 0.06655 valid_loss: 0.07716 test_loss: 0.03985 \n",
      "[ 45/100] train_loss: 0.06480 valid_loss: 0.07749 test_loss: 0.03833 \n",
      "[ 46/100] train_loss: 0.05907 valid_loss: 0.07950 test_loss: 0.03622 \n",
      "[ 47/100] train_loss: 0.06478 valid_loss: 0.08251 test_loss: 0.03858 \n",
      "[ 48/100] train_loss: 0.06555 valid_loss: 0.07651 test_loss: 0.03681 \n",
      "[ 49/100] train_loss: 0.06723 valid_loss: 0.07908 test_loss: 0.03888 \n",
      "[ 50/100] train_loss: 0.06531 valid_loss: 0.08445 test_loss: 0.04033 \n",
      "[ 51/100] train_loss: 0.06403 valid_loss: 0.07369 test_loss: 0.03663 \n",
      "Validation loss decreased (0.076113 --> 0.073688).  Saving model ...\n",
      "[ 52/100] train_loss: 0.06311 valid_loss: 0.08041 test_loss: 0.03558 \n",
      "[ 53/100] train_loss: 0.06435 valid_loss: 0.08313 test_loss: 0.03931 \n",
      "[ 54/100] train_loss: 0.06020 valid_loss: 0.07653 test_loss: 0.03529 \n",
      "[ 55/100] train_loss: 0.06178 valid_loss: 0.09150 test_loss: 0.04249 \n",
      "[ 56/100] train_loss: 0.06025 valid_loss: 0.07922 test_loss: 0.03634 \n",
      "[ 57/100] train_loss: 0.06010 valid_loss: 0.08175 test_loss: 0.03570 \n",
      "[ 58/100] train_loss: 0.06355 valid_loss: 0.08254 test_loss: 0.03965 \n",
      "[ 59/100] train_loss: 0.06020 valid_loss: 0.07519 test_loss: 0.03226 \n",
      "[ 60/100] train_loss: 0.06271 valid_loss: 0.07074 test_loss: 0.03529 \n",
      "Validation loss decreased (0.073688 --> 0.070739).  Saving model ...\n",
      "[ 61/100] train_loss: 0.06419 valid_loss: 0.08609 test_loss: 0.03576 \n",
      "[ 62/100] train_loss: 0.05843 valid_loss: 0.08254 test_loss: 0.03491 \n",
      "[ 63/100] train_loss: 0.05915 valid_loss: 0.08175 test_loss: 0.03476 \n",
      "[ 64/100] train_loss: 0.05898 valid_loss: 0.07195 test_loss: 0.03346 \n",
      "[ 65/100] train_loss: 0.05944 valid_loss: 0.07161 test_loss: 0.03746 \n",
      "[ 66/100] train_loss: 0.06098 valid_loss: 0.08532 test_loss: 0.03708 \n",
      "[ 67/100] train_loss: 0.05861 valid_loss: 0.09206 test_loss: 0.04007 \n",
      "[ 68/100] train_loss: 0.06373 valid_loss: 0.07876 test_loss: 0.03341 \n",
      "[ 69/100] train_loss: 0.06257 valid_loss: 0.07692 test_loss: 0.03613 \n",
      "[ 70/100] train_loss: 0.06249 valid_loss: 0.07732 test_loss: 0.03380 \n",
      "[ 71/100] train_loss: 0.05969 valid_loss: 0.07883 test_loss: 0.03142 \n",
      "[ 72/100] train_loss: 0.05843 valid_loss: 0.07604 test_loss: 0.03360 \n",
      "[ 73/100] train_loss: 0.05748 valid_loss: 0.07872 test_loss: 0.03369 \n",
      "[ 74/100] train_loss: 0.05685 valid_loss: 0.07923 test_loss: 0.03234 \n",
      "[ 75/100] train_loss: 0.05665 valid_loss: 0.07187 test_loss: 0.03404 \n",
      "[ 76/100] train_loss: 0.06129 valid_loss: 0.07741 test_loss: 0.03518 \n",
      "[ 77/100] train_loss: 0.05882 valid_loss: 0.07615 test_loss: 0.02928 \n",
      "[ 78/100] train_loss: 0.05560 valid_loss: 0.07897 test_loss: 0.03262 \n",
      "[ 79/100] train_loss: 0.05676 valid_loss: 0.07959 test_loss: 0.03402 \n",
      "[ 80/100] train_loss: 0.05420 valid_loss: 0.07761 test_loss: 0.03098 \n",
      "[ 81/100] train_loss: 0.05950 valid_loss: 0.08842 test_loss: 0.04265 \n",
      "[ 82/100] train_loss: 0.06091 valid_loss: 0.09180 test_loss: 0.04446 \n",
      "[ 83/100] train_loss: 0.05737 valid_loss: 0.08093 test_loss: 0.03326 \n",
      "[ 84/100] train_loss: 0.05718 valid_loss: 0.08434 test_loss: 0.03150 \n",
      "[ 85/100] train_loss: 0.05429 valid_loss: 0.07617 test_loss: 0.03221 \n",
      "[ 86/100] train_loss: 0.05579 valid_loss: 0.08054 test_loss: 0.03140 \n",
      "[ 87/100] train_loss: 0.05320 valid_loss: 0.07718 test_loss: 0.03162 \n",
      "[ 88/100] train_loss: 0.05805 valid_loss: 0.08595 test_loss: 0.03685 \n",
      "[ 89/100] train_loss: 0.05874 valid_loss: 0.08074 test_loss: 0.03830 \n",
      "[ 90/100] train_loss: 0.05513 valid_loss: 0.08648 test_loss: 0.03597 \n",
      "[ 91/100] train_loss: 0.05314 valid_loss: 0.08465 test_loss: 0.03002 \n",
      "[ 92/100] train_loss: 0.05806 valid_loss: 0.08254 test_loss: 0.03925 \n",
      "[ 93/100] train_loss: 0.05731 valid_loss: 0.08977 test_loss: 0.03383 \n",
      "[ 94/100] train_loss: 0.05567 valid_loss: 0.07903 test_loss: 0.03563 \n",
      "[ 95/100] train_loss: 0.05636 valid_loss: 0.07614 test_loss: 0.03006 \n",
      "[ 96/100] train_loss: 0.05697 valid_loss: 0.09166 test_loss: 0.03585 \n",
      "[ 97/100] train_loss: 0.06018 valid_loss: 0.07392 test_loss: 0.03235 \n",
      "[ 98/100] train_loss: 0.04977 valid_loss: 0.09824 test_loss: 0.03895 \n",
      "[ 99/100] train_loss: 0.05571 valid_loss: 0.08163 test_loss: 0.03562 \n",
      "[100/100] train_loss: 0.05826 valid_loss: 0.08149 test_loss: 0.03170 \n",
      "TRAINING MODEL 8\n",
      "[  1/100] train_loss: 0.18310 valid_loss: 0.19670 test_loss: 0.17245 \n",
      "Validation loss decreased (inf --> 0.196702).  Saving model ...\n",
      "[  2/100] train_loss: 0.17605 valid_loss: 0.19060 test_loss: 0.16490 \n",
      "Validation loss decreased (0.196702 --> 0.190596).  Saving model ...\n",
      "[  3/100] train_loss: 0.17071 valid_loss: 0.18330 test_loss: 0.15549 \n",
      "Validation loss decreased (0.190596 --> 0.183303).  Saving model ...\n",
      "[  4/100] train_loss: 0.16187 valid_loss: 0.17604 test_loss: 0.14550 \n",
      "Validation loss decreased (0.183303 --> 0.176041).  Saving model ...\n",
      "[  5/100] train_loss: 0.15394 valid_loss: 0.16782 test_loss: 0.13501 \n",
      "Validation loss decreased (0.176041 --> 0.167821).  Saving model ...\n",
      "[  6/100] train_loss: 0.14383 valid_loss: 0.15918 test_loss: 0.12389 \n",
      "Validation loss decreased (0.167821 --> 0.159175).  Saving model ...\n",
      "[  7/100] train_loss: 0.13449 valid_loss: 0.14943 test_loss: 0.11406 \n",
      "Validation loss decreased (0.159175 --> 0.149434).  Saving model ...\n",
      "[  8/100] train_loss: 0.12703 valid_loss: 0.13998 test_loss: 0.10394 \n",
      "Validation loss decreased (0.149434 --> 0.139982).  Saving model ...\n",
      "[  9/100] train_loss: 0.11718 valid_loss: 0.13190 test_loss: 0.09597 \n",
      "Validation loss decreased (0.139982 --> 0.131905).  Saving model ...\n",
      "[ 10/100] train_loss: 0.10806 valid_loss: 0.12363 test_loss: 0.08784 \n",
      "Validation loss decreased (0.131905 --> 0.123628).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10235 valid_loss: 0.11635 test_loss: 0.08203 \n",
      "Validation loss decreased (0.123628 --> 0.116350).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10099 valid_loss: 0.10640 test_loss: 0.07499 \n",
      "Validation loss decreased (0.116350 --> 0.106402).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09508 valid_loss: 0.10891 test_loss: 0.07224 \n",
      "[ 14/100] train_loss: 0.08940 valid_loss: 0.10247 test_loss: 0.06822 \n",
      "Validation loss decreased (0.106402 --> 0.102469).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09100 valid_loss: 0.09988 test_loss: 0.06742 \n",
      "Validation loss decreased (0.102469 --> 0.099876).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08642 valid_loss: 0.10143 test_loss: 0.06409 \n",
      "[ 17/100] train_loss: 0.08089 valid_loss: 0.09574 test_loss: 0.05973 \n",
      "Validation loss decreased (0.099876 --> 0.095743).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08131 valid_loss: 0.10519 test_loss: 0.06063 \n",
      "[ 19/100] train_loss: 0.08216 valid_loss: 0.10105 test_loss: 0.05788 \n",
      "[ 20/100] train_loss: 0.08124 valid_loss: 0.08854 test_loss: 0.05565 \n",
      "Validation loss decreased (0.095743 --> 0.088539).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07900 valid_loss: 0.10000 test_loss: 0.05457 \n",
      "[ 22/100] train_loss: 0.07772 valid_loss: 0.09465 test_loss: 0.05319 \n",
      "[ 23/100] train_loss: 0.08118 valid_loss: 0.09381 test_loss: 0.05353 \n",
      "[ 24/100] train_loss: 0.07925 valid_loss: 0.09359 test_loss: 0.05214 \n",
      "[ 25/100] train_loss: 0.07644 valid_loss: 0.09314 test_loss: 0.05119 \n",
      "[ 26/100] train_loss: 0.07995 valid_loss: 0.09093 test_loss: 0.04916 \n",
      "[ 27/100] train_loss: 0.07380 valid_loss: 0.08880 test_loss: 0.04786 \n",
      "[ 28/100] train_loss: 0.07273 valid_loss: 0.08699 test_loss: 0.04528 \n",
      "Validation loss decreased (0.088539 --> 0.086986).  Saving model ...\n",
      "[ 29/100] train_loss: 0.07503 valid_loss: 0.08914 test_loss: 0.04759 \n",
      "[ 30/100] train_loss: 0.07490 valid_loss: 0.09312 test_loss: 0.04796 \n",
      "[ 31/100] train_loss: 0.06812 valid_loss: 0.08584 test_loss: 0.04421 \n",
      "Validation loss decreased (0.086986 --> 0.085838).  Saving model ...\n",
      "[ 32/100] train_loss: 0.07236 valid_loss: 0.08632 test_loss: 0.04572 \n",
      "[ 33/100] train_loss: 0.07622 valid_loss: 0.08832 test_loss: 0.04636 \n",
      "[ 34/100] train_loss: 0.06983 valid_loss: 0.08893 test_loss: 0.04455 \n",
      "[ 35/100] train_loss: 0.06595 valid_loss: 0.09055 test_loss: 0.04206 \n",
      "[ 36/100] train_loss: 0.06913 valid_loss: 0.09810 test_loss: 0.04547 \n",
      "[ 37/100] train_loss: 0.07073 valid_loss: 0.08461 test_loss: 0.04476 \n",
      "Validation loss decreased (0.085838 --> 0.084609).  Saving model ...\n",
      "[ 38/100] train_loss: 0.07067 valid_loss: 0.08577 test_loss: 0.04230 \n",
      "[ 39/100] train_loss: 0.07043 valid_loss: 0.09689 test_loss: 0.04584 \n",
      "[ 40/100] train_loss: 0.06996 valid_loss: 0.09272 test_loss: 0.04303 \n",
      "[ 41/100] train_loss: 0.06950 valid_loss: 0.08854 test_loss: 0.04103 \n",
      "[ 42/100] train_loss: 0.06785 valid_loss: 0.09258 test_loss: 0.04199 \n",
      "[ 43/100] train_loss: 0.06702 valid_loss: 0.09203 test_loss: 0.04300 \n",
      "[ 44/100] train_loss: 0.06700 valid_loss: 0.08247 test_loss: 0.03797 \n",
      "Validation loss decreased (0.084609 --> 0.082466).  Saving model ...\n",
      "[ 45/100] train_loss: 0.06383 valid_loss: 0.09762 test_loss: 0.04328 \n",
      "[ 46/100] train_loss: 0.06610 valid_loss: 0.09117 test_loss: 0.04145 \n",
      "[ 47/100] train_loss: 0.06748 valid_loss: 0.09205 test_loss: 0.04334 \n",
      "[ 48/100] train_loss: 0.06629 valid_loss: 0.09154 test_loss: 0.04107 \n",
      "[ 49/100] train_loss: 0.06566 valid_loss: 0.09352 test_loss: 0.04340 \n",
      "[ 50/100] train_loss: 0.06314 valid_loss: 0.08774 test_loss: 0.03779 \n",
      "[ 51/100] train_loss: 0.06534 valid_loss: 0.09731 test_loss: 0.04361 \n",
      "[ 52/100] train_loss: 0.06190 valid_loss: 0.09291 test_loss: 0.04232 \n",
      "[ 53/100] train_loss: 0.06432 valid_loss: 0.08596 test_loss: 0.04036 \n",
      "[ 54/100] train_loss: 0.06395 valid_loss: 0.08774 test_loss: 0.03847 \n",
      "[ 55/100] train_loss: 0.06736 valid_loss: 0.09563 test_loss: 0.04334 \n",
      "[ 56/100] train_loss: 0.06388 valid_loss: 0.09573 test_loss: 0.04187 \n",
      "[ 57/100] train_loss: 0.06220 valid_loss: 0.09144 test_loss: 0.04193 \n",
      "[ 58/100] train_loss: 0.06532 valid_loss: 0.09308 test_loss: 0.03992 \n",
      "[ 59/100] train_loss: 0.06195 valid_loss: 0.09223 test_loss: 0.04084 \n",
      "[ 60/100] train_loss: 0.06031 valid_loss: 0.09543 test_loss: 0.04119 \n",
      "[ 61/100] train_loss: 0.06179 valid_loss: 0.09879 test_loss: 0.04491 \n",
      "[ 62/100] train_loss: 0.05616 valid_loss: 0.08822 test_loss: 0.03816 \n",
      "[ 63/100] train_loss: 0.06080 valid_loss: 0.08897 test_loss: 0.03990 \n",
      "[ 64/100] train_loss: 0.06236 valid_loss: 0.09239 test_loss: 0.03945 \n",
      "[ 65/100] train_loss: 0.06132 valid_loss: 0.08851 test_loss: 0.04168 \n",
      "[ 66/100] train_loss: 0.06287 valid_loss: 0.09543 test_loss: 0.04191 \n",
      "[ 67/100] train_loss: 0.06015 valid_loss: 0.09786 test_loss: 0.04177 \n",
      "[ 68/100] train_loss: 0.05831 valid_loss: 0.09066 test_loss: 0.04016 \n",
      "[ 69/100] train_loss: 0.06220 valid_loss: 0.10457 test_loss: 0.05026 \n",
      "[ 70/100] train_loss: 0.05883 valid_loss: 0.09379 test_loss: 0.03923 \n",
      "[ 71/100] train_loss: 0.05862 valid_loss: 0.08113 test_loss: 0.03851 \n",
      "Validation loss decreased (0.082466 --> 0.081134).  Saving model ...\n",
      "[ 72/100] train_loss: 0.06094 valid_loss: 0.10135 test_loss: 0.04526 \n",
      "[ 73/100] train_loss: 0.05881 valid_loss: 0.09495 test_loss: 0.04231 \n",
      "[ 74/100] train_loss: 0.05729 valid_loss: 0.08930 test_loss: 0.03981 \n",
      "[ 75/100] train_loss: 0.05469 valid_loss: 0.09249 test_loss: 0.03840 \n",
      "[ 76/100] train_loss: 0.05600 valid_loss: 0.09537 test_loss: 0.04241 \n",
      "[ 77/100] train_loss: 0.06165 valid_loss: 0.10814 test_loss: 0.04725 \n",
      "[ 78/100] train_loss: 0.05682 valid_loss: 0.08687 test_loss: 0.03762 \n",
      "[ 79/100] train_loss: 0.05544 valid_loss: 0.09039 test_loss: 0.03580 \n",
      "[ 80/100] train_loss: 0.05642 valid_loss: 0.08965 test_loss: 0.03763 \n",
      "[ 81/100] train_loss: 0.05657 valid_loss: 0.09087 test_loss: 0.03775 \n",
      "[ 82/100] train_loss: 0.05858 valid_loss: 0.09344 test_loss: 0.04310 \n",
      "[ 83/100] train_loss: 0.05564 valid_loss: 0.08902 test_loss: 0.03823 \n",
      "[ 84/100] train_loss: 0.05435 valid_loss: 0.09425 test_loss: 0.04009 \n",
      "[ 85/100] train_loss: 0.05613 valid_loss: 0.08698 test_loss: 0.03572 \n",
      "[ 86/100] train_loss: 0.05530 valid_loss: 0.09986 test_loss: 0.04438 \n",
      "[ 87/100] train_loss: 0.05842 valid_loss: 0.10326 test_loss: 0.04603 \n",
      "[ 88/100] train_loss: 0.05835 valid_loss: 0.09040 test_loss: 0.04212 \n",
      "[ 89/100] train_loss: 0.05019 valid_loss: 0.10105 test_loss: 0.04249 \n",
      "[ 90/100] train_loss: 0.05426 valid_loss: 0.09718 test_loss: 0.04363 \n",
      "[ 91/100] train_loss: 0.05618 valid_loss: 0.08958 test_loss: 0.04066 \n",
      "[ 92/100] train_loss: 0.05688 valid_loss: 0.09349 test_loss: 0.03594 \n",
      "[ 93/100] train_loss: 0.05209 valid_loss: 0.09770 test_loss: 0.04360 \n",
      "[ 94/100] train_loss: 0.05428 valid_loss: 0.08734 test_loss: 0.03737 \n",
      "[ 95/100] train_loss: 0.05350 valid_loss: 0.08336 test_loss: 0.03881 \n",
      "[ 96/100] train_loss: 0.05514 valid_loss: 0.08903 test_loss: 0.03652 \n",
      "[ 97/100] train_loss: 0.05586 valid_loss: 0.08853 test_loss: 0.04042 \n",
      "[ 98/100] train_loss: 0.05194 valid_loss: 0.09981 test_loss: 0.04274 \n",
      "[ 99/100] train_loss: 0.05690 valid_loss: 0.10047 test_loss: 0.04833 \n",
      "[100/100] train_loss: 0.05242 valid_loss: 0.08946 test_loss: 0.03317 \n",
      "TRAINING MODEL 9\n",
      "[  1/100] train_loss: 0.18348 valid_loss: 0.19580 test_loss: 0.17462 \n",
      "Validation loss decreased (inf --> 0.195804).  Saving model ...\n",
      "[  2/100] train_loss: 0.17937 valid_loss: 0.19031 test_loss: 0.16900 \n",
      "Validation loss decreased (0.195804 --> 0.190311).  Saving model ...\n",
      "[  3/100] train_loss: 0.17404 valid_loss: 0.18289 test_loss: 0.16058 \n",
      "Validation loss decreased (0.190311 --> 0.182889).  Saving model ...\n",
      "[  4/100] train_loss: 0.16530 valid_loss: 0.17472 test_loss: 0.15099 \n",
      "Validation loss decreased (0.182889 --> 0.174720).  Saving model ...\n",
      "[  5/100] train_loss: 0.15903 valid_loss: 0.16462 test_loss: 0.13996 \n",
      "Validation loss decreased (0.174720 --> 0.164617).  Saving model ...\n",
      "[  6/100] train_loss: 0.14848 valid_loss: 0.15864 test_loss: 0.13178 \n",
      "Validation loss decreased (0.164617 --> 0.158643).  Saving model ...\n",
      "[  7/100] train_loss: 0.13818 valid_loss: 0.14697 test_loss: 0.11945 \n",
      "Validation loss decreased (0.158643 --> 0.146966).  Saving model ...\n",
      "[  8/100] train_loss: 0.13170 valid_loss: 0.13292 test_loss: 0.10936 \n",
      "Validation loss decreased (0.146966 --> 0.132919).  Saving model ...\n",
      "[  9/100] train_loss: 0.11846 valid_loss: 0.12475 test_loss: 0.09885 \n",
      "Validation loss decreased (0.132919 --> 0.124748).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11315 valid_loss: 0.11676 test_loss: 0.08947 \n",
      "Validation loss decreased (0.124748 --> 0.116756).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10539 valid_loss: 0.11006 test_loss: 0.08152 \n",
      "Validation loss decreased (0.116756 --> 0.110056).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09757 valid_loss: 0.10253 test_loss: 0.07485 \n",
      "Validation loss decreased (0.110056 --> 0.102532).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09584 valid_loss: 0.09815 test_loss: 0.06965 \n",
      "Validation loss decreased (0.102532 --> 0.098153).  Saving model ...\n",
      "[ 14/100] train_loss: 0.08945 valid_loss: 0.09174 test_loss: 0.06445 \n",
      "Validation loss decreased (0.098153 --> 0.091741).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09165 valid_loss: 0.09302 test_loss: 0.06158 \n",
      "[ 16/100] train_loss: 0.08587 valid_loss: 0.09213 test_loss: 0.05835 \n",
      "[ 17/100] train_loss: 0.08387 valid_loss: 0.09337 test_loss: 0.05569 \n",
      "[ 18/100] train_loss: 0.08518 valid_loss: 0.08671 test_loss: 0.05564 \n",
      "Validation loss decreased (0.091741 --> 0.086707).  Saving model ...\n",
      "[ 19/100] train_loss: 0.07985 valid_loss: 0.08571 test_loss: 0.05395 \n",
      "Validation loss decreased (0.086707 --> 0.085710).  Saving model ...\n",
      "[ 20/100] train_loss: 0.07767 valid_loss: 0.08557 test_loss: 0.05329 \n",
      "Validation loss decreased (0.085710 --> 0.085567).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07638 valid_loss: 0.09136 test_loss: 0.05062 \n",
      "[ 22/100] train_loss: 0.07424 valid_loss: 0.08238 test_loss: 0.04849 \n",
      "Validation loss decreased (0.085567 --> 0.082379).  Saving model ...\n",
      "[ 23/100] train_loss: 0.07484 valid_loss: 0.08650 test_loss: 0.04906 \n",
      "[ 24/100] train_loss: 0.07869 valid_loss: 0.08776 test_loss: 0.04795 \n",
      "[ 25/100] train_loss: 0.07949 valid_loss: 0.08841 test_loss: 0.04539 \n",
      "[ 26/100] train_loss: 0.07304 valid_loss: 0.08428 test_loss: 0.04660 \n",
      "[ 27/100] train_loss: 0.06860 valid_loss: 0.09067 test_loss: 0.04399 \n",
      "[ 28/100] train_loss: 0.07138 valid_loss: 0.08771 test_loss: 0.04277 \n",
      "[ 29/100] train_loss: 0.06980 valid_loss: 0.08765 test_loss: 0.04375 \n",
      "[ 30/100] train_loss: 0.06940 valid_loss: 0.08419 test_loss: 0.04307 \n",
      "[ 31/100] train_loss: 0.07034 valid_loss: 0.08642 test_loss: 0.04165 \n",
      "[ 32/100] train_loss: 0.07379 valid_loss: 0.08633 test_loss: 0.04429 \n",
      "[ 33/100] train_loss: 0.07374 valid_loss: 0.08656 test_loss: 0.04324 \n",
      "[ 34/100] train_loss: 0.07308 valid_loss: 0.08246 test_loss: 0.04050 \n",
      "[ 35/100] train_loss: 0.06694 valid_loss: 0.08421 test_loss: 0.03982 \n",
      "[ 36/100] train_loss: 0.06758 valid_loss: 0.08574 test_loss: 0.04274 \n",
      "[ 37/100] train_loss: 0.06790 valid_loss: 0.08317 test_loss: 0.04239 \n",
      "[ 38/100] train_loss: 0.07071 valid_loss: 0.08251 test_loss: 0.04057 \n",
      "[ 39/100] train_loss: 0.06669 valid_loss: 0.08303 test_loss: 0.03983 \n",
      "[ 40/100] train_loss: 0.06495 valid_loss: 0.08440 test_loss: 0.03921 \n",
      "[ 41/100] train_loss: 0.07042 valid_loss: 0.09439 test_loss: 0.04119 \n",
      "[ 42/100] train_loss: 0.06441 valid_loss: 0.08492 test_loss: 0.03824 \n",
      "[ 43/100] train_loss: 0.06439 valid_loss: 0.08823 test_loss: 0.03931 \n",
      "[ 44/100] train_loss: 0.06614 valid_loss: 0.08756 test_loss: 0.04072 \n",
      "[ 45/100] train_loss: 0.06599 valid_loss: 0.08122 test_loss: 0.04025 \n",
      "Validation loss decreased (0.082379 --> 0.081222).  Saving model ...\n",
      "[ 46/100] train_loss: 0.06322 valid_loss: 0.07852 test_loss: 0.03542 \n",
      "Validation loss decreased (0.081222 --> 0.078519).  Saving model ...\n",
      "[ 47/100] train_loss: 0.06490 valid_loss: 0.08355 test_loss: 0.04096 \n",
      "[ 48/100] train_loss: 0.06134 valid_loss: 0.09121 test_loss: 0.03786 \n",
      "[ 49/100] train_loss: 0.06356 valid_loss: 0.09240 test_loss: 0.04138 \n",
      "[ 50/100] train_loss: 0.06256 valid_loss: 0.08455 test_loss: 0.03677 \n",
      "[ 51/100] train_loss: 0.06678 valid_loss: 0.08710 test_loss: 0.03875 \n",
      "[ 52/100] train_loss: 0.05967 valid_loss: 0.09111 test_loss: 0.03762 \n",
      "[ 53/100] train_loss: 0.06057 valid_loss: 0.09199 test_loss: 0.03985 \n",
      "[ 54/100] train_loss: 0.06172 valid_loss: 0.09486 test_loss: 0.03809 \n",
      "[ 55/100] train_loss: 0.05814 valid_loss: 0.09101 test_loss: 0.03841 \n",
      "[ 56/100] train_loss: 0.06346 valid_loss: 0.09135 test_loss: 0.03911 \n",
      "[ 57/100] train_loss: 0.06246 valid_loss: 0.08394 test_loss: 0.03632 \n",
      "[ 58/100] train_loss: 0.06650 valid_loss: 0.09038 test_loss: 0.04030 \n",
      "[ 59/100] train_loss: 0.06290 valid_loss: 0.08405 test_loss: 0.03578 \n",
      "[ 60/100] train_loss: 0.05710 valid_loss: 0.08665 test_loss: 0.03817 \n",
      "[ 61/100] train_loss: 0.06167 valid_loss: 0.09064 test_loss: 0.03702 \n",
      "[ 62/100] train_loss: 0.06130 valid_loss: 0.08560 test_loss: 0.03937 \n",
      "[ 63/100] train_loss: 0.05852 valid_loss: 0.09401 test_loss: 0.03868 \n",
      "[ 64/100] train_loss: 0.06243 valid_loss: 0.07818 test_loss: 0.03581 \n",
      "Validation loss decreased (0.078519 --> 0.078178).  Saving model ...\n",
      "[ 65/100] train_loss: 0.05747 valid_loss: 0.09033 test_loss: 0.03615 \n",
      "[ 66/100] train_loss: 0.05848 valid_loss: 0.09470 test_loss: 0.03915 \n",
      "[ 67/100] train_loss: 0.06147 valid_loss: 0.08375 test_loss: 0.03574 \n",
      "[ 68/100] train_loss: 0.06418 valid_loss: 0.10073 test_loss: 0.04350 \n",
      "[ 69/100] train_loss: 0.06483 valid_loss: 0.10203 test_loss: 0.03977 \n",
      "[ 70/100] train_loss: 0.06223 valid_loss: 0.08074 test_loss: 0.03669 \n",
      "[ 71/100] train_loss: 0.06099 valid_loss: 0.09646 test_loss: 0.04179 \n",
      "[ 72/100] train_loss: 0.05814 valid_loss: 0.08338 test_loss: 0.03297 \n",
      "[ 73/100] train_loss: 0.05446 valid_loss: 0.09494 test_loss: 0.03805 \n",
      "[ 74/100] train_loss: 0.05681 valid_loss: 0.08520 test_loss: 0.03408 \n",
      "[ 75/100] train_loss: 0.05596 valid_loss: 0.08452 test_loss: 0.03428 \n",
      "[ 76/100] train_loss: 0.05953 valid_loss: 0.09687 test_loss: 0.04133 \n",
      "[ 77/100] train_loss: 0.05648 valid_loss: 0.08773 test_loss: 0.03166 \n",
      "[ 78/100] train_loss: 0.05877 valid_loss: 0.08847 test_loss: 0.03965 \n",
      "[ 79/100] train_loss: 0.05542 valid_loss: 0.08625 test_loss: 0.03242 \n",
      "[ 80/100] train_loss: 0.06133 valid_loss: 0.08023 test_loss: 0.04453 \n",
      "[ 81/100] train_loss: 0.06006 valid_loss: 0.09633 test_loss: 0.03598 \n",
      "[ 82/100] train_loss: 0.05785 valid_loss: 0.09708 test_loss: 0.04479 \n",
      "[ 83/100] train_loss: 0.05366 valid_loss: 0.08686 test_loss: 0.03471 \n",
      "[ 84/100] train_loss: 0.05620 valid_loss: 0.09450 test_loss: 0.04014 \n",
      "[ 85/100] train_loss: 0.05796 valid_loss: 0.10516 test_loss: 0.04578 \n",
      "[ 86/100] train_loss: 0.05595 valid_loss: 0.08748 test_loss: 0.03364 \n",
      "[ 87/100] train_loss: 0.05527 valid_loss: 0.08552 test_loss: 0.03127 \n",
      "[ 88/100] train_loss: 0.05336 valid_loss: 0.08712 test_loss: 0.03486 \n",
      "[ 89/100] train_loss: 0.05437 valid_loss: 0.09051 test_loss: 0.03459 \n",
      "[ 90/100] train_loss: 0.05613 valid_loss: 0.09372 test_loss: 0.03685 \n",
      "[ 91/100] train_loss: 0.05417 valid_loss: 0.09084 test_loss: 0.03394 \n",
      "[ 92/100] train_loss: 0.05863 valid_loss: 0.09503 test_loss: 0.04167 \n",
      "[ 93/100] train_loss: 0.05539 valid_loss: 0.08088 test_loss: 0.03179 \n",
      "[ 94/100] train_loss: 0.05507 valid_loss: 0.08934 test_loss: 0.03760 \n",
      "[ 95/100] train_loss: 0.05280 valid_loss: 0.09493 test_loss: 0.03584 \n",
      "[ 96/100] train_loss: 0.05748 valid_loss: 0.09355 test_loss: 0.03885 \n",
      "[ 97/100] train_loss: 0.05594 valid_loss: 0.08621 test_loss: 0.03571 \n",
      "[ 98/100] train_loss: 0.05769 valid_loss: 0.09033 test_loss: 0.03797 \n",
      "[ 99/100] train_loss: 0.05687 valid_loss: 0.09280 test_loss: 0.03608 \n",
      "[100/100] train_loss: 0.04981 valid_loss: 0.08454 test_loss: 0.03232 \n",
      "TRAINING MODEL 10\n",
      "[  1/100] train_loss: 0.18450 valid_loss: 0.18933 test_loss: 0.17869 \n",
      "Validation loss decreased (inf --> 0.189328).  Saving model ...\n",
      "[  2/100] train_loss: 0.18003 valid_loss: 0.18406 test_loss: 0.17277 \n",
      "Validation loss decreased (0.189328 --> 0.184062).  Saving model ...\n",
      "[  3/100] train_loss: 0.17462 valid_loss: 0.17776 test_loss: 0.16617 \n",
      "Validation loss decreased (0.184062 --> 0.177765).  Saving model ...\n",
      "[  4/100] train_loss: 0.16805 valid_loss: 0.17046 test_loss: 0.15847 \n",
      "Validation loss decreased (0.177765 --> 0.170464).  Saving model ...\n",
      "[  5/100] train_loss: 0.15915 valid_loss: 0.16215 test_loss: 0.14973 \n",
      "Validation loss decreased (0.170464 --> 0.162148).  Saving model ...\n",
      "[  6/100] train_loss: 0.15139 valid_loss: 0.15572 test_loss: 0.14084 \n",
      "Validation loss decreased (0.162148 --> 0.155719).  Saving model ...\n",
      "[  7/100] train_loss: 0.14240 valid_loss: 0.14581 test_loss: 0.13017 \n",
      "Validation loss decreased (0.155719 --> 0.145808).  Saving model ...\n",
      "[  8/100] train_loss: 0.13172 valid_loss: 0.13569 test_loss: 0.11784 \n",
      "Validation loss decreased (0.145808 --> 0.135693).  Saving model ...\n",
      "[  9/100] train_loss: 0.12388 valid_loss: 0.11944 test_loss: 0.10542 \n",
      "Validation loss decreased (0.135693 --> 0.119438).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11310 valid_loss: 0.11220 test_loss: 0.09572 \n",
      "Validation loss decreased (0.119438 --> 0.112201).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10868 valid_loss: 0.10413 test_loss: 0.08776 \n",
      "Validation loss decreased (0.112201 --> 0.104132).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09918 valid_loss: 0.10177 test_loss: 0.08003 \n",
      "Validation loss decreased (0.104132 --> 0.101768).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09586 valid_loss: 0.09504 test_loss: 0.07320 \n",
      "Validation loss decreased (0.101768 --> 0.095036).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09153 valid_loss: 0.09000 test_loss: 0.06736 \n",
      "Validation loss decreased (0.095036 --> 0.089996).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09204 valid_loss: 0.08787 test_loss: 0.06539 \n",
      "Validation loss decreased (0.089996 --> 0.087866).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08866 valid_loss: 0.08772 test_loss: 0.06070 \n",
      "Validation loss decreased (0.087866 --> 0.087722).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08830 valid_loss: 0.08559 test_loss: 0.05918 \n",
      "Validation loss decreased (0.087722 --> 0.085593).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08300 valid_loss: 0.08840 test_loss: 0.05569 \n",
      "[ 19/100] train_loss: 0.08035 valid_loss: 0.08437 test_loss: 0.05383 \n",
      "Validation loss decreased (0.085593 --> 0.084370).  Saving model ...\n",
      "[ 20/100] train_loss: 0.07764 valid_loss: 0.08454 test_loss: 0.05241 \n",
      "[ 21/100] train_loss: 0.07731 valid_loss: 0.08269 test_loss: 0.05163 \n",
      "Validation loss decreased (0.084370 --> 0.082693).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07728 valid_loss: 0.08504 test_loss: 0.04943 \n",
      "[ 23/100] train_loss: 0.07283 valid_loss: 0.08166 test_loss: 0.04897 \n",
      "Validation loss decreased (0.082693 --> 0.081660).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07337 valid_loss: 0.08175 test_loss: 0.04755 \n",
      "[ 25/100] train_loss: 0.07299 valid_loss: 0.08195 test_loss: 0.04543 \n",
      "[ 26/100] train_loss: 0.07054 valid_loss: 0.08196 test_loss: 0.04588 \n",
      "[ 27/100] train_loss: 0.07325 valid_loss: 0.08408 test_loss: 0.04675 \n",
      "[ 28/100] train_loss: 0.07407 valid_loss: 0.07983 test_loss: 0.04437 \n",
      "Validation loss decreased (0.081660 --> 0.079827).  Saving model ...\n",
      "[ 29/100] train_loss: 0.07365 valid_loss: 0.09308 test_loss: 0.04443 \n",
      "[ 30/100] train_loss: 0.07209 valid_loss: 0.08481 test_loss: 0.04336 \n",
      "[ 31/100] train_loss: 0.07090 valid_loss: 0.08535 test_loss: 0.04181 \n",
      "[ 32/100] train_loss: 0.07310 valid_loss: 0.08471 test_loss: 0.04328 \n",
      "[ 33/100] train_loss: 0.06959 valid_loss: 0.08681 test_loss: 0.04141 \n",
      "[ 34/100] train_loss: 0.07028 valid_loss: 0.08180 test_loss: 0.04201 \n",
      "[ 35/100] train_loss: 0.07033 valid_loss: 0.08251 test_loss: 0.04176 \n",
      "[ 36/100] train_loss: 0.07024 valid_loss: 0.08461 test_loss: 0.04078 \n",
      "[ 37/100] train_loss: 0.06303 valid_loss: 0.08322 test_loss: 0.03837 \n",
      "[ 38/100] train_loss: 0.07171 valid_loss: 0.08950 test_loss: 0.04472 \n",
      "[ 39/100] train_loss: 0.06612 valid_loss: 0.08104 test_loss: 0.03823 \n",
      "[ 40/100] train_loss: 0.06910 valid_loss: 0.08077 test_loss: 0.04018 \n",
      "[ 41/100] train_loss: 0.06916 valid_loss: 0.07987 test_loss: 0.04066 \n",
      "[ 42/100] train_loss: 0.06599 valid_loss: 0.08063 test_loss: 0.03805 \n",
      "[ 43/100] train_loss: 0.06694 valid_loss: 0.08371 test_loss: 0.03890 \n",
      "[ 44/100] train_loss: 0.06728 valid_loss: 0.08495 test_loss: 0.04029 \n",
      "[ 45/100] train_loss: 0.06517 valid_loss: 0.07999 test_loss: 0.03812 \n",
      "[ 46/100] train_loss: 0.06226 valid_loss: 0.08614 test_loss: 0.03982 \n",
      "[ 47/100] train_loss: 0.06719 valid_loss: 0.08095 test_loss: 0.03773 \n",
      "[ 48/100] train_loss: 0.06292 valid_loss: 0.08137 test_loss: 0.03633 \n",
      "[ 49/100] train_loss: 0.06781 valid_loss: 0.08115 test_loss: 0.03726 \n",
      "[ 50/100] train_loss: 0.06004 valid_loss: 0.08131 test_loss: 0.03682 \n",
      "[ 51/100] train_loss: 0.06318 valid_loss: 0.07637 test_loss: 0.03417 \n",
      "Validation loss decreased (0.079827 --> 0.076372).  Saving model ...\n",
      "[ 52/100] train_loss: 0.06362 valid_loss: 0.08199 test_loss: 0.04071 \n",
      "[ 53/100] train_loss: 0.06203 valid_loss: 0.08306 test_loss: 0.03704 \n",
      "[ 54/100] train_loss: 0.06296 valid_loss: 0.08251 test_loss: 0.03531 \n",
      "[ 55/100] train_loss: 0.06506 valid_loss: 0.07855 test_loss: 0.03521 \n",
      "[ 56/100] train_loss: 0.06426 valid_loss: 0.07748 test_loss: 0.03827 \n",
      "[ 57/100] train_loss: 0.06253 valid_loss: 0.07944 test_loss: 0.03584 \n",
      "[ 58/100] train_loss: 0.06497 valid_loss: 0.08192 test_loss: 0.03727 \n",
      "[ 59/100] train_loss: 0.06541 valid_loss: 0.08227 test_loss: 0.03706 \n",
      "[ 60/100] train_loss: 0.06136 valid_loss: 0.08644 test_loss: 0.03844 \n",
      "[ 61/100] train_loss: 0.06196 valid_loss: 0.07705 test_loss: 0.03189 \n",
      "[ 62/100] train_loss: 0.06377 valid_loss: 0.08453 test_loss: 0.03927 \n",
      "[ 63/100] train_loss: 0.06176 valid_loss: 0.07633 test_loss: 0.03337 \n",
      "Validation loss decreased (0.076372 --> 0.076325).  Saving model ...\n",
      "[ 64/100] train_loss: 0.05991 valid_loss: 0.07193 test_loss: 0.03663 \n",
      "Validation loss decreased (0.076325 --> 0.071926).  Saving model ...\n",
      "[ 65/100] train_loss: 0.06071 valid_loss: 0.07835 test_loss: 0.03515 \n",
      "[ 66/100] train_loss: 0.05978 valid_loss: 0.08349 test_loss: 0.03619 \n",
      "[ 67/100] train_loss: 0.06098 valid_loss: 0.08507 test_loss: 0.03663 \n",
      "[ 68/100] train_loss: 0.06098 valid_loss: 0.07970 test_loss: 0.03787 \n",
      "[ 69/100] train_loss: 0.05944 valid_loss: 0.07377 test_loss: 0.03412 \n",
      "[ 70/100] train_loss: 0.05909 valid_loss: 0.08197 test_loss: 0.03552 \n",
      "[ 71/100] train_loss: 0.06004 valid_loss: 0.09153 test_loss: 0.03983 \n",
      "[ 72/100] train_loss: 0.06115 valid_loss: 0.07947 test_loss: 0.03480 \n",
      "[ 73/100] train_loss: 0.05824 valid_loss: 0.07251 test_loss: 0.03393 \n",
      "[ 74/100] train_loss: 0.05826 valid_loss: 0.08463 test_loss: 0.03530 \n",
      "[ 75/100] train_loss: 0.05777 valid_loss: 0.08144 test_loss: 0.03463 \n",
      "[ 76/100] train_loss: 0.05228 valid_loss: 0.08045 test_loss: 0.03406 \n",
      "[ 77/100] train_loss: 0.06619 valid_loss: 0.08062 test_loss: 0.03748 \n",
      "[ 78/100] train_loss: 0.05603 valid_loss: 0.08671 test_loss: 0.03817 \n",
      "[ 79/100] train_loss: 0.06102 valid_loss: 0.07851 test_loss: 0.03424 \n",
      "[ 80/100] train_loss: 0.05938 valid_loss: 0.07977 test_loss: 0.03440 \n",
      "[ 81/100] train_loss: 0.05540 valid_loss: 0.07896 test_loss: 0.03309 \n",
      "[ 82/100] train_loss: 0.05690 valid_loss: 0.07965 test_loss: 0.03521 \n",
      "[ 83/100] train_loss: 0.06290 valid_loss: 0.07605 test_loss: 0.03523 \n",
      "[ 84/100] train_loss: 0.05620 valid_loss: 0.07881 test_loss: 0.03303 \n",
      "[ 85/100] train_loss: 0.05442 valid_loss: 0.07660 test_loss: 0.03400 \n",
      "[ 86/100] train_loss: 0.05524 valid_loss: 0.07802 test_loss: 0.03376 \n",
      "[ 87/100] train_loss: 0.05791 valid_loss: 0.07704 test_loss: 0.03594 \n",
      "[ 88/100] train_loss: 0.05484 valid_loss: 0.07949 test_loss: 0.03215 \n",
      "[ 89/100] train_loss: 0.05521 valid_loss: 0.08193 test_loss: 0.03774 \n",
      "[ 90/100] train_loss: 0.05666 valid_loss: 0.07769 test_loss: 0.03164 \n",
      "[ 91/100] train_loss: 0.05889 valid_loss: 0.08801 test_loss: 0.03812 \n",
      "[ 92/100] train_loss: 0.05893 valid_loss: 0.07896 test_loss: 0.03590 \n",
      "[ 93/100] train_loss: 0.05567 valid_loss: 0.09042 test_loss: 0.03886 \n",
      "[ 94/100] train_loss: 0.05662 valid_loss: 0.07399 test_loss: 0.03166 \n",
      "[ 95/100] train_loss: 0.05827 valid_loss: 0.07828 test_loss: 0.03717 \n",
      "[ 96/100] train_loss: 0.05174 valid_loss: 0.08140 test_loss: 0.03293 \n",
      "[ 97/100] train_loss: 0.05804 valid_loss: 0.07877 test_loss: 0.03621 \n",
      "[ 98/100] train_loss: 0.05562 valid_loss: 0.07770 test_loss: 0.03126 \n",
      "[ 99/100] train_loss: 0.05882 valid_loss: 0.09204 test_loss: 0.03999 \n",
      "[100/100] train_loss: 0.05769 valid_loss: 0.08212 test_loss: 0.03523 \n",
      "TRAINING MODEL 11\n",
      "[  1/100] train_loss: 0.18526 valid_loss: 0.19123 test_loss: 0.17887 \n",
      "Validation loss decreased (inf --> 0.191229).  Saving model ...\n",
      "[  2/100] train_loss: 0.18012 valid_loss: 0.18684 test_loss: 0.17366 \n",
      "Validation loss decreased (0.191229 --> 0.186845).  Saving model ...\n",
      "[  3/100] train_loss: 0.17538 valid_loss: 0.18161 test_loss: 0.16602 \n",
      "Validation loss decreased (0.186845 --> 0.181615).  Saving model ...\n",
      "[  4/100] train_loss: 0.17024 valid_loss: 0.17465 test_loss: 0.15918 \n",
      "Validation loss decreased (0.181615 --> 0.174655).  Saving model ...\n",
      "[  5/100] train_loss: 0.16346 valid_loss: 0.16738 test_loss: 0.15046 \n",
      "Validation loss decreased (0.174655 --> 0.167382).  Saving model ...\n",
      "[  6/100] train_loss: 0.15484 valid_loss: 0.16167 test_loss: 0.14116 \n",
      "Validation loss decreased (0.167382 --> 0.161666).  Saving model ...\n",
      "[  7/100] train_loss: 0.14597 valid_loss: 0.14841 test_loss: 0.13030 \n",
      "Validation loss decreased (0.161666 --> 0.148411).  Saving model ...\n",
      "[  8/100] train_loss: 0.13563 valid_loss: 0.14353 test_loss: 0.12148 \n",
      "Validation loss decreased (0.148411 --> 0.143529).  Saving model ...\n",
      "[  9/100] train_loss: 0.12649 valid_loss: 0.12616 test_loss: 0.10895 \n",
      "Validation loss decreased (0.143529 --> 0.126160).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11858 valid_loss: 0.12087 test_loss: 0.09956 \n",
      "Validation loss decreased (0.126160 --> 0.120866).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11232 valid_loss: 0.11128 test_loss: 0.09015 \n",
      "Validation loss decreased (0.120866 --> 0.111284).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10572 valid_loss: 0.10412 test_loss: 0.08276 \n",
      "Validation loss decreased (0.111284 --> 0.104116).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10099 valid_loss: 0.09977 test_loss: 0.07778 \n",
      "Validation loss decreased (0.104116 --> 0.099768).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09640 valid_loss: 0.09759 test_loss: 0.07318 \n",
      "Validation loss decreased (0.099768 --> 0.097586).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08849 valid_loss: 0.09690 test_loss: 0.06720 \n",
      "Validation loss decreased (0.097586 --> 0.096897).  Saving model ...\n",
      "[ 16/100] train_loss: 0.09080 valid_loss: 0.09673 test_loss: 0.06513 \n",
      "Validation loss decreased (0.096897 --> 0.096732).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08542 valid_loss: 0.08853 test_loss: 0.06130 \n",
      "Validation loss decreased (0.096732 --> 0.088531).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08577 valid_loss: 0.08644 test_loss: 0.05896 \n",
      "Validation loss decreased (0.088531 --> 0.086435).  Saving model ...\n",
      "[ 19/100] train_loss: 0.07936 valid_loss: 0.09276 test_loss: 0.05702 \n",
      "[ 20/100] train_loss: 0.08294 valid_loss: 0.09045 test_loss: 0.05432 \n",
      "[ 21/100] train_loss: 0.07606 valid_loss: 0.08274 test_loss: 0.05474 \n",
      "Validation loss decreased (0.086435 --> 0.082741).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07882 valid_loss: 0.08942 test_loss: 0.05092 \n",
      "[ 23/100] train_loss: 0.07626 valid_loss: 0.08341 test_loss: 0.05088 \n",
      "[ 24/100] train_loss: 0.07818 valid_loss: 0.08696 test_loss: 0.05044 \n",
      "[ 25/100] train_loss: 0.07937 valid_loss: 0.08558 test_loss: 0.04796 \n",
      "[ 26/100] train_loss: 0.07566 valid_loss: 0.08233 test_loss: 0.04721 \n",
      "Validation loss decreased (0.082741 --> 0.082329).  Saving model ...\n",
      "[ 27/100] train_loss: 0.07260 valid_loss: 0.08278 test_loss: 0.04698 \n",
      "[ 28/100] train_loss: 0.07548 valid_loss: 0.08424 test_loss: 0.04662 \n",
      "[ 29/100] train_loss: 0.06756 valid_loss: 0.08389 test_loss: 0.04332 \n",
      "[ 30/100] train_loss: 0.06851 valid_loss: 0.08009 test_loss: 0.04467 \n",
      "Validation loss decreased (0.082329 --> 0.080092).  Saving model ...\n",
      "[ 31/100] train_loss: 0.07048 valid_loss: 0.08705 test_loss: 0.04379 \n",
      "[ 32/100] train_loss: 0.06831 valid_loss: 0.08630 test_loss: 0.04247 \n",
      "[ 33/100] train_loss: 0.06729 valid_loss: 0.08250 test_loss: 0.04193 \n",
      "[ 34/100] train_loss: 0.06968 valid_loss: 0.07998 test_loss: 0.04208 \n",
      "Validation loss decreased (0.080092 --> 0.079976).  Saving model ...\n",
      "[ 35/100] train_loss: 0.07153 valid_loss: 0.07957 test_loss: 0.04195 \n",
      "Validation loss decreased (0.079976 --> 0.079575).  Saving model ...\n",
      "[ 36/100] train_loss: 0.06307 valid_loss: 0.08735 test_loss: 0.04093 \n",
      "[ 37/100] train_loss: 0.06957 valid_loss: 0.08790 test_loss: 0.04397 \n",
      "[ 38/100] train_loss: 0.06778 valid_loss: 0.08324 test_loss: 0.04023 \n",
      "[ 39/100] train_loss: 0.06906 valid_loss: 0.08414 test_loss: 0.04307 \n",
      "[ 40/100] train_loss: 0.06947 valid_loss: 0.07889 test_loss: 0.03824 \n",
      "Validation loss decreased (0.079575 --> 0.078892).  Saving model ...\n",
      "[ 41/100] train_loss: 0.06354 valid_loss: 0.08048 test_loss: 0.03971 \n",
      "[ 42/100] train_loss: 0.06592 valid_loss: 0.08015 test_loss: 0.03802 \n",
      "[ 43/100] train_loss: 0.07140 valid_loss: 0.08333 test_loss: 0.04229 \n",
      "[ 44/100] train_loss: 0.06477 valid_loss: 0.08679 test_loss: 0.03817 \n",
      "[ 45/100] train_loss: 0.06604 valid_loss: 0.08962 test_loss: 0.04067 \n",
      "[ 46/100] train_loss: 0.06292 valid_loss: 0.08437 test_loss: 0.03728 \n",
      "[ 47/100] train_loss: 0.06225 valid_loss: 0.08706 test_loss: 0.03932 \n",
      "[ 48/100] train_loss: 0.06472 valid_loss: 0.07573 test_loss: 0.03737 \n",
      "Validation loss decreased (0.078892 --> 0.075733).  Saving model ...\n",
      "[ 49/100] train_loss: 0.06924 valid_loss: 0.08646 test_loss: 0.03803 \n",
      "[ 50/100] train_loss: 0.06728 valid_loss: 0.07724 test_loss: 0.03619 \n",
      "[ 51/100] train_loss: 0.06639 valid_loss: 0.09133 test_loss: 0.04082 \n",
      "[ 52/100] train_loss: 0.06590 valid_loss: 0.08966 test_loss: 0.04127 \n",
      "[ 53/100] train_loss: 0.06126 valid_loss: 0.07913 test_loss: 0.03377 \n",
      "[ 54/100] train_loss: 0.06259 valid_loss: 0.08606 test_loss: 0.04060 \n",
      "[ 55/100] train_loss: 0.06701 valid_loss: 0.08295 test_loss: 0.03560 \n",
      "[ 56/100] train_loss: 0.05721 valid_loss: 0.08350 test_loss: 0.03612 \n",
      "[ 57/100] train_loss: 0.06548 valid_loss: 0.08120 test_loss: 0.03832 \n",
      "[ 58/100] train_loss: 0.06395 valid_loss: 0.09382 test_loss: 0.03842 \n",
      "[ 59/100] train_loss: 0.06490 valid_loss: 0.08122 test_loss: 0.03764 \n",
      "[ 60/100] train_loss: 0.06131 valid_loss: 0.08067 test_loss: 0.03338 \n",
      "[ 61/100] train_loss: 0.05699 valid_loss: 0.08892 test_loss: 0.03749 \n",
      "[ 62/100] train_loss: 0.05751 valid_loss: 0.09037 test_loss: 0.03451 \n",
      "[ 63/100] train_loss: 0.06131 valid_loss: 0.08838 test_loss: 0.03692 \n",
      "[ 64/100] train_loss: 0.06254 valid_loss: 0.08266 test_loss: 0.03414 \n",
      "[ 65/100] train_loss: 0.06204 valid_loss: 0.08919 test_loss: 0.03891 \n",
      "[ 66/100] train_loss: 0.06141 valid_loss: 0.09047 test_loss: 0.03550 \n",
      "[ 67/100] train_loss: 0.06346 valid_loss: 0.09059 test_loss: 0.03979 \n",
      "[ 68/100] train_loss: 0.06117 valid_loss: 0.08495 test_loss: 0.03348 \n",
      "[ 69/100] train_loss: 0.06109 valid_loss: 0.08880 test_loss: 0.03771 \n",
      "[ 70/100] train_loss: 0.06056 valid_loss: 0.08304 test_loss: 0.03311 \n",
      "[ 71/100] train_loss: 0.06254 valid_loss: 0.08731 test_loss: 0.03674 \n",
      "[ 72/100] train_loss: 0.05931 valid_loss: 0.09411 test_loss: 0.03874 \n",
      "[ 73/100] train_loss: 0.05336 valid_loss: 0.09311 test_loss: 0.03435 \n",
      "[ 74/100] train_loss: 0.05960 valid_loss: 0.09084 test_loss: 0.03587 \n",
      "[ 75/100] train_loss: 0.05777 valid_loss: 0.08715 test_loss: 0.03838 \n",
      "[ 76/100] train_loss: 0.05695 valid_loss: 0.08981 test_loss: 0.03656 \n",
      "[ 77/100] train_loss: 0.05413 valid_loss: 0.09468 test_loss: 0.03736 \n",
      "[ 78/100] train_loss: 0.05853 valid_loss: 0.09217 test_loss: 0.03810 \n",
      "[ 79/100] train_loss: 0.05601 valid_loss: 0.09135 test_loss: 0.03612 \n",
      "[ 80/100] train_loss: 0.05645 valid_loss: 0.08516 test_loss: 0.03187 \n",
      "[ 81/100] train_loss: 0.05566 valid_loss: 0.09127 test_loss: 0.03561 \n",
      "[ 82/100] train_loss: 0.05706 valid_loss: 0.09639 test_loss: 0.03762 \n",
      "[ 83/100] train_loss: 0.06029 valid_loss: 0.08508 test_loss: 0.03530 \n",
      "[ 84/100] train_loss: 0.05950 valid_loss: 0.09354 test_loss: 0.03879 \n",
      "[ 85/100] train_loss: 0.05306 valid_loss: 0.09458 test_loss: 0.03723 \n",
      "[ 86/100] train_loss: 0.05946 valid_loss: 0.08996 test_loss: 0.03540 \n",
      "[ 87/100] train_loss: 0.05571 valid_loss: 0.08493 test_loss: 0.03326 \n",
      "[ 88/100] train_loss: 0.05796 valid_loss: 0.08621 test_loss: 0.03199 \n",
      "[ 89/100] train_loss: 0.05970 valid_loss: 0.10924 test_loss: 0.04854 \n",
      "[ 90/100] train_loss: 0.05659 valid_loss: 0.08961 test_loss: 0.03220 \n",
      "[ 91/100] train_loss: 0.06269 valid_loss: 0.10126 test_loss: 0.04507 \n",
      "[ 92/100] train_loss: 0.05489 valid_loss: 0.08722 test_loss: 0.02888 \n",
      "[ 93/100] train_loss: 0.05489 valid_loss: 0.09421 test_loss: 0.03623 \n",
      "[ 94/100] train_loss: 0.05575 valid_loss: 0.10035 test_loss: 0.03618 \n",
      "[ 95/100] train_loss: 0.05780 valid_loss: 0.09213 test_loss: 0.03734 \n",
      "[ 96/100] train_loss: 0.05214 valid_loss: 0.08449 test_loss: 0.02852 \n",
      "[ 97/100] train_loss: 0.05505 valid_loss: 0.10200 test_loss: 0.04098 \n",
      "[ 98/100] train_loss: 0.05416 valid_loss: 0.09791 test_loss: 0.03595 \n",
      "[ 99/100] train_loss: 0.05406 valid_loss: 0.08670 test_loss: 0.03204 \n",
      "[100/100] train_loss: 0.05127 valid_loss: 0.09542 test_loss: 0.03514 \n",
      "TRAINING MODEL 12\n",
      "[  1/100] train_loss: 0.18440 valid_loss: 0.18995 test_loss: 0.18088 \n",
      "Validation loss decreased (inf --> 0.189950).  Saving model ...\n",
      "[  2/100] train_loss: 0.17920 valid_loss: 0.18547 test_loss: 0.17466 \n",
      "Validation loss decreased (0.189950 --> 0.185472).  Saving model ...\n",
      "[  3/100] train_loss: 0.17364 valid_loss: 0.17949 test_loss: 0.16585 \n",
      "Validation loss decreased (0.185472 --> 0.179491).  Saving model ...\n",
      "[  4/100] train_loss: 0.16742 valid_loss: 0.17403 test_loss: 0.15656 \n",
      "Validation loss decreased (0.179491 --> 0.174026).  Saving model ...\n",
      "[  5/100] train_loss: 0.15999 valid_loss: 0.16676 test_loss: 0.14859 \n",
      "Validation loss decreased (0.174026 --> 0.166756).  Saving model ...\n",
      "[  6/100] train_loss: 0.15171 valid_loss: 0.15952 test_loss: 0.14091 \n",
      "Validation loss decreased (0.166756 --> 0.159516).  Saving model ...\n",
      "[  7/100] train_loss: 0.14254 valid_loss: 0.15322 test_loss: 0.13006 \n",
      "Validation loss decreased (0.159516 --> 0.153224).  Saving model ...\n",
      "[  8/100] train_loss: 0.13516 valid_loss: 0.14553 test_loss: 0.12143 \n",
      "Validation loss decreased (0.153224 --> 0.145528).  Saving model ...\n",
      "[  9/100] train_loss: 0.12685 valid_loss: 0.13215 test_loss: 0.11061 \n",
      "Validation loss decreased (0.145528 --> 0.132147).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11883 valid_loss: 0.12949 test_loss: 0.10226 \n",
      "Validation loss decreased (0.132147 --> 0.129492).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11273 valid_loss: 0.11828 test_loss: 0.09317 \n",
      "Validation loss decreased (0.129492 --> 0.118278).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10651 valid_loss: 0.11227 test_loss: 0.08652 \n",
      "Validation loss decreased (0.118278 --> 0.112267).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10068 valid_loss: 0.10853 test_loss: 0.07998 \n",
      "Validation loss decreased (0.112267 --> 0.108531).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09454 valid_loss: 0.09982 test_loss: 0.07316 \n",
      "Validation loss decreased (0.108531 --> 0.099817).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09162 valid_loss: 0.10250 test_loss: 0.06953 \n",
      "[ 16/100] train_loss: 0.08966 valid_loss: 0.09417 test_loss: 0.06590 \n",
      "Validation loss decreased (0.099817 --> 0.094173).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08597 valid_loss: 0.09240 test_loss: 0.06186 \n",
      "Validation loss decreased (0.094173 --> 0.092395).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08973 valid_loss: 0.08992 test_loss: 0.06280 \n",
      "Validation loss decreased (0.092395 --> 0.089920).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08080 valid_loss: 0.09475 test_loss: 0.05759 \n",
      "[ 20/100] train_loss: 0.08074 valid_loss: 0.09009 test_loss: 0.05620 \n",
      "[ 21/100] train_loss: 0.08080 valid_loss: 0.08641 test_loss: 0.05339 \n",
      "Validation loss decreased (0.089920 --> 0.086407).  Saving model ...\n",
      "[ 22/100] train_loss: 0.08191 valid_loss: 0.08626 test_loss: 0.05302 \n",
      "Validation loss decreased (0.086407 --> 0.086259).  Saving model ...\n",
      "[ 23/100] train_loss: 0.08105 valid_loss: 0.08724 test_loss: 0.05183 \n",
      "[ 24/100] train_loss: 0.08116 valid_loss: 0.08873 test_loss: 0.05148 \n",
      "[ 25/100] train_loss: 0.08108 valid_loss: 0.08551 test_loss: 0.05040 \n",
      "Validation loss decreased (0.086259 --> 0.085511).  Saving model ...\n",
      "[ 26/100] train_loss: 0.07658 valid_loss: 0.08341 test_loss: 0.05176 \n",
      "Validation loss decreased (0.085511 --> 0.083413).  Saving model ...\n",
      "[ 27/100] train_loss: 0.07567 valid_loss: 0.08771 test_loss: 0.04790 \n",
      "[ 28/100] train_loss: 0.07526 valid_loss: 0.08717 test_loss: 0.04660 \n",
      "[ 29/100] train_loss: 0.07644 valid_loss: 0.09738 test_loss: 0.04820 \n",
      "[ 30/100] train_loss: 0.07590 valid_loss: 0.08323 test_loss: 0.04686 \n",
      "Validation loss decreased (0.083413 --> 0.083228).  Saving model ...\n",
      "[ 31/100] train_loss: 0.06980 valid_loss: 0.07989 test_loss: 0.04853 \n",
      "Validation loss decreased (0.083228 --> 0.079886).  Saving model ...\n",
      "[ 32/100] train_loss: 0.07326 valid_loss: 0.08233 test_loss: 0.04498 \n",
      "[ 33/100] train_loss: 0.07169 valid_loss: 0.08980 test_loss: 0.04715 \n",
      "[ 34/100] train_loss: 0.07026 valid_loss: 0.08072 test_loss: 0.04376 \n",
      "[ 35/100] train_loss: 0.07329 valid_loss: 0.08037 test_loss: 0.04470 \n",
      "[ 36/100] train_loss: 0.06675 valid_loss: 0.08330 test_loss: 0.04219 \n",
      "[ 37/100] train_loss: 0.07034 valid_loss: 0.08191 test_loss: 0.04421 \n",
      "[ 38/100] train_loss: 0.06694 valid_loss: 0.08860 test_loss: 0.04222 \n",
      "[ 39/100] train_loss: 0.07154 valid_loss: 0.09455 test_loss: 0.04639 \n",
      "[ 40/100] train_loss: 0.06810 valid_loss: 0.08417 test_loss: 0.04264 \n",
      "[ 41/100] train_loss: 0.06458 valid_loss: 0.08021 test_loss: 0.03901 \n",
      "[ 42/100] train_loss: 0.06435 valid_loss: 0.08353 test_loss: 0.04349 \n",
      "[ 43/100] train_loss: 0.07012 valid_loss: 0.08401 test_loss: 0.03904 \n",
      "[ 44/100] train_loss: 0.06133 valid_loss: 0.08657 test_loss: 0.03873 \n",
      "[ 45/100] train_loss: 0.06668 valid_loss: 0.08294 test_loss: 0.03985 \n",
      "[ 46/100] train_loss: 0.06244 valid_loss: 0.08707 test_loss: 0.03802 \n",
      "[ 47/100] train_loss: 0.06304 valid_loss: 0.08330 test_loss: 0.03788 \n",
      "[ 48/100] train_loss: 0.06072 valid_loss: 0.08924 test_loss: 0.03948 \n",
      "[ 49/100] train_loss: 0.06095 valid_loss: 0.09207 test_loss: 0.04116 \n",
      "[ 50/100] train_loss: 0.06695 valid_loss: 0.08020 test_loss: 0.04066 \n",
      "[ 51/100] train_loss: 0.06494 valid_loss: 0.07676 test_loss: 0.03896 \n",
      "Validation loss decreased (0.079886 --> 0.076757).  Saving model ...\n",
      "[ 52/100] train_loss: 0.06319 valid_loss: 0.08813 test_loss: 0.03723 \n",
      "[ 53/100] train_loss: 0.06350 valid_loss: 0.08692 test_loss: 0.04021 \n",
      "[ 54/100] train_loss: 0.05820 valid_loss: 0.09353 test_loss: 0.03815 \n",
      "[ 55/100] train_loss: 0.06433 valid_loss: 0.08286 test_loss: 0.04349 \n",
      "[ 56/100] train_loss: 0.06755 valid_loss: 0.07833 test_loss: 0.03710 \n",
      "[ 57/100] train_loss: 0.06275 valid_loss: 0.08325 test_loss: 0.03757 \n",
      "[ 58/100] train_loss: 0.06199 valid_loss: 0.09662 test_loss: 0.04271 \n",
      "[ 59/100] train_loss: 0.05820 valid_loss: 0.09799 test_loss: 0.04100 \n",
      "[ 60/100] train_loss: 0.05872 valid_loss: 0.07729 test_loss: 0.03607 \n",
      "[ 61/100] train_loss: 0.06234 valid_loss: 0.08476 test_loss: 0.04120 \n",
      "[ 62/100] train_loss: 0.05838 valid_loss: 0.08046 test_loss: 0.03227 \n",
      "[ 63/100] train_loss: 0.05902 valid_loss: 0.08815 test_loss: 0.03969 \n",
      "[ 64/100] train_loss: 0.06067 valid_loss: 0.08707 test_loss: 0.03777 \n",
      "[ 65/100] train_loss: 0.06035 valid_loss: 0.07960 test_loss: 0.03497 \n",
      "[ 66/100] train_loss: 0.05835 valid_loss: 0.08752 test_loss: 0.03454 \n",
      "[ 67/100] train_loss: 0.06234 valid_loss: 0.07518 test_loss: 0.03899 \n",
      "Validation loss decreased (0.076757 --> 0.075181).  Saving model ...\n",
      "[ 68/100] train_loss: 0.05780 valid_loss: 0.08138 test_loss: 0.03426 \n",
      "[ 69/100] train_loss: 0.05921 valid_loss: 0.08865 test_loss: 0.03773 \n",
      "[ 70/100] train_loss: 0.05853 valid_loss: 0.07754 test_loss: 0.03507 \n",
      "[ 71/100] train_loss: 0.05390 valid_loss: 0.08234 test_loss: 0.03227 \n",
      "[ 72/100] train_loss: 0.05685 valid_loss: 0.07404 test_loss: 0.03612 \n",
      "Validation loss decreased (0.075181 --> 0.074041).  Saving model ...\n",
      "[ 73/100] train_loss: 0.06363 valid_loss: 0.08906 test_loss: 0.03944 \n",
      "[ 74/100] train_loss: 0.05905 valid_loss: 0.08871 test_loss: 0.03478 \n",
      "[ 75/100] train_loss: 0.05626 valid_loss: 0.07908 test_loss: 0.03188 \n",
      "[ 76/100] train_loss: 0.05746 valid_loss: 0.08055 test_loss: 0.03461 \n",
      "[ 77/100] train_loss: 0.05846 valid_loss: 0.08251 test_loss: 0.03906 \n",
      "[ 78/100] train_loss: 0.05793 valid_loss: 0.07597 test_loss: 0.03169 \n",
      "[ 79/100] train_loss: 0.05785 valid_loss: 0.08124 test_loss: 0.03588 \n",
      "[ 80/100] train_loss: 0.05999 valid_loss: 0.07984 test_loss: 0.03444 \n",
      "[ 81/100] train_loss: 0.05935 valid_loss: 0.09497 test_loss: 0.04011 \n",
      "[ 82/100] train_loss: 0.06144 valid_loss: 0.08178 test_loss: 0.03532 \n",
      "[ 83/100] train_loss: 0.05902 valid_loss: 0.07590 test_loss: 0.03615 \n",
      "[ 84/100] train_loss: 0.05641 valid_loss: 0.08813 test_loss: 0.03550 \n",
      "[ 85/100] train_loss: 0.05593 valid_loss: 0.08253 test_loss: 0.03388 \n",
      "[ 86/100] train_loss: 0.05605 valid_loss: 0.07800 test_loss: 0.03663 \n",
      "[ 87/100] train_loss: 0.06071 valid_loss: 0.07662 test_loss: 0.03604 \n",
      "[ 88/100] train_loss: 0.05871 valid_loss: 0.08388 test_loss: 0.03422 \n",
      "[ 89/100] train_loss: 0.05297 valid_loss: 0.08227 test_loss: 0.03430 \n",
      "[ 90/100] train_loss: 0.05556 valid_loss: 0.08026 test_loss: 0.03558 \n",
      "[ 91/100] train_loss: 0.05731 valid_loss: 0.08875 test_loss: 0.03604 \n",
      "[ 92/100] train_loss: 0.05641 valid_loss: 0.07640 test_loss: 0.03590 \n",
      "[ 93/100] train_loss: 0.05272 valid_loss: 0.08509 test_loss: 0.03201 \n",
      "[ 94/100] train_loss: 0.05570 valid_loss: 0.07570 test_loss: 0.03682 \n",
      "[ 95/100] train_loss: 0.05832 valid_loss: 0.08877 test_loss: 0.03877 \n",
      "[ 96/100] train_loss: 0.05752 valid_loss: 0.08682 test_loss: 0.03386 \n",
      "[ 97/100] train_loss: 0.05508 valid_loss: 0.08473 test_loss: 0.03323 \n",
      "[ 98/100] train_loss: 0.05170 valid_loss: 0.08692 test_loss: 0.03455 \n",
      "[ 99/100] train_loss: 0.05343 valid_loss: 0.08419 test_loss: 0.03500 \n",
      "[100/100] train_loss: 0.05627 valid_loss: 0.07801 test_loss: 0.03570 \n",
      "TRAINING MODEL 13\n",
      "[  1/100] train_loss: 0.18465 valid_loss: 0.18768 test_loss: 0.18073 \n",
      "Validation loss decreased (inf --> 0.187676).  Saving model ...\n",
      "[  2/100] train_loss: 0.17789 valid_loss: 0.18328 test_loss: 0.17239 \n",
      "Validation loss decreased (0.187676 --> 0.183280).  Saving model ...\n",
      "[  3/100] train_loss: 0.17306 valid_loss: 0.17774 test_loss: 0.16243 \n",
      "Validation loss decreased (0.183280 --> 0.177744).  Saving model ...\n",
      "[  4/100] train_loss: 0.16707 valid_loss: 0.17003 test_loss: 0.15336 \n",
      "Validation loss decreased (0.177744 --> 0.170034).  Saving model ...\n",
      "[  5/100] train_loss: 0.15805 valid_loss: 0.16352 test_loss: 0.14414 \n",
      "Validation loss decreased (0.170034 --> 0.163518).  Saving model ...\n",
      "[  6/100] train_loss: 0.14929 valid_loss: 0.15595 test_loss: 0.13474 \n",
      "Validation loss decreased (0.163518 --> 0.155947).  Saving model ...\n",
      "[  7/100] train_loss: 0.14175 valid_loss: 0.14565 test_loss: 0.12477 \n",
      "Validation loss decreased (0.155947 --> 0.145654).  Saving model ...\n",
      "[  8/100] train_loss: 0.13423 valid_loss: 0.13642 test_loss: 0.11358 \n",
      "Validation loss decreased (0.145654 --> 0.136418).  Saving model ...\n",
      "[  9/100] train_loss: 0.12402 valid_loss: 0.13068 test_loss: 0.10571 \n",
      "Validation loss decreased (0.136418 --> 0.130683).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11633 valid_loss: 0.12018 test_loss: 0.09655 \n",
      "Validation loss decreased (0.130683 --> 0.120185).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11067 valid_loss: 0.11565 test_loss: 0.08910 \n",
      "Validation loss decreased (0.120185 --> 0.115649).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10663 valid_loss: 0.10708 test_loss: 0.08224 \n",
      "Validation loss decreased (0.115649 --> 0.107076).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10026 valid_loss: 0.10482 test_loss: 0.07668 \n",
      "Validation loss decreased (0.107076 --> 0.104825).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09651 valid_loss: 0.10173 test_loss: 0.07192 \n",
      "Validation loss decreased (0.104825 --> 0.101728).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08737 valid_loss: 0.09932 test_loss: 0.06697 \n",
      "Validation loss decreased (0.101728 --> 0.099315).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08867 valid_loss: 0.09767 test_loss: 0.06288 \n",
      "Validation loss decreased (0.099315 --> 0.097671).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08854 valid_loss: 0.09584 test_loss: 0.06083 \n",
      "Validation loss decreased (0.097671 --> 0.095840).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08221 valid_loss: 0.09185 test_loss: 0.05778 \n",
      "Validation loss decreased (0.095840 --> 0.091849).  Saving model ...\n",
      "[ 19/100] train_loss: 0.07884 valid_loss: 0.08937 test_loss: 0.05442 \n",
      "Validation loss decreased (0.091849 --> 0.089367).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08017 valid_loss: 0.08684 test_loss: 0.05483 \n",
      "Validation loss decreased (0.089367 --> 0.086840).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07725 valid_loss: 0.08734 test_loss: 0.05131 \n",
      "[ 22/100] train_loss: 0.07642 valid_loss: 0.08792 test_loss: 0.05004 \n",
      "[ 23/100] train_loss: 0.08123 valid_loss: 0.08526 test_loss: 0.05063 \n",
      "Validation loss decreased (0.086840 --> 0.085263).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07481 valid_loss: 0.08752 test_loss: 0.04872 \n",
      "[ 25/100] train_loss: 0.07450 valid_loss: 0.08186 test_loss: 0.04966 \n",
      "Validation loss decreased (0.085263 --> 0.081856).  Saving model ...\n",
      "[ 26/100] train_loss: 0.07707 valid_loss: 0.08407 test_loss: 0.04743 \n",
      "[ 27/100] train_loss: 0.07554 valid_loss: 0.09158 test_loss: 0.04676 \n",
      "[ 28/100] train_loss: 0.07521 valid_loss: 0.08384 test_loss: 0.04564 \n",
      "[ 29/100] train_loss: 0.07185 valid_loss: 0.08653 test_loss: 0.04473 \n",
      "[ 30/100] train_loss: 0.07017 valid_loss: 0.08497 test_loss: 0.04218 \n",
      "[ 31/100] train_loss: 0.06824 valid_loss: 0.08293 test_loss: 0.04235 \n",
      "[ 32/100] train_loss: 0.07321 valid_loss: 0.08306 test_loss: 0.04412 \n",
      "[ 33/100] train_loss: 0.06832 valid_loss: 0.08859 test_loss: 0.04136 \n",
      "[ 34/100] train_loss: 0.07166 valid_loss: 0.08139 test_loss: 0.04151 \n",
      "Validation loss decreased (0.081856 --> 0.081394).  Saving model ...\n",
      "[ 35/100] train_loss: 0.06868 valid_loss: 0.07792 test_loss: 0.04130 \n",
      "Validation loss decreased (0.081394 --> 0.077923).  Saving model ...\n",
      "[ 36/100] train_loss: 0.06842 valid_loss: 0.08793 test_loss: 0.03963 \n",
      "[ 37/100] train_loss: 0.06921 valid_loss: 0.08052 test_loss: 0.03911 \n",
      "[ 38/100] train_loss: 0.07022 valid_loss: 0.07623 test_loss: 0.04308 \n",
      "Validation loss decreased (0.077923 --> 0.076228).  Saving model ...\n",
      "[ 39/100] train_loss: 0.06529 valid_loss: 0.07610 test_loss: 0.03877 \n",
      "Validation loss decreased (0.076228 --> 0.076103).  Saving model ...\n",
      "[ 40/100] train_loss: 0.06662 valid_loss: 0.08066 test_loss: 0.03888 \n",
      "[ 41/100] train_loss: 0.06458 valid_loss: 0.07650 test_loss: 0.03733 \n",
      "[ 42/100] train_loss: 0.06467 valid_loss: 0.07616 test_loss: 0.03869 \n",
      "[ 43/100] train_loss: 0.06826 valid_loss: 0.08188 test_loss: 0.03721 \n",
      "[ 44/100] train_loss: 0.06227 valid_loss: 0.08012 test_loss: 0.03686 \n",
      "[ 45/100] train_loss: 0.06784 valid_loss: 0.08442 test_loss: 0.03744 \n",
      "[ 46/100] train_loss: 0.06777 valid_loss: 0.07901 test_loss: 0.03867 \n",
      "[ 47/100] train_loss: 0.06397 valid_loss: 0.07904 test_loss: 0.03704 \n",
      "[ 48/100] train_loss: 0.06569 valid_loss: 0.07469 test_loss: 0.04181 \n",
      "Validation loss decreased (0.076103 --> 0.074693).  Saving model ...\n",
      "[ 49/100] train_loss: 0.06385 valid_loss: 0.08762 test_loss: 0.03589 \n",
      "[ 50/100] train_loss: 0.06531 valid_loss: 0.07421 test_loss: 0.03750 \n",
      "Validation loss decreased (0.074693 --> 0.074213).  Saving model ...\n",
      "[ 51/100] train_loss: 0.06539 valid_loss: 0.08286 test_loss: 0.03810 \n",
      "[ 52/100] train_loss: 0.06415 valid_loss: 0.08497 test_loss: 0.03620 \n",
      "[ 53/100] train_loss: 0.06368 valid_loss: 0.08440 test_loss: 0.03916 \n",
      "[ 54/100] train_loss: 0.05530 valid_loss: 0.08031 test_loss: 0.03378 \n",
      "[ 55/100] train_loss: 0.05978 valid_loss: 0.08222 test_loss: 0.03687 \n",
      "[ 56/100] train_loss: 0.06396 valid_loss: 0.08285 test_loss: 0.03566 \n",
      "[ 57/100] train_loss: 0.05983 valid_loss: 0.08020 test_loss: 0.03366 \n",
      "[ 58/100] train_loss: 0.05811 valid_loss: 0.08016 test_loss: 0.03249 \n",
      "[ 59/100] train_loss: 0.06621 valid_loss: 0.07938 test_loss: 0.03467 \n",
      "[ 60/100] train_loss: 0.06087 valid_loss: 0.08812 test_loss: 0.03492 \n",
      "[ 61/100] train_loss: 0.06700 valid_loss: 0.07525 test_loss: 0.03701 \n",
      "[ 62/100] train_loss: 0.05763 valid_loss: 0.07898 test_loss: 0.03282 \n",
      "[ 63/100] train_loss: 0.06394 valid_loss: 0.07886 test_loss: 0.03622 \n",
      "[ 64/100] train_loss: 0.05764 valid_loss: 0.07961 test_loss: 0.03156 \n",
      "[ 65/100] train_loss: 0.06199 valid_loss: 0.09252 test_loss: 0.04148 \n",
      "[ 66/100] train_loss: 0.06313 valid_loss: 0.08780 test_loss: 0.03613 \n",
      "[ 67/100] train_loss: 0.06001 valid_loss: 0.07838 test_loss: 0.03412 \n",
      "[ 68/100] train_loss: 0.05744 valid_loss: 0.08045 test_loss: 0.03196 \n",
      "[ 69/100] train_loss: 0.05744 valid_loss: 0.08104 test_loss: 0.03525 \n",
      "[ 70/100] train_loss: 0.05890 valid_loss: 0.07981 test_loss: 0.03271 \n",
      "[ 71/100] train_loss: 0.06208 valid_loss: 0.08678 test_loss: 0.03537 \n",
      "[ 72/100] train_loss: 0.05894 valid_loss: 0.07289 test_loss: 0.03406 \n",
      "Validation loss decreased (0.074213 --> 0.072891).  Saving model ...\n",
      "[ 73/100] train_loss: 0.05581 valid_loss: 0.08083 test_loss: 0.03256 \n",
      "[ 74/100] train_loss: 0.05767 valid_loss: 0.08260 test_loss: 0.03185 \n",
      "[ 75/100] train_loss: 0.06318 valid_loss: 0.07896 test_loss: 0.03615 \n",
      "[ 76/100] train_loss: 0.05641 valid_loss: 0.07693 test_loss: 0.03009 \n",
      "[ 77/100] train_loss: 0.06037 valid_loss: 0.07731 test_loss: 0.03198 \n",
      "[ 78/100] train_loss: 0.05598 valid_loss: 0.08395 test_loss: 0.03228 \n",
      "[ 79/100] train_loss: 0.05477 valid_loss: 0.08217 test_loss: 0.03163 \n",
      "[ 80/100] train_loss: 0.05382 valid_loss: 0.08819 test_loss: 0.03209 \n",
      "[ 81/100] train_loss: 0.05897 valid_loss: 0.07269 test_loss: 0.03235 \n",
      "Validation loss decreased (0.072891 --> 0.072689).  Saving model ...\n",
      "[ 82/100] train_loss: 0.05579 valid_loss: 0.08460 test_loss: 0.03311 \n",
      "[ 83/100] train_loss: 0.06186 valid_loss: 0.08987 test_loss: 0.03832 \n",
      "[ 84/100] train_loss: 0.06291 valid_loss: 0.07481 test_loss: 0.03110 \n",
      "[ 85/100] train_loss: 0.05868 valid_loss: 0.07984 test_loss: 0.03308 \n",
      "[ 86/100] train_loss: 0.05358 valid_loss: 0.08442 test_loss: 0.03220 \n",
      "[ 87/100] train_loss: 0.05188 valid_loss: 0.08974 test_loss: 0.03414 \n",
      "[ 88/100] train_loss: 0.06018 valid_loss: 0.08169 test_loss: 0.03502 \n",
      "[ 89/100] train_loss: 0.05855 valid_loss: 0.07576 test_loss: 0.03059 \n",
      "[ 90/100] train_loss: 0.05638 valid_loss: 0.07864 test_loss: 0.03127 \n",
      "[ 91/100] train_loss: 0.05942 valid_loss: 0.08220 test_loss: 0.03549 \n",
      "[ 92/100] train_loss: 0.05572 valid_loss: 0.08119 test_loss: 0.02942 \n",
      "[ 93/100] train_loss: 0.05551 valid_loss: 0.08277 test_loss: 0.03284 \n",
      "[ 94/100] train_loss: 0.05614 valid_loss: 0.08140 test_loss: 0.03273 \n",
      "[ 95/100] train_loss: 0.05288 valid_loss: 0.08787 test_loss: 0.03313 \n",
      "[ 96/100] train_loss: 0.05841 valid_loss: 0.07877 test_loss: 0.02940 \n",
      "[ 97/100] train_loss: 0.05935 valid_loss: 0.08130 test_loss: 0.03328 \n",
      "[ 98/100] train_loss: 0.05650 valid_loss: 0.08611 test_loss: 0.03186 \n",
      "[ 99/100] train_loss: 0.05466 valid_loss: 0.08282 test_loss: 0.02930 \n",
      "[100/100] train_loss: 0.05709 valid_loss: 0.08379 test_loss: 0.03227 \n",
      "TRAINING MODEL 14\n",
      "[  1/100] train_loss: 0.18310 valid_loss: 0.19479 test_loss: 0.17876 \n",
      "Validation loss decreased (inf --> 0.194794).  Saving model ...\n",
      "[  2/100] train_loss: 0.17907 valid_loss: 0.18927 test_loss: 0.17196 \n",
      "Validation loss decreased (0.194794 --> 0.189271).  Saving model ...\n",
      "[  3/100] train_loss: 0.17305 valid_loss: 0.18138 test_loss: 0.16301 \n",
      "Validation loss decreased (0.189271 --> 0.181384).  Saving model ...\n",
      "[  4/100] train_loss: 0.16547 valid_loss: 0.17316 test_loss: 0.15436 \n",
      "Validation loss decreased (0.181384 --> 0.173162).  Saving model ...\n",
      "[  5/100] train_loss: 0.15723 valid_loss: 0.16445 test_loss: 0.14315 \n",
      "Validation loss decreased (0.173162 --> 0.164446).  Saving model ...\n",
      "[  6/100] train_loss: 0.14844 valid_loss: 0.15428 test_loss: 0.13284 \n",
      "Validation loss decreased (0.164446 --> 0.154284).  Saving model ...\n",
      "[  7/100] train_loss: 0.13927 valid_loss: 0.14482 test_loss: 0.12084 \n",
      "Validation loss decreased (0.154284 --> 0.144816).  Saving model ...\n",
      "[  8/100] train_loss: 0.13139 valid_loss: 0.13270 test_loss: 0.10987 \n",
      "Validation loss decreased (0.144816 --> 0.132699).  Saving model ...\n",
      "[  9/100] train_loss: 0.12292 valid_loss: 0.12700 test_loss: 0.10150 \n",
      "Validation loss decreased (0.132699 --> 0.126999).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11272 valid_loss: 0.11383 test_loss: 0.09123 \n",
      "Validation loss decreased (0.126999 --> 0.113828).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10596 valid_loss: 0.10788 test_loss: 0.08396 \n",
      "Validation loss decreased (0.113828 --> 0.107883).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09822 valid_loss: 0.10503 test_loss: 0.07706 \n",
      "Validation loss decreased (0.107883 --> 0.105031).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09421 valid_loss: 0.09581 test_loss: 0.07304 \n",
      "Validation loss decreased (0.105031 --> 0.095810).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09087 valid_loss: 0.09503 test_loss: 0.06627 \n",
      "Validation loss decreased (0.095810 --> 0.095030).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08477 valid_loss: 0.09392 test_loss: 0.06205 \n",
      "Validation loss decreased (0.095030 --> 0.093918).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08610 valid_loss: 0.08927 test_loss: 0.05999 \n",
      "Validation loss decreased (0.093918 --> 0.089273).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08190 valid_loss: 0.08475 test_loss: 0.05962 \n",
      "Validation loss decreased (0.089273 --> 0.084750).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08074 valid_loss: 0.08833 test_loss: 0.05414 \n",
      "[ 19/100] train_loss: 0.07838 valid_loss: 0.09120 test_loss: 0.05183 \n",
      "[ 20/100] train_loss: 0.08258 valid_loss: 0.08673 test_loss: 0.05222 \n",
      "[ 21/100] train_loss: 0.07837 valid_loss: 0.08323 test_loss: 0.05147 \n",
      "Validation loss decreased (0.084750 --> 0.083232).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07630 valid_loss: 0.08530 test_loss: 0.04900 \n",
      "[ 23/100] train_loss: 0.07558 valid_loss: 0.09631 test_loss: 0.04993 \n",
      "[ 24/100] train_loss: 0.07862 valid_loss: 0.08266 test_loss: 0.04962 \n",
      "Validation loss decreased (0.083232 --> 0.082658).  Saving model ...\n",
      "[ 25/100] train_loss: 0.07537 valid_loss: 0.08312 test_loss: 0.04763 \n",
      "[ 26/100] train_loss: 0.07380 valid_loss: 0.08577 test_loss: 0.04474 \n",
      "[ 27/100] train_loss: 0.07007 valid_loss: 0.08328 test_loss: 0.04544 \n",
      "[ 28/100] train_loss: 0.07219 valid_loss: 0.08359 test_loss: 0.04377 \n",
      "[ 29/100] train_loss: 0.06888 valid_loss: 0.08630 test_loss: 0.04348 \n",
      "[ 30/100] train_loss: 0.07242 valid_loss: 0.07920 test_loss: 0.04698 \n",
      "Validation loss decreased (0.082658 --> 0.079197).  Saving model ...\n",
      "[ 31/100] train_loss: 0.06731 valid_loss: 0.08031 test_loss: 0.04111 \n",
      "[ 32/100] train_loss: 0.07071 valid_loss: 0.08325 test_loss: 0.04571 \n",
      "[ 33/100] train_loss: 0.06694 valid_loss: 0.07685 test_loss: 0.04054 \n",
      "Validation loss decreased (0.079197 --> 0.076852).  Saving model ...\n",
      "[ 34/100] train_loss: 0.07161 valid_loss: 0.08380 test_loss: 0.04219 \n",
      "[ 35/100] train_loss: 0.06592 valid_loss: 0.07832 test_loss: 0.04020 \n",
      "[ 36/100] train_loss: 0.06970 valid_loss: 0.07949 test_loss: 0.04035 \n",
      "[ 37/100] train_loss: 0.06799 valid_loss: 0.08072 test_loss: 0.04324 \n",
      "[ 38/100] train_loss: 0.06464 valid_loss: 0.08578 test_loss: 0.03803 \n",
      "[ 39/100] train_loss: 0.06929 valid_loss: 0.08378 test_loss: 0.04088 \n",
      "[ 40/100] train_loss: 0.06754 valid_loss: 0.07702 test_loss: 0.04112 \n",
      "[ 41/100] train_loss: 0.06371 valid_loss: 0.07542 test_loss: 0.03851 \n",
      "Validation loss decreased (0.076852 --> 0.075421).  Saving model ...\n",
      "[ 42/100] train_loss: 0.06321 valid_loss: 0.08708 test_loss: 0.04257 \n",
      "[ 43/100] train_loss: 0.06891 valid_loss: 0.07966 test_loss: 0.03964 \n",
      "[ 44/100] train_loss: 0.06218 valid_loss: 0.07808 test_loss: 0.03972 \n",
      "[ 45/100] train_loss: 0.06538 valid_loss: 0.07559 test_loss: 0.03613 \n",
      "[ 46/100] train_loss: 0.06575 valid_loss: 0.08425 test_loss: 0.03674 \n",
      "[ 47/100] train_loss: 0.06310 valid_loss: 0.07862 test_loss: 0.03613 \n",
      "[ 48/100] train_loss: 0.06328 valid_loss: 0.07481 test_loss: 0.03704 \n",
      "Validation loss decreased (0.075421 --> 0.074805).  Saving model ...\n",
      "[ 49/100] train_loss: 0.06582 valid_loss: 0.08079 test_loss: 0.03943 \n",
      "[ 50/100] train_loss: 0.06573 valid_loss: 0.08262 test_loss: 0.04313 \n",
      "[ 51/100] train_loss: 0.06781 valid_loss: 0.07270 test_loss: 0.04046 \n",
      "Validation loss decreased (0.074805 --> 0.072698).  Saving model ...\n",
      "[ 52/100] train_loss: 0.06175 valid_loss: 0.07771 test_loss: 0.03457 \n",
      "[ 53/100] train_loss: 0.06168 valid_loss: 0.07448 test_loss: 0.03550 \n",
      "[ 54/100] train_loss: 0.06041 valid_loss: 0.08255 test_loss: 0.03772 \n",
      "[ 55/100] train_loss: 0.06207 valid_loss: 0.07724 test_loss: 0.03667 \n",
      "[ 56/100] train_loss: 0.06423 valid_loss: 0.08003 test_loss: 0.03642 \n",
      "[ 57/100] train_loss: 0.06126 valid_loss: 0.07175 test_loss: 0.03811 \n",
      "Validation loss decreased (0.072698 --> 0.071749).  Saving model ...\n",
      "[ 58/100] train_loss: 0.06202 valid_loss: 0.07685 test_loss: 0.03525 \n",
      "[ 59/100] train_loss: 0.06000 valid_loss: 0.09561 test_loss: 0.04455 \n",
      "[ 60/100] train_loss: 0.05976 valid_loss: 0.07474 test_loss: 0.03552 \n",
      "[ 61/100] train_loss: 0.06566 valid_loss: 0.07276 test_loss: 0.03880 \n",
      "[ 62/100] train_loss: 0.05829 valid_loss: 0.08495 test_loss: 0.03621 \n",
      "[ 63/100] train_loss: 0.05440 valid_loss: 0.07399 test_loss: 0.03339 \n",
      "[ 64/100] train_loss: 0.06254 valid_loss: 0.08116 test_loss: 0.03857 \n",
      "[ 65/100] train_loss: 0.05811 valid_loss: 0.07143 test_loss: 0.03574 \n",
      "Validation loss decreased (0.071749 --> 0.071434).  Saving model ...\n",
      "[ 66/100] train_loss: 0.05895 valid_loss: 0.07717 test_loss: 0.03413 \n",
      "[ 67/100] train_loss: 0.05650 valid_loss: 0.07811 test_loss: 0.03601 \n",
      "[ 68/100] train_loss: 0.05979 valid_loss: 0.07479 test_loss: 0.03687 \n",
      "[ 69/100] train_loss: 0.05797 valid_loss: 0.07863 test_loss: 0.03724 \n",
      "[ 70/100] train_loss: 0.06076 valid_loss: 0.07529 test_loss: 0.03659 \n",
      "[ 71/100] train_loss: 0.05931 valid_loss: 0.08387 test_loss: 0.03497 \n",
      "[ 72/100] train_loss: 0.06174 valid_loss: 0.07582 test_loss: 0.03385 \n",
      "[ 73/100] train_loss: 0.05786 valid_loss: 0.08397 test_loss: 0.03819 \n",
      "[ 74/100] train_loss: 0.05301 valid_loss: 0.08095 test_loss: 0.03353 \n",
      "[ 75/100] train_loss: 0.05782 valid_loss: 0.07898 test_loss: 0.03781 \n",
      "[ 76/100] train_loss: 0.05857 valid_loss: 0.07686 test_loss: 0.03469 \n",
      "[ 77/100] train_loss: 0.05803 valid_loss: 0.07557 test_loss: 0.03548 \n",
      "[ 78/100] train_loss: 0.05753 valid_loss: 0.07569 test_loss: 0.03409 \n",
      "[ 79/100] train_loss: 0.05869 valid_loss: 0.08444 test_loss: 0.04029 \n",
      "[ 80/100] train_loss: 0.05896 valid_loss: 0.07500 test_loss: 0.03113 \n",
      "[ 81/100] train_loss: 0.05778 valid_loss: 0.07148 test_loss: 0.03554 \n",
      "[ 82/100] train_loss: 0.05730 valid_loss: 0.08536 test_loss: 0.04205 \n",
      "[ 83/100] train_loss: 0.06079 valid_loss: 0.07884 test_loss: 0.03299 \n",
      "[ 84/100] train_loss: 0.05532 valid_loss: 0.08282 test_loss: 0.03679 \n",
      "[ 85/100] train_loss: 0.05879 valid_loss: 0.07759 test_loss: 0.03523 \n",
      "[ 86/100] train_loss: 0.05657 valid_loss: 0.07966 test_loss: 0.03808 \n",
      "[ 87/100] train_loss: 0.05769 valid_loss: 0.07832 test_loss: 0.03575 \n",
      "[ 88/100] train_loss: 0.06180 valid_loss: 0.08233 test_loss: 0.04081 \n",
      "[ 89/100] train_loss: 0.05079 valid_loss: 0.07854 test_loss: 0.03000 \n",
      "[ 90/100] train_loss: 0.05685 valid_loss: 0.07873 test_loss: 0.04555 \n",
      "[ 91/100] train_loss: 0.05547 valid_loss: 0.07626 test_loss: 0.03486 \n",
      "[ 92/100] train_loss: 0.05513 valid_loss: 0.07921 test_loss: 0.03560 \n",
      "[ 93/100] train_loss: 0.05668 valid_loss: 0.07979 test_loss: 0.03620 \n",
      "[ 94/100] train_loss: 0.05111 valid_loss: 0.08086 test_loss: 0.03654 \n",
      "[ 95/100] train_loss: 0.05207 valid_loss: 0.08190 test_loss: 0.03631 \n",
      "[ 96/100] train_loss: 0.05592 valid_loss: 0.07421 test_loss: 0.03607 \n",
      "[ 97/100] train_loss: 0.05409 valid_loss: 0.08187 test_loss: 0.03572 \n",
      "[ 98/100] train_loss: 0.05504 valid_loss: 0.08625 test_loss: 0.04048 \n",
      "[ 99/100] train_loss: 0.05638 valid_loss: 0.07602 test_loss: 0.03277 \n",
      "[100/100] train_loss: 0.05474 valid_loss: 0.06747 test_loss: 0.03122 \n",
      "Validation loss decreased (0.071434 --> 0.067471).  Saving model ...\n",
      "TRAINING MODEL 15\n",
      "[  1/100] train_loss: 0.18493 valid_loss: 0.19205 test_loss: 0.18159 \n",
      "Validation loss decreased (inf --> 0.192049).  Saving model ...\n",
      "[  2/100] train_loss: 0.18080 valid_loss: 0.18828 test_loss: 0.17579 \n",
      "Validation loss decreased (0.192049 --> 0.188280).  Saving model ...\n",
      "[  3/100] train_loss: 0.17698 valid_loss: 0.18348 test_loss: 0.16944 \n",
      "Validation loss decreased (0.188280 --> 0.183481).  Saving model ...\n",
      "[  4/100] train_loss: 0.17071 valid_loss: 0.17841 test_loss: 0.16141 \n",
      "Validation loss decreased (0.183481 --> 0.178407).  Saving model ...\n",
      "[  5/100] train_loss: 0.16319 valid_loss: 0.17228 test_loss: 0.15255 \n",
      "Validation loss decreased (0.178407 --> 0.172279).  Saving model ...\n",
      "[  6/100] train_loss: 0.15554 valid_loss: 0.16314 test_loss: 0.14168 \n",
      "Validation loss decreased (0.172279 --> 0.163145).  Saving model ...\n",
      "[  7/100] train_loss: 0.14740 valid_loss: 0.15221 test_loss: 0.13057 \n",
      "Validation loss decreased (0.163145 --> 0.152212).  Saving model ...\n",
      "[  8/100] train_loss: 0.13987 valid_loss: 0.14544 test_loss: 0.12088 \n",
      "Validation loss decreased (0.152212 --> 0.145437).  Saving model ...\n",
      "[  9/100] train_loss: 0.13032 valid_loss: 0.13168 test_loss: 0.10982 \n",
      "Validation loss decreased (0.145437 --> 0.131683).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11968 valid_loss: 0.12619 test_loss: 0.09945 \n",
      "Validation loss decreased (0.131683 --> 0.126190).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11303 valid_loss: 0.12336 test_loss: 0.09239 \n",
      "Validation loss decreased (0.126190 --> 0.123359).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10683 valid_loss: 0.11495 test_loss: 0.08304 \n",
      "Validation loss decreased (0.123359 --> 0.114954).  Saving model ...\n",
      "[ 13/100] train_loss: 0.10486 valid_loss: 0.11101 test_loss: 0.07864 \n",
      "Validation loss decreased (0.114954 --> 0.111011).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09754 valid_loss: 0.10457 test_loss: 0.07230 \n",
      "Validation loss decreased (0.111011 --> 0.104572).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09364 valid_loss: 0.10120 test_loss: 0.06776 \n",
      "Validation loss decreased (0.104572 --> 0.101204).  Saving model ...\n",
      "[ 16/100] train_loss: 0.09050 valid_loss: 0.09799 test_loss: 0.06512 \n",
      "Validation loss decreased (0.101204 --> 0.097987).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08923 valid_loss: 0.10452 test_loss: 0.06279 \n",
      "[ 18/100] train_loss: 0.08562 valid_loss: 0.09480 test_loss: 0.05955 \n",
      "Validation loss decreased (0.097987 --> 0.094801).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08530 valid_loss: 0.09393 test_loss: 0.05760 \n",
      "Validation loss decreased (0.094801 --> 0.093934).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08542 valid_loss: 0.09239 test_loss: 0.05577 \n",
      "Validation loss decreased (0.093934 --> 0.092387).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07983 valid_loss: 0.09306 test_loss: 0.05356 \n",
      "[ 22/100] train_loss: 0.08060 valid_loss: 0.08712 test_loss: 0.05605 \n",
      "Validation loss decreased (0.092387 --> 0.087119).  Saving model ...\n",
      "[ 23/100] train_loss: 0.07664 valid_loss: 0.09708 test_loss: 0.05212 \n",
      "[ 24/100] train_loss: 0.07724 valid_loss: 0.08686 test_loss: 0.04975 \n",
      "Validation loss decreased (0.087119 --> 0.086864).  Saving model ...\n",
      "[ 25/100] train_loss: 0.07405 valid_loss: 0.09162 test_loss: 0.05047 \n",
      "[ 26/100] train_loss: 0.07519 valid_loss: 0.09171 test_loss: 0.04732 \n",
      "[ 27/100] train_loss: 0.07749 valid_loss: 0.08666 test_loss: 0.04819 \n",
      "Validation loss decreased (0.086864 --> 0.086665).  Saving model ...\n",
      "[ 28/100] train_loss: 0.07380 valid_loss: 0.08954 test_loss: 0.04661 \n",
      "[ 29/100] train_loss: 0.07622 valid_loss: 0.08712 test_loss: 0.04745 \n",
      "[ 30/100] train_loss: 0.06928 valid_loss: 0.08640 test_loss: 0.04388 \n",
      "Validation loss decreased (0.086665 --> 0.086397).  Saving model ...\n",
      "[ 31/100] train_loss: 0.06961 valid_loss: 0.09238 test_loss: 0.04559 \n",
      "[ 32/100] train_loss: 0.06871 valid_loss: 0.08004 test_loss: 0.04612 \n",
      "Validation loss decreased (0.086397 --> 0.080043).  Saving model ...\n",
      "[ 33/100] train_loss: 0.07436 valid_loss: 0.09308 test_loss: 0.04751 \n",
      "[ 34/100] train_loss: 0.06783 valid_loss: 0.08500 test_loss: 0.04198 \n",
      "[ 35/100] train_loss: 0.07353 valid_loss: 0.09389 test_loss: 0.04745 \n",
      "[ 36/100] train_loss: 0.07156 valid_loss: 0.08770 test_loss: 0.04168 \n",
      "[ 37/100] train_loss: 0.07246 valid_loss: 0.08496 test_loss: 0.04214 \n",
      "[ 38/100] train_loss: 0.06631 valid_loss: 0.08643 test_loss: 0.03981 \n",
      "[ 39/100] train_loss: 0.06846 valid_loss: 0.08909 test_loss: 0.04381 \n",
      "[ 40/100] train_loss: 0.06741 valid_loss: 0.09315 test_loss: 0.04232 \n",
      "[ 41/100] train_loss: 0.06999 valid_loss: 0.09156 test_loss: 0.04133 \n",
      "[ 42/100] train_loss: 0.06730 valid_loss: 0.08621 test_loss: 0.04157 \n",
      "[ 43/100] train_loss: 0.06490 valid_loss: 0.08070 test_loss: 0.03911 \n",
      "[ 44/100] train_loss: 0.06896 valid_loss: 0.09861 test_loss: 0.04463 \n",
      "[ 45/100] train_loss: 0.06790 valid_loss: 0.08669 test_loss: 0.03896 \n",
      "[ 46/100] train_loss: 0.06527 valid_loss: 0.08733 test_loss: 0.03932 \n",
      "[ 47/100] train_loss: 0.06694 valid_loss: 0.09273 test_loss: 0.04336 \n",
      "[ 48/100] train_loss: 0.06547 valid_loss: 0.08465 test_loss: 0.03914 \n",
      "[ 49/100] train_loss: 0.06064 valid_loss: 0.08260 test_loss: 0.03570 \n",
      "[ 50/100] train_loss: 0.06181 valid_loss: 0.08536 test_loss: 0.03827 \n",
      "[ 51/100] train_loss: 0.06401 valid_loss: 0.08931 test_loss: 0.04043 \n",
      "[ 52/100] train_loss: 0.06140 valid_loss: 0.08598 test_loss: 0.03862 \n",
      "[ 53/100] train_loss: 0.06295 valid_loss: 0.09340 test_loss: 0.04034 \n",
      "[ 54/100] train_loss: 0.06201 valid_loss: 0.07989 test_loss: 0.03635 \n",
      "Validation loss decreased (0.080043 --> 0.079887).  Saving model ...\n",
      "[ 55/100] train_loss: 0.06205 valid_loss: 0.08311 test_loss: 0.03781 \n",
      "[ 56/100] train_loss: 0.06101 valid_loss: 0.08669 test_loss: 0.03923 \n",
      "[ 57/100] train_loss: 0.06174 valid_loss: 0.09073 test_loss: 0.03834 \n",
      "[ 58/100] train_loss: 0.06464 valid_loss: 0.09401 test_loss: 0.04182 \n",
      "[ 59/100] train_loss: 0.06174 valid_loss: 0.07360 test_loss: 0.03688 \n",
      "Validation loss decreased (0.079887 --> 0.073603).  Saving model ...\n",
      "[ 60/100] train_loss: 0.06492 valid_loss: 0.10104 test_loss: 0.04925 \n",
      "[ 61/100] train_loss: 0.06694 valid_loss: 0.08728 test_loss: 0.04195 \n",
      "[ 62/100] train_loss: 0.05939 valid_loss: 0.08972 test_loss: 0.03660 \n",
      "[ 63/100] train_loss: 0.05978 valid_loss: 0.08795 test_loss: 0.04107 \n",
      "[ 64/100] train_loss: 0.05728 valid_loss: 0.08391 test_loss: 0.03686 \n",
      "[ 65/100] train_loss: 0.06135 valid_loss: 0.08305 test_loss: 0.04264 \n",
      "[ 66/100] train_loss: 0.06176 valid_loss: 0.08806 test_loss: 0.03679 \n",
      "[ 67/100] train_loss: 0.05930 valid_loss: 0.08862 test_loss: 0.03706 \n",
      "[ 68/100] train_loss: 0.05507 valid_loss: 0.08477 test_loss: 0.03613 \n",
      "[ 69/100] train_loss: 0.05590 valid_loss: 0.08577 test_loss: 0.03706 \n",
      "[ 70/100] train_loss: 0.05712 valid_loss: 0.07766 test_loss: 0.03759 \n",
      "[ 71/100] train_loss: 0.06294 valid_loss: 0.07814 test_loss: 0.03732 \n",
      "[ 72/100] train_loss: 0.05610 valid_loss: 0.08839 test_loss: 0.03498 \n",
      "[ 73/100] train_loss: 0.05061 valid_loss: 0.09168 test_loss: 0.04047 \n",
      "[ 74/100] train_loss: 0.05713 valid_loss: 0.08168 test_loss: 0.03336 \n",
      "[ 75/100] train_loss: 0.05549 valid_loss: 0.09021 test_loss: 0.03670 \n",
      "[ 76/100] train_loss: 0.06035 valid_loss: 0.08467 test_loss: 0.03961 \n",
      "[ 77/100] train_loss: 0.05574 valid_loss: 0.08963 test_loss: 0.03692 \n",
      "[ 78/100] train_loss: 0.05466 valid_loss: 0.08299 test_loss: 0.03421 \n",
      "[ 79/100] train_loss: 0.05607 valid_loss: 0.09327 test_loss: 0.03993 \n",
      "[ 80/100] train_loss: 0.05763 valid_loss: 0.07847 test_loss: 0.03429 \n",
      "[ 81/100] train_loss: 0.06034 valid_loss: 0.10457 test_loss: 0.04637 \n",
      "[ 82/100] train_loss: 0.05972 valid_loss: 0.09411 test_loss: 0.03988 \n",
      "[ 83/100] train_loss: 0.05510 valid_loss: 0.08689 test_loss: 0.03421 \n",
      "[ 84/100] train_loss: 0.05666 valid_loss: 0.08145 test_loss: 0.03853 \n",
      "[ 85/100] train_loss: 0.05768 valid_loss: 0.08598 test_loss: 0.03844 \n",
      "[ 86/100] train_loss: 0.05619 valid_loss: 0.08456 test_loss: 0.03971 \n",
      "[ 87/100] train_loss: 0.05749 valid_loss: 0.08564 test_loss: 0.03609 \n",
      "[ 88/100] train_loss: 0.06045 valid_loss: 0.08541 test_loss: 0.03941 \n",
      "[ 89/100] train_loss: 0.05434 valid_loss: 0.10223 test_loss: 0.04567 \n",
      "[ 90/100] train_loss: 0.05734 valid_loss: 0.07677 test_loss: 0.03501 \n",
      "[ 91/100] train_loss: 0.05622 valid_loss: 0.09040 test_loss: 0.03686 \n",
      "[ 92/100] train_loss: 0.06065 valid_loss: 0.09090 test_loss: 0.04837 \n",
      "[ 93/100] train_loss: 0.05517 valid_loss: 0.08503 test_loss: 0.03349 \n",
      "[ 94/100] train_loss: 0.05324 valid_loss: 0.08734 test_loss: 0.03672 \n",
      "[ 95/100] train_loss: 0.04821 valid_loss: 0.08496 test_loss: 0.03353 \n",
      "[ 96/100] train_loss: 0.05729 valid_loss: 0.08495 test_loss: 0.03644 \n",
      "[ 97/100] train_loss: 0.05524 valid_loss: 0.09504 test_loss: 0.04131 \n",
      "[ 98/100] train_loss: 0.05214 valid_loss: 0.08859 test_loss: 0.03537 \n",
      "[ 99/100] train_loss: 0.05433 valid_loss: 0.08294 test_loss: 0.03720 \n",
      "[100/100] train_loss: 0.06096 valid_loss: 0.09670 test_loss: 0.04676 \n",
      "TRAINING MODEL 16\n",
      "[  1/100] train_loss: 0.18444 valid_loss: 0.19440 test_loss: 0.17859 \n",
      "Validation loss decreased (inf --> 0.194403).  Saving model ...\n",
      "[  2/100] train_loss: 0.17923 valid_loss: 0.19032 test_loss: 0.17304 \n",
      "Validation loss decreased (0.194403 --> 0.190317).  Saving model ...\n",
      "[  3/100] train_loss: 0.17550 valid_loss: 0.18350 test_loss: 0.16497 \n",
      "Validation loss decreased (0.190317 --> 0.183496).  Saving model ...\n",
      "[  4/100] train_loss: 0.16853 valid_loss: 0.17586 test_loss: 0.15797 \n",
      "Validation loss decreased (0.183496 --> 0.175858).  Saving model ...\n",
      "[  5/100] train_loss: 0.16132 valid_loss: 0.16579 test_loss: 0.14702 \n",
      "Validation loss decreased (0.175858 --> 0.165787).  Saving model ...\n",
      "[  6/100] train_loss: 0.15149 valid_loss: 0.15480 test_loss: 0.13677 \n",
      "Validation loss decreased (0.165787 --> 0.154805).  Saving model ...\n",
      "[  7/100] train_loss: 0.14130 valid_loss: 0.14694 test_loss: 0.12634 \n",
      "Validation loss decreased (0.154805 --> 0.146944).  Saving model ...\n",
      "[  8/100] train_loss: 0.13561 valid_loss: 0.13141 test_loss: 0.11538 \n",
      "Validation loss decreased (0.146944 --> 0.131406).  Saving model ...\n",
      "[  9/100] train_loss: 0.12337 valid_loss: 0.12454 test_loss: 0.10596 \n",
      "Validation loss decreased (0.131406 --> 0.124542).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11428 valid_loss: 0.11858 test_loss: 0.09693 \n",
      "Validation loss decreased (0.124542 --> 0.118576).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11051 valid_loss: 0.10653 test_loss: 0.08865 \n",
      "Validation loss decreased (0.118576 --> 0.106532).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10446 valid_loss: 0.10395 test_loss: 0.08180 \n",
      "Validation loss decreased (0.106532 --> 0.103954).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09757 valid_loss: 0.09772 test_loss: 0.07622 \n",
      "Validation loss decreased (0.103954 --> 0.097717).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09463 valid_loss: 0.09195 test_loss: 0.07132 \n",
      "Validation loss decreased (0.097717 --> 0.091947).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09126 valid_loss: 0.08801 test_loss: 0.06765 \n",
      "Validation loss decreased (0.091947 --> 0.088011).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08980 valid_loss: 0.08811 test_loss: 0.06466 \n",
      "[ 17/100] train_loss: 0.08521 valid_loss: 0.08586 test_loss: 0.06242 \n",
      "Validation loss decreased (0.088011 --> 0.085863).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08469 valid_loss: 0.08556 test_loss: 0.05911 \n",
      "Validation loss decreased (0.085863 --> 0.085565).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08196 valid_loss: 0.09021 test_loss: 0.05539 \n",
      "[ 20/100] train_loss: 0.07971 valid_loss: 0.08863 test_loss: 0.05347 \n",
      "[ 21/100] train_loss: 0.07731 valid_loss: 0.08358 test_loss: 0.05305 \n",
      "Validation loss decreased (0.085565 --> 0.083580).  Saving model ...\n",
      "[ 22/100] train_loss: 0.07749 valid_loss: 0.08898 test_loss: 0.05092 \n",
      "[ 23/100] train_loss: 0.07974 valid_loss: 0.08332 test_loss: 0.05103 \n",
      "Validation loss decreased (0.083580 --> 0.083321).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07627 valid_loss: 0.08187 test_loss: 0.04986 \n",
      "Validation loss decreased (0.083321 --> 0.081869).  Saving model ...\n",
      "[ 25/100] train_loss: 0.07545 valid_loss: 0.08579 test_loss: 0.04681 \n",
      "[ 26/100] train_loss: 0.07614 valid_loss: 0.08671 test_loss: 0.04745 \n",
      "[ 27/100] train_loss: 0.07311 valid_loss: 0.08323 test_loss: 0.04604 \n",
      "[ 28/100] train_loss: 0.07624 valid_loss: 0.08183 test_loss: 0.04798 \n",
      "Validation loss decreased (0.081869 --> 0.081829).  Saving model ...\n",
      "[ 29/100] train_loss: 0.07491 valid_loss: 0.08133 test_loss: 0.04931 \n",
      "Validation loss decreased (0.081829 --> 0.081333).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07290 valid_loss: 0.08105 test_loss: 0.04519 \n",
      "Validation loss decreased (0.081333 --> 0.081046).  Saving model ...\n",
      "[ 31/100] train_loss: 0.06772 valid_loss: 0.08511 test_loss: 0.04285 \n",
      "[ 32/100] train_loss: 0.06997 valid_loss: 0.07983 test_loss: 0.04519 \n",
      "Validation loss decreased (0.081046 --> 0.079831).  Saving model ...\n",
      "[ 33/100] train_loss: 0.07504 valid_loss: 0.08131 test_loss: 0.04269 \n",
      "[ 34/100] train_loss: 0.07061 valid_loss: 0.08210 test_loss: 0.04135 \n",
      "[ 35/100] train_loss: 0.06738 valid_loss: 0.08196 test_loss: 0.04158 \n",
      "[ 36/100] train_loss: 0.07097 valid_loss: 0.08535 test_loss: 0.04372 \n",
      "[ 37/100] train_loss: 0.06601 valid_loss: 0.08066 test_loss: 0.04004 \n",
      "[ 38/100] train_loss: 0.06666 valid_loss: 0.08023 test_loss: 0.04130 \n",
      "[ 39/100] train_loss: 0.07068 valid_loss: 0.08305 test_loss: 0.04140 \n",
      "[ 40/100] train_loss: 0.06682 valid_loss: 0.08077 test_loss: 0.03823 \n",
      "[ 41/100] train_loss: 0.06694 valid_loss: 0.08207 test_loss: 0.04007 \n",
      "[ 42/100] train_loss: 0.06433 valid_loss: 0.07759 test_loss: 0.03987 \n",
      "Validation loss decreased (0.079831 --> 0.077591).  Saving model ...\n",
      "[ 43/100] train_loss: 0.06623 valid_loss: 0.08499 test_loss: 0.04117 \n",
      "[ 44/100] train_loss: 0.06477 valid_loss: 0.08110 test_loss: 0.03614 \n",
      "[ 45/100] train_loss: 0.06278 valid_loss: 0.07729 test_loss: 0.03732 \n",
      "Validation loss decreased (0.077591 --> 0.077290).  Saving model ...\n",
      "[ 46/100] train_loss: 0.06511 valid_loss: 0.07829 test_loss: 0.03970 \n",
      "[ 47/100] train_loss: 0.06620 valid_loss: 0.08211 test_loss: 0.03876 \n",
      "[ 48/100] train_loss: 0.06688 valid_loss: 0.08334 test_loss: 0.03650 \n",
      "[ 49/100] train_loss: 0.06162 valid_loss: 0.08123 test_loss: 0.03875 \n",
      "[ 50/100] train_loss: 0.06385 valid_loss: 0.08080 test_loss: 0.03798 \n",
      "[ 51/100] train_loss: 0.06285 valid_loss: 0.08099 test_loss: 0.03599 \n",
      "[ 52/100] train_loss: 0.06379 valid_loss: 0.07680 test_loss: 0.03691 \n",
      "Validation loss decreased (0.077290 --> 0.076801).  Saving model ...\n",
      "[ 53/100] train_loss: 0.06451 valid_loss: 0.08209 test_loss: 0.03589 \n",
      "[ 54/100] train_loss: 0.06462 valid_loss: 0.08189 test_loss: 0.03654 \n",
      "[ 55/100] train_loss: 0.06047 valid_loss: 0.07879 test_loss: 0.03461 \n",
      "[ 56/100] train_loss: 0.06076 valid_loss: 0.08032 test_loss: 0.03799 \n",
      "[ 57/100] train_loss: 0.06482 valid_loss: 0.08776 test_loss: 0.03863 \n",
      "[ 58/100] train_loss: 0.06009 valid_loss: 0.07449 test_loss: 0.03439 \n",
      "Validation loss decreased (0.076801 --> 0.074494).  Saving model ...\n",
      "[ 59/100] train_loss: 0.06274 valid_loss: 0.08497 test_loss: 0.03990 \n",
      "[ 60/100] train_loss: 0.06148 valid_loss: 0.07816 test_loss: 0.03466 \n",
      "[ 61/100] train_loss: 0.06443 valid_loss: 0.08376 test_loss: 0.03422 \n",
      "[ 62/100] train_loss: 0.06234 valid_loss: 0.08673 test_loss: 0.03734 \n",
      "[ 63/100] train_loss: 0.06021 valid_loss: 0.07837 test_loss: 0.03646 \n",
      "[ 64/100] train_loss: 0.06275 valid_loss: 0.08084 test_loss: 0.03810 \n",
      "[ 65/100] train_loss: 0.06068 valid_loss: 0.09075 test_loss: 0.03765 \n",
      "[ 66/100] train_loss: 0.05961 valid_loss: 0.08221 test_loss: 0.03513 \n",
      "[ 67/100] train_loss: 0.05937 valid_loss: 0.08591 test_loss: 0.03690 \n",
      "[ 68/100] train_loss: 0.05731 valid_loss: 0.09093 test_loss: 0.03895 \n",
      "[ 69/100] train_loss: 0.05737 valid_loss: 0.08454 test_loss: 0.03496 \n",
      "[ 70/100] train_loss: 0.06403 valid_loss: 0.09428 test_loss: 0.04334 \n",
      "[ 71/100] train_loss: 0.05918 valid_loss: 0.07579 test_loss: 0.03077 \n",
      "[ 72/100] train_loss: 0.05868 valid_loss: 0.08154 test_loss: 0.03653 \n",
      "[ 73/100] train_loss: 0.05950 valid_loss: 0.08309 test_loss: 0.03431 \n",
      "[ 74/100] train_loss: 0.06133 valid_loss: 0.07695 test_loss: 0.03393 \n",
      "[ 75/100] train_loss: 0.05899 valid_loss: 0.08418 test_loss: 0.03509 \n",
      "[ 76/100] train_loss: 0.05407 valid_loss: 0.07849 test_loss: 0.03060 \n",
      "[ 77/100] train_loss: 0.05607 valid_loss: 0.08233 test_loss: 0.03347 \n",
      "[ 78/100] train_loss: 0.05834 valid_loss: 0.08139 test_loss: 0.03227 \n",
      "[ 79/100] train_loss: 0.05779 valid_loss: 0.08956 test_loss: 0.03828 \n",
      "[ 80/100] train_loss: 0.05768 valid_loss: 0.08973 test_loss: 0.03869 \n",
      "[ 81/100] train_loss: 0.06040 valid_loss: 0.08896 test_loss: 0.04299 \n",
      "[ 82/100] train_loss: 0.06007 valid_loss: 0.08772 test_loss: 0.03759 \n",
      "[ 83/100] train_loss: 0.05882 valid_loss: 0.08433 test_loss: 0.03579 \n",
      "[ 84/100] train_loss: 0.05588 valid_loss: 0.07725 test_loss: 0.03032 \n",
      "[ 85/100] train_loss: 0.05672 valid_loss: 0.08498 test_loss: 0.03674 \n",
      "[ 86/100] train_loss: 0.05564 valid_loss: 0.07385 test_loss: 0.03103 \n",
      "Validation loss decreased (0.074494 --> 0.073853).  Saving model ...\n",
      "[ 87/100] train_loss: 0.06054 valid_loss: 0.08951 test_loss: 0.04472 \n",
      "[ 88/100] train_loss: 0.05717 valid_loss: 0.09437 test_loss: 0.03420 \n",
      "[ 89/100] train_loss: 0.05915 valid_loss: 0.08472 test_loss: 0.03841 \n",
      "[ 90/100] train_loss: 0.05603 valid_loss: 0.08220 test_loss: 0.03224 \n",
      "[ 91/100] train_loss: 0.05963 valid_loss: 0.08165 test_loss: 0.03317 \n",
      "[ 92/100] train_loss: 0.05694 valid_loss: 0.09149 test_loss: 0.04200 \n",
      "[ 93/100] train_loss: 0.05444 valid_loss: 0.08448 test_loss: 0.03295 \n",
      "[ 94/100] train_loss: 0.05291 valid_loss: 0.08612 test_loss: 0.03820 \n",
      "[ 95/100] train_loss: 0.05320 valid_loss: 0.08355 test_loss: 0.03308 \n",
      "[ 96/100] train_loss: 0.05696 valid_loss: 0.08942 test_loss: 0.03675 \n",
      "[ 97/100] train_loss: 0.05806 valid_loss: 0.09309 test_loss: 0.03853 \n",
      "[ 98/100] train_loss: 0.05525 valid_loss: 0.08436 test_loss: 0.03333 \n",
      "[ 99/100] train_loss: 0.05659 valid_loss: 0.07861 test_loss: 0.03050 \n",
      "[100/100] train_loss: 0.05222 valid_loss: 0.09057 test_loss: 0.03782 \n",
      "TRAINING MODEL 17\n",
      "[  1/100] train_loss: 0.18527 valid_loss: 0.19413 test_loss: 0.17605 \n",
      "Validation loss decreased (inf --> 0.194131).  Saving model ...\n",
      "[  2/100] train_loss: 0.17797 valid_loss: 0.18885 test_loss: 0.16940 \n",
      "Validation loss decreased (0.194131 --> 0.188848).  Saving model ...\n",
      "[  3/100] train_loss: 0.17146 valid_loss: 0.17818 test_loss: 0.15924 \n",
      "Validation loss decreased (0.188848 --> 0.178180).  Saving model ...\n",
      "[  4/100] train_loss: 0.16138 valid_loss: 0.16876 test_loss: 0.14796 \n",
      "Validation loss decreased (0.178180 --> 0.168761).  Saving model ...\n",
      "[  5/100] train_loss: 0.15325 valid_loss: 0.15677 test_loss: 0.13828 \n",
      "Validation loss decreased (0.168761 --> 0.156769).  Saving model ...\n",
      "[  6/100] train_loss: 0.14411 valid_loss: 0.14792 test_loss: 0.12652 \n",
      "Validation loss decreased (0.156769 --> 0.147918).  Saving model ...\n",
      "[  7/100] train_loss: 0.13174 valid_loss: 0.14005 test_loss: 0.11746 \n",
      "Validation loss decreased (0.147918 --> 0.140046).  Saving model ...\n",
      "[  8/100] train_loss: 0.12283 valid_loss: 0.12725 test_loss: 0.10621 \n",
      "Validation loss decreased (0.140046 --> 0.127255).  Saving model ...\n",
      "[  9/100] train_loss: 0.11708 valid_loss: 0.12312 test_loss: 0.09960 \n",
      "Validation loss decreased (0.127255 --> 0.123124).  Saving model ...\n",
      "[ 10/100] train_loss: 0.10914 valid_loss: 0.11740 test_loss: 0.09194 \n",
      "Validation loss decreased (0.123124 --> 0.117395).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10283 valid_loss: 0.11576 test_loss: 0.08633 \n",
      "Validation loss decreased (0.117395 --> 0.115765).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09954 valid_loss: 0.10173 test_loss: 0.07882 \n",
      "Validation loss decreased (0.115765 --> 0.101730).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09422 valid_loss: 0.10118 test_loss: 0.07211 \n",
      "Validation loss decreased (0.101730 --> 0.101185).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09540 valid_loss: 0.09976 test_loss: 0.06995 \n",
      "Validation loss decreased (0.101185 --> 0.099759).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08535 valid_loss: 0.09342 test_loss: 0.06482 \n",
      "Validation loss decreased (0.099759 --> 0.093422).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08651 valid_loss: 0.09354 test_loss: 0.06257 \n",
      "[ 17/100] train_loss: 0.08897 valid_loss: 0.08956 test_loss: 0.06159 \n",
      "Validation loss decreased (0.093422 --> 0.089563).  Saving model ...\n",
      "[ 18/100] train_loss: 0.08355 valid_loss: 0.09071 test_loss: 0.05723 \n",
      "[ 19/100] train_loss: 0.08276 valid_loss: 0.09164 test_loss: 0.05479 \n",
      "[ 20/100] train_loss: 0.08343 valid_loss: 0.08594 test_loss: 0.05452 \n",
      "Validation loss decreased (0.089563 --> 0.085945).  Saving model ...\n",
      "[ 21/100] train_loss: 0.08160 valid_loss: 0.08520 test_loss: 0.05293 \n",
      "Validation loss decreased (0.085945 --> 0.085201).  Saving model ...\n",
      "[ 22/100] train_loss: 0.08131 valid_loss: 0.08449 test_loss: 0.05124 \n",
      "Validation loss decreased (0.085201 --> 0.084489).  Saving model ...\n",
      "[ 23/100] train_loss: 0.08044 valid_loss: 0.08654 test_loss: 0.05114 \n",
      "[ 24/100] train_loss: 0.07749 valid_loss: 0.08385 test_loss: 0.05097 \n",
      "Validation loss decreased (0.084489 --> 0.083852).  Saving model ...\n",
      "[ 25/100] train_loss: 0.07666 valid_loss: 0.08837 test_loss: 0.04953 \n",
      "[ 26/100] train_loss: 0.07628 valid_loss: 0.08386 test_loss: 0.04765 \n",
      "[ 27/100] train_loss: 0.07348 valid_loss: 0.08472 test_loss: 0.04898 \n",
      "[ 28/100] train_loss: 0.07556 valid_loss: 0.09154 test_loss: 0.04671 \n",
      "[ 29/100] train_loss: 0.07097 valid_loss: 0.08251 test_loss: 0.04615 \n",
      "Validation loss decreased (0.083852 --> 0.082506).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07008 valid_loss: 0.08286 test_loss: 0.04512 \n",
      "[ 31/100] train_loss: 0.07044 valid_loss: 0.08591 test_loss: 0.04433 \n",
      "[ 32/100] train_loss: 0.06768 valid_loss: 0.08391 test_loss: 0.04286 \n",
      "[ 33/100] train_loss: 0.06988 valid_loss: 0.08321 test_loss: 0.04359 \n",
      "[ 34/100] train_loss: 0.07001 valid_loss: 0.08196 test_loss: 0.04133 \n",
      "Validation loss decreased (0.082506 --> 0.081961).  Saving model ...\n",
      "[ 35/100] train_loss: 0.07476 valid_loss: 0.08852 test_loss: 0.04370 \n",
      "[ 36/100] train_loss: 0.07332 valid_loss: 0.08920 test_loss: 0.04517 \n",
      "[ 37/100] train_loss: 0.06885 valid_loss: 0.08371 test_loss: 0.04487 \n",
      "[ 38/100] train_loss: 0.06635 valid_loss: 0.08765 test_loss: 0.04086 \n",
      "[ 39/100] train_loss: 0.07001 valid_loss: 0.07963 test_loss: 0.04067 \n",
      "Validation loss decreased (0.081961 --> 0.079627).  Saving model ...\n",
      "[ 40/100] train_loss: 0.06639 valid_loss: 0.08114 test_loss: 0.04225 \n",
      "[ 41/100] train_loss: 0.06579 valid_loss: 0.08011 test_loss: 0.03854 \n",
      "[ 42/100] train_loss: 0.06484 valid_loss: 0.08243 test_loss: 0.03977 \n",
      "[ 43/100] train_loss: 0.06798 valid_loss: 0.08390 test_loss: 0.04066 \n",
      "[ 44/100] train_loss: 0.06277 valid_loss: 0.08431 test_loss: 0.03762 \n",
      "[ 45/100] train_loss: 0.06509 valid_loss: 0.08355 test_loss: 0.04332 \n",
      "[ 46/100] train_loss: 0.06065 valid_loss: 0.09340 test_loss: 0.04053 \n",
      "[ 47/100] train_loss: 0.06579 valid_loss: 0.08855 test_loss: 0.04197 \n",
      "[ 48/100] train_loss: 0.06899 valid_loss: 0.07797 test_loss: 0.03697 \n",
      "Validation loss decreased (0.079627 --> 0.077969).  Saving model ...\n",
      "[ 49/100] train_loss: 0.06362 valid_loss: 0.09271 test_loss: 0.04403 \n",
      "[ 50/100] train_loss: 0.06363 valid_loss: 0.08516 test_loss: 0.03890 \n",
      "[ 51/100] train_loss: 0.06429 valid_loss: 0.09021 test_loss: 0.04270 \n",
      "[ 52/100] train_loss: 0.06473 valid_loss: 0.08558 test_loss: 0.03946 \n",
      "[ 53/100] train_loss: 0.06798 valid_loss: 0.08418 test_loss: 0.04120 \n",
      "[ 54/100] train_loss: 0.05768 valid_loss: 0.08143 test_loss: 0.03635 \n",
      "[ 55/100] train_loss: 0.05926 valid_loss: 0.08484 test_loss: 0.03712 \n",
      "[ 56/100] train_loss: 0.06736 valid_loss: 0.07873 test_loss: 0.03837 \n",
      "[ 57/100] train_loss: 0.06123 valid_loss: 0.08289 test_loss: 0.03822 \n",
      "[ 58/100] train_loss: 0.06563 valid_loss: 0.09154 test_loss: 0.04441 \n",
      "[ 59/100] train_loss: 0.06371 valid_loss: 0.08960 test_loss: 0.04221 \n",
      "[ 60/100] train_loss: 0.06078 valid_loss: 0.08538 test_loss: 0.03729 \n",
      "[ 61/100] train_loss: 0.06040 valid_loss: 0.09152 test_loss: 0.04274 \n",
      "[ 62/100] train_loss: 0.06604 valid_loss: 0.08456 test_loss: 0.04178 \n",
      "[ 63/100] train_loss: 0.06093 valid_loss: 0.09057 test_loss: 0.03734 \n",
      "[ 64/100] train_loss: 0.06041 valid_loss: 0.08155 test_loss: 0.03833 \n",
      "[ 65/100] train_loss: 0.06263 valid_loss: 0.08737 test_loss: 0.03942 \n",
      "[ 66/100] train_loss: 0.05560 valid_loss: 0.09986 test_loss: 0.03877 \n",
      "[ 67/100] train_loss: 0.06027 valid_loss: 0.08617 test_loss: 0.04560 \n",
      "[ 68/100] train_loss: 0.05547 valid_loss: 0.09140 test_loss: 0.03997 \n",
      "[ 69/100] train_loss: 0.05593 valid_loss: 0.07880 test_loss: 0.03351 \n",
      "[ 70/100] train_loss: 0.05957 valid_loss: 0.08423 test_loss: 0.04047 \n",
      "[ 71/100] train_loss: 0.06068 valid_loss: 0.08735 test_loss: 0.03492 \n",
      "[ 72/100] train_loss: 0.05916 valid_loss: 0.10308 test_loss: 0.04825 \n",
      "[ 73/100] train_loss: 0.06073 valid_loss: 0.08157 test_loss: 0.03426 \n",
      "[ 74/100] train_loss: 0.05536 valid_loss: 0.08300 test_loss: 0.04103 \n",
      "[ 75/100] train_loss: 0.06201 valid_loss: 0.10107 test_loss: 0.04625 \n",
      "[ 76/100] train_loss: 0.06003 valid_loss: 0.09130 test_loss: 0.04134 \n",
      "[ 77/100] train_loss: 0.06282 valid_loss: 0.09142 test_loss: 0.04157 \n",
      "[ 78/100] train_loss: 0.05601 valid_loss: 0.08769 test_loss: 0.03893 \n",
      "[ 79/100] train_loss: 0.05506 valid_loss: 0.08707 test_loss: 0.03491 \n",
      "[ 80/100] train_loss: 0.05540 valid_loss: 0.08681 test_loss: 0.04020 \n",
      "[ 81/100] train_loss: 0.05724 valid_loss: 0.09400 test_loss: 0.04406 \n",
      "[ 82/100] train_loss: 0.05728 valid_loss: 0.08872 test_loss: 0.04310 \n",
      "[ 83/100] train_loss: 0.05592 valid_loss: 0.08147 test_loss: 0.03460 \n",
      "[ 84/100] train_loss: 0.05819 valid_loss: 0.09791 test_loss: 0.04666 \n",
      "[ 85/100] train_loss: 0.05867 valid_loss: 0.08405 test_loss: 0.03542 \n",
      "[ 86/100] train_loss: 0.05533 valid_loss: 0.09090 test_loss: 0.04046 \n",
      "[ 87/100] train_loss: 0.05809 valid_loss: 0.08215 test_loss: 0.03963 \n",
      "[ 88/100] train_loss: 0.05478 valid_loss: 0.09258 test_loss: 0.04247 \n",
      "[ 89/100] train_loss: 0.05772 valid_loss: 0.09608 test_loss: 0.04290 \n",
      "[ 90/100] train_loss: 0.05710 valid_loss: 0.08821 test_loss: 0.03980 \n",
      "[ 91/100] train_loss: 0.05651 valid_loss: 0.09550 test_loss: 0.04511 \n",
      "[ 92/100] train_loss: 0.05891 valid_loss: 0.08079 test_loss: 0.03523 \n",
      "[ 93/100] train_loss: 0.05851 valid_loss: 0.08564 test_loss: 0.04257 \n",
      "[ 94/100] train_loss: 0.05373 valid_loss: 0.08676 test_loss: 0.03868 \n",
      "[ 95/100] train_loss: 0.05342 valid_loss: 0.09098 test_loss: 0.04118 \n",
      "[ 96/100] train_loss: 0.05539 valid_loss: 0.08334 test_loss: 0.03910 \n",
      "[ 97/100] train_loss: 0.05307 valid_loss: 0.08111 test_loss: 0.03512 \n",
      "[ 98/100] train_loss: 0.05358 valid_loss: 0.08218 test_loss: 0.03621 \n",
      "[ 99/100] train_loss: 0.05430 valid_loss: 0.07961 test_loss: 0.03363 \n",
      "[100/100] train_loss: 0.05346 valid_loss: 0.08558 test_loss: 0.03569 \n",
      "TRAINING MODEL 18\n",
      "[  1/100] train_loss: 0.18239 valid_loss: 0.19226 test_loss: 0.17721 \n",
      "Validation loss decreased (inf --> 0.192264).  Saving model ...\n",
      "[  2/100] train_loss: 0.17557 valid_loss: 0.18482 test_loss: 0.16858 \n",
      "Validation loss decreased (0.192264 --> 0.184820).  Saving model ...\n",
      "[  3/100] train_loss: 0.16890 valid_loss: 0.17641 test_loss: 0.15806 \n",
      "Validation loss decreased (0.184820 --> 0.176409).  Saving model ...\n",
      "[  4/100] train_loss: 0.16024 valid_loss: 0.16650 test_loss: 0.14847 \n",
      "Validation loss decreased (0.176409 --> 0.166501).  Saving model ...\n",
      "[  5/100] train_loss: 0.15056 valid_loss: 0.15534 test_loss: 0.13861 \n",
      "Validation loss decreased (0.166501 --> 0.155337).  Saving model ...\n",
      "[  6/100] train_loss: 0.14331 valid_loss: 0.14482 test_loss: 0.12757 \n",
      "Validation loss decreased (0.155337 --> 0.144821).  Saving model ...\n",
      "[  7/100] train_loss: 0.13283 valid_loss: 0.13447 test_loss: 0.11653 \n",
      "Validation loss decreased (0.144821 --> 0.134471).  Saving model ...\n",
      "[  8/100] train_loss: 0.12448 valid_loss: 0.12948 test_loss: 0.10758 \n",
      "Validation loss decreased (0.134471 --> 0.129479).  Saving model ...\n",
      "[  9/100] train_loss: 0.11807 valid_loss: 0.12380 test_loss: 0.09944 \n",
      "Validation loss decreased (0.129479 --> 0.123803).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11102 valid_loss: 0.11122 test_loss: 0.08996 \n",
      "Validation loss decreased (0.123803 --> 0.111218).  Saving model ...\n",
      "[ 11/100] train_loss: 0.10475 valid_loss: 0.10100 test_loss: 0.08397 \n",
      "Validation loss decreased (0.111218 --> 0.100999).  Saving model ...\n",
      "[ 12/100] train_loss: 0.09615 valid_loss: 0.10048 test_loss: 0.07675 \n",
      "Validation loss decreased (0.100999 --> 0.100485).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09477 valid_loss: 0.09610 test_loss: 0.07119 \n",
      "Validation loss decreased (0.100485 --> 0.096102).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09187 valid_loss: 0.09604 test_loss: 0.06632 \n",
      "Validation loss decreased (0.096102 --> 0.096044).  Saving model ...\n",
      "[ 15/100] train_loss: 0.08832 valid_loss: 0.09165 test_loss: 0.06290 \n",
      "Validation loss decreased (0.096044 --> 0.091652).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08394 valid_loss: 0.08839 test_loss: 0.06006 \n",
      "Validation loss decreased (0.091652 --> 0.088392).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08219 valid_loss: 0.08460 test_loss: 0.05957 \n",
      "Validation loss decreased (0.088392 --> 0.084596).  Saving model ...\n",
      "[ 18/100] train_loss: 0.07959 valid_loss: 0.08632 test_loss: 0.05513 \n",
      "[ 19/100] train_loss: 0.07887 valid_loss: 0.08753 test_loss: 0.05247 \n",
      "[ 20/100] train_loss: 0.07935 valid_loss: 0.08500 test_loss: 0.05226 \n",
      "[ 21/100] train_loss: 0.07786 valid_loss: 0.08705 test_loss: 0.04922 \n",
      "[ 22/100] train_loss: 0.07813 valid_loss: 0.08663 test_loss: 0.04982 \n",
      "[ 23/100] train_loss: 0.07947 valid_loss: 0.08560 test_loss: 0.04666 \n",
      "[ 24/100] train_loss: 0.07410 valid_loss: 0.08617 test_loss: 0.04540 \n",
      "[ 25/100] train_loss: 0.07883 valid_loss: 0.09020 test_loss: 0.04729 \n",
      "[ 26/100] train_loss: 0.07292 valid_loss: 0.08288 test_loss: 0.04443 \n",
      "Validation loss decreased (0.084596 --> 0.082876).  Saving model ...\n",
      "[ 27/100] train_loss: 0.07420 valid_loss: 0.08462 test_loss: 0.04398 \n",
      "[ 28/100] train_loss: 0.06876 valid_loss: 0.08529 test_loss: 0.04405 \n",
      "[ 29/100] train_loss: 0.07277 valid_loss: 0.08240 test_loss: 0.04704 \n",
      "Validation loss decreased (0.082876 --> 0.082396).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07064 valid_loss: 0.08630 test_loss: 0.04177 \n",
      "[ 31/100] train_loss: 0.07186 valid_loss: 0.09518 test_loss: 0.04556 \n",
      "[ 32/100] train_loss: 0.07473 valid_loss: 0.08211 test_loss: 0.04259 \n",
      "Validation loss decreased (0.082396 --> 0.082114).  Saving model ...\n",
      "[ 33/100] train_loss: 0.06705 valid_loss: 0.07875 test_loss: 0.04182 \n",
      "Validation loss decreased (0.082114 --> 0.078747).  Saving model ...\n",
      "[ 34/100] train_loss: 0.06828 valid_loss: 0.08467 test_loss: 0.04031 \n",
      "[ 35/100] train_loss: 0.07035 valid_loss: 0.08892 test_loss: 0.04131 \n",
      "[ 36/100] train_loss: 0.07095 valid_loss: 0.08773 test_loss: 0.04247 \n",
      "[ 37/100] train_loss: 0.06910 valid_loss: 0.08743 test_loss: 0.03967 \n",
      "[ 38/100] train_loss: 0.07271 valid_loss: 0.08295 test_loss: 0.04186 \n",
      "[ 39/100] train_loss: 0.06975 valid_loss: 0.08236 test_loss: 0.03998 \n",
      "[ 40/100] train_loss: 0.06519 valid_loss: 0.08044 test_loss: 0.03878 \n",
      "[ 41/100] train_loss: 0.06483 valid_loss: 0.08505 test_loss: 0.03885 \n",
      "[ 42/100] train_loss: 0.06710 valid_loss: 0.08900 test_loss: 0.04279 \n",
      "[ 43/100] train_loss: 0.06868 valid_loss: 0.08355 test_loss: 0.03996 \n",
      "[ 44/100] train_loss: 0.06701 valid_loss: 0.08023 test_loss: 0.03818 \n",
      "[ 45/100] train_loss: 0.06365 valid_loss: 0.08382 test_loss: 0.03762 \n",
      "[ 46/100] train_loss: 0.06480 valid_loss: 0.07882 test_loss: 0.03538 \n",
      "[ 47/100] train_loss: 0.06570 valid_loss: 0.08090 test_loss: 0.03743 \n",
      "[ 48/100] train_loss: 0.06508 valid_loss: 0.08122 test_loss: 0.03613 \n",
      "[ 49/100] train_loss: 0.06179 valid_loss: 0.08998 test_loss: 0.04059 \n",
      "[ 50/100] train_loss: 0.06720 valid_loss: 0.07627 test_loss: 0.03756 \n",
      "Validation loss decreased (0.078747 --> 0.076274).  Saving model ...\n",
      "[ 51/100] train_loss: 0.06408 valid_loss: 0.08290 test_loss: 0.03868 \n",
      "[ 52/100] train_loss: 0.06644 valid_loss: 0.08112 test_loss: 0.03727 \n",
      "[ 53/100] train_loss: 0.06461 valid_loss: 0.07661 test_loss: 0.03655 \n",
      "[ 54/100] train_loss: 0.06088 valid_loss: 0.07696 test_loss: 0.03335 \n",
      "[ 55/100] train_loss: 0.06107 valid_loss: 0.07924 test_loss: 0.03537 \n",
      "[ 56/100] train_loss: 0.06495 valid_loss: 0.07742 test_loss: 0.03982 \n",
      "[ 57/100] train_loss: 0.06192 valid_loss: 0.08052 test_loss: 0.03560 \n",
      "[ 58/100] train_loss: 0.05702 valid_loss: 0.07950 test_loss: 0.03601 \n",
      "[ 59/100] train_loss: 0.05690 valid_loss: 0.07858 test_loss: 0.03431 \n",
      "[ 60/100] train_loss: 0.06113 valid_loss: 0.07826 test_loss: 0.03696 \n",
      "[ 61/100] train_loss: 0.06076 valid_loss: 0.07651 test_loss: 0.03444 \n",
      "[ 62/100] train_loss: 0.05881 valid_loss: 0.08710 test_loss: 0.03881 \n",
      "[ 63/100] train_loss: 0.06284 valid_loss: 0.07582 test_loss: 0.03608 \n",
      "Validation loss decreased (0.076274 --> 0.075815).  Saving model ...\n",
      "[ 64/100] train_loss: 0.06213 valid_loss: 0.08191 test_loss: 0.03571 \n",
      "[ 65/100] train_loss: 0.05901 valid_loss: 0.07401 test_loss: 0.03391 \n",
      "Validation loss decreased (0.075815 --> 0.074014).  Saving model ...\n",
      "[ 66/100] train_loss: 0.06536 valid_loss: 0.07847 test_loss: 0.03958 \n",
      "[ 67/100] train_loss: 0.06089 valid_loss: 0.07996 test_loss: 0.03571 \n",
      "[ 68/100] train_loss: 0.06227 valid_loss: 0.07626 test_loss: 0.03519 \n",
      "[ 69/100] train_loss: 0.06186 valid_loss: 0.08212 test_loss: 0.03645 \n",
      "[ 70/100] train_loss: 0.06320 valid_loss: 0.07620 test_loss: 0.03818 \n",
      "[ 71/100] train_loss: 0.05789 valid_loss: 0.08055 test_loss: 0.03300 \n",
      "[ 72/100] train_loss: 0.05327 valid_loss: 0.08295 test_loss: 0.03517 \n",
      "[ 73/100] train_loss: 0.06041 valid_loss: 0.07544 test_loss: 0.03535 \n",
      "[ 74/100] train_loss: 0.06139 valid_loss: 0.07710 test_loss: 0.03654 \n",
      "[ 75/100] train_loss: 0.05850 valid_loss: 0.07417 test_loss: 0.03330 \n",
      "[ 76/100] train_loss: 0.05519 valid_loss: 0.08096 test_loss: 0.03047 \n",
      "[ 77/100] train_loss: 0.05920 valid_loss: 0.07793 test_loss: 0.03760 \n",
      "[ 78/100] train_loss: 0.05147 valid_loss: 0.08333 test_loss: 0.03173 \n",
      "[ 79/100] train_loss: 0.05880 valid_loss: 0.07439 test_loss: 0.03957 \n",
      "[ 80/100] train_loss: 0.05742 valid_loss: 0.07200 test_loss: 0.03165 \n",
      "Validation loss decreased (0.074014 --> 0.071999).  Saving model ...\n",
      "[ 81/100] train_loss: 0.05904 valid_loss: 0.06971 test_loss: 0.03567 \n",
      "Validation loss decreased (0.071999 --> 0.069711).  Saving model ...\n",
      "[ 82/100] train_loss: 0.06095 valid_loss: 0.09205 test_loss: 0.03876 \n",
      "[ 83/100] train_loss: 0.05752 valid_loss: 0.08444 test_loss: 0.03628 \n",
      "[ 84/100] train_loss: 0.05661 valid_loss: 0.07470 test_loss: 0.03386 \n",
      "[ 85/100] train_loss: 0.05434 valid_loss: 0.07282 test_loss: 0.03382 \n",
      "[ 86/100] train_loss: 0.05790 valid_loss: 0.07627 test_loss: 0.03650 \n",
      "[ 87/100] train_loss: 0.06004 valid_loss: 0.07853 test_loss: 0.03297 \n",
      "[ 88/100] train_loss: 0.05603 valid_loss: 0.07389 test_loss: 0.03270 \n",
      "[ 89/100] train_loss: 0.05807 valid_loss: 0.07801 test_loss: 0.03572 \n",
      "[ 90/100] train_loss: 0.05245 valid_loss: 0.07889 test_loss: 0.03315 \n",
      "[ 91/100] train_loss: 0.05884 valid_loss: 0.07538 test_loss: 0.03439 \n",
      "[ 92/100] train_loss: 0.05647 valid_loss: 0.07482 test_loss: 0.03259 \n",
      "[ 93/100] train_loss: 0.05984 valid_loss: 0.07728 test_loss: 0.03507 \n",
      "[ 94/100] train_loss: 0.05731 valid_loss: 0.07294 test_loss: 0.03436 \n",
      "[ 95/100] train_loss: 0.05498 valid_loss: 0.07508 test_loss: 0.03163 \n",
      "[ 96/100] train_loss: 0.06086 valid_loss: 0.07032 test_loss: 0.03508 \n",
      "[ 97/100] train_loss: 0.05210 valid_loss: 0.08092 test_loss: 0.02788 \n",
      "[ 98/100] train_loss: 0.06050 valid_loss: 0.07794 test_loss: 0.04249 \n",
      "[ 99/100] train_loss: 0.05309 valid_loss: 0.07463 test_loss: 0.03139 \n",
      "[100/100] train_loss: 0.05974 valid_loss: 0.08185 test_loss: 0.03586 \n",
      "TRAINING MODEL 19\n",
      "[  1/100] train_loss: 0.18422 valid_loss: 0.19353 test_loss: 0.17777 \n",
      "Validation loss decreased (inf --> 0.193528).  Saving model ...\n",
      "[  2/100] train_loss: 0.17974 valid_loss: 0.18888 test_loss: 0.17185 \n",
      "Validation loss decreased (0.193528 --> 0.188875).  Saving model ...\n",
      "[  3/100] train_loss: 0.17349 valid_loss: 0.18290 test_loss: 0.16285 \n",
      "Validation loss decreased (0.188875 --> 0.182902).  Saving model ...\n",
      "[  4/100] train_loss: 0.16717 valid_loss: 0.17496 test_loss: 0.15300 \n",
      "Validation loss decreased (0.182902 --> 0.174958).  Saving model ...\n",
      "[  5/100] train_loss: 0.15996 valid_loss: 0.16522 test_loss: 0.14409 \n",
      "Validation loss decreased (0.174958 --> 0.165219).  Saving model ...\n",
      "[  6/100] train_loss: 0.14929 valid_loss: 0.15516 test_loss: 0.13257 \n",
      "Validation loss decreased (0.165219 --> 0.155164).  Saving model ...\n",
      "[  7/100] train_loss: 0.14141 valid_loss: 0.14441 test_loss: 0.12276 \n",
      "Validation loss decreased (0.155164 --> 0.144413).  Saving model ...\n",
      "[  8/100] train_loss: 0.13137 valid_loss: 0.13851 test_loss: 0.11281 \n",
      "Validation loss decreased (0.144413 --> 0.138511).  Saving model ...\n",
      "[  9/100] train_loss: 0.12425 valid_loss: 0.12923 test_loss: 0.10412 \n",
      "Validation loss decreased (0.138511 --> 0.129233).  Saving model ...\n",
      "[ 10/100] train_loss: 0.11404 valid_loss: 0.12234 test_loss: 0.09522 \n",
      "Validation loss decreased (0.129233 --> 0.122342).  Saving model ...\n",
      "[ 11/100] train_loss: 0.11072 valid_loss: 0.11289 test_loss: 0.08739 \n",
      "Validation loss decreased (0.122342 --> 0.112886).  Saving model ...\n",
      "[ 12/100] train_loss: 0.10307 valid_loss: 0.10470 test_loss: 0.08159 \n",
      "Validation loss decreased (0.112886 --> 0.104696).  Saving model ...\n",
      "[ 13/100] train_loss: 0.09869 valid_loss: 0.10014 test_loss: 0.07596 \n",
      "Validation loss decreased (0.104696 --> 0.100135).  Saving model ...\n",
      "[ 14/100] train_loss: 0.09038 valid_loss: 0.09478 test_loss: 0.07208 \n",
      "Validation loss decreased (0.100135 --> 0.094784).  Saving model ...\n",
      "[ 15/100] train_loss: 0.09134 valid_loss: 0.09316 test_loss: 0.06666 \n",
      "Validation loss decreased (0.094784 --> 0.093157).  Saving model ...\n",
      "[ 16/100] train_loss: 0.08953 valid_loss: 0.08884 test_loss: 0.06483 \n",
      "Validation loss decreased (0.093157 --> 0.088843).  Saving model ...\n",
      "[ 17/100] train_loss: 0.08483 valid_loss: 0.08940 test_loss: 0.06075 \n",
      "[ 18/100] train_loss: 0.08716 valid_loss: 0.08831 test_loss: 0.05945 \n",
      "Validation loss decreased (0.088843 --> 0.088312).  Saving model ...\n",
      "[ 19/100] train_loss: 0.08238 valid_loss: 0.08777 test_loss: 0.05676 \n",
      "Validation loss decreased (0.088312 --> 0.087769).  Saving model ...\n",
      "[ 20/100] train_loss: 0.08183 valid_loss: 0.08370 test_loss: 0.05569 \n",
      "Validation loss decreased (0.087769 --> 0.083697).  Saving model ...\n",
      "[ 21/100] train_loss: 0.07671 valid_loss: 0.08400 test_loss: 0.05306 \n",
      "[ 22/100] train_loss: 0.07905 valid_loss: 0.08715 test_loss: 0.05095 \n",
      "[ 23/100] train_loss: 0.08031 valid_loss: 0.08197 test_loss: 0.05615 \n",
      "Validation loss decreased (0.083697 --> 0.081970).  Saving model ...\n",
      "[ 24/100] train_loss: 0.07367 valid_loss: 0.08298 test_loss: 0.04924 \n",
      "[ 25/100] train_loss: 0.07661 valid_loss: 0.08266 test_loss: 0.05006 \n",
      "[ 26/100] train_loss: 0.07514 valid_loss: 0.08586 test_loss: 0.04850 \n",
      "[ 27/100] train_loss: 0.08124 valid_loss: 0.08289 test_loss: 0.04926 \n",
      "[ 28/100] train_loss: 0.07834 valid_loss: 0.08181 test_loss: 0.04965 \n",
      "Validation loss decreased (0.081970 --> 0.081810).  Saving model ...\n",
      "[ 29/100] train_loss: 0.07606 valid_loss: 0.07995 test_loss: 0.05087 \n",
      "Validation loss decreased (0.081810 --> 0.079949).  Saving model ...\n",
      "[ 30/100] train_loss: 0.07747 valid_loss: 0.07982 test_loss: 0.04840 \n",
      "Validation loss decreased (0.079949 --> 0.079818).  Saving model ...\n",
      "[ 31/100] train_loss: 0.07285 valid_loss: 0.08436 test_loss: 0.05045 \n",
      "[ 32/100] train_loss: 0.07297 valid_loss: 0.07820 test_loss: 0.04640 \n",
      "Validation loss decreased (0.079818 --> 0.078200).  Saving model ...\n",
      "[ 33/100] train_loss: 0.07134 valid_loss: 0.08208 test_loss: 0.04610 \n",
      "[ 34/100] train_loss: 0.07058 valid_loss: 0.07999 test_loss: 0.04827 \n",
      "[ 35/100] train_loss: 0.07051 valid_loss: 0.07814 test_loss: 0.04636 \n",
      "Validation loss decreased (0.078200 --> 0.078143).  Saving model ...\n",
      "[ 36/100] train_loss: 0.07195 valid_loss: 0.08249 test_loss: 0.04629 \n",
      "[ 37/100] train_loss: 0.06955 valid_loss: 0.07905 test_loss: 0.04550 \n",
      "[ 38/100] train_loss: 0.07077 valid_loss: 0.07773 test_loss: 0.04476 \n",
      "Validation loss decreased (0.078143 --> 0.077731).  Saving model ...\n",
      "[ 39/100] train_loss: 0.06981 valid_loss: 0.08123 test_loss: 0.04814 \n",
      "[ 40/100] train_loss: 0.06721 valid_loss: 0.08019 test_loss: 0.04891 \n",
      "[ 41/100] train_loss: 0.06470 valid_loss: 0.07745 test_loss: 0.04044 \n",
      "Validation loss decreased (0.077731 --> 0.077449).  Saving model ...\n",
      "[ 42/100] train_loss: 0.06963 valid_loss: 0.09043 test_loss: 0.04306 \n",
      "[ 43/100] train_loss: 0.06565 valid_loss: 0.08134 test_loss: 0.04046 \n",
      "[ 44/100] train_loss: 0.06896 valid_loss: 0.07890 test_loss: 0.04186 \n",
      "[ 45/100] train_loss: 0.06585 valid_loss: 0.08003 test_loss: 0.03746 \n",
      "[ 46/100] train_loss: 0.06627 valid_loss: 0.08112 test_loss: 0.03859 \n",
      "[ 47/100] train_loss: 0.06620 valid_loss: 0.08191 test_loss: 0.04157 \n",
      "[ 48/100] train_loss: 0.06710 valid_loss: 0.08010 test_loss: 0.03838 \n",
      "[ 49/100] train_loss: 0.06683 valid_loss: 0.08150 test_loss: 0.03850 \n",
      "[ 50/100] train_loss: 0.06805 valid_loss: 0.08060 test_loss: 0.03930 \n",
      "[ 51/100] train_loss: 0.06380 valid_loss: 0.07948 test_loss: 0.03888 \n",
      "[ 52/100] train_loss: 0.06581 valid_loss: 0.07967 test_loss: 0.03907 \n",
      "[ 53/100] train_loss: 0.06304 valid_loss: 0.08563 test_loss: 0.03820 \n",
      "[ 54/100] train_loss: 0.06451 valid_loss: 0.07447 test_loss: 0.03848 \n",
      "Validation loss decreased (0.077449 --> 0.074472).  Saving model ...\n",
      "[ 55/100] train_loss: 0.06521 valid_loss: 0.08486 test_loss: 0.04162 \n",
      "[ 56/100] train_loss: 0.06161 valid_loss: 0.07442 test_loss: 0.03563 \n",
      "Validation loss decreased (0.074472 --> 0.074418).  Saving model ...\n",
      "[ 57/100] train_loss: 0.06259 valid_loss: 0.07739 test_loss: 0.03684 \n",
      "[ 58/100] train_loss: 0.06212 valid_loss: 0.08085 test_loss: 0.03854 \n",
      "[ 59/100] train_loss: 0.05820 valid_loss: 0.07601 test_loss: 0.03943 \n",
      "[ 60/100] train_loss: 0.06149 valid_loss: 0.07499 test_loss: 0.04081 \n",
      "[ 61/100] train_loss: 0.06475 valid_loss: 0.07711 test_loss: 0.03804 \n",
      "[ 62/100] train_loss: 0.06119 valid_loss: 0.08120 test_loss: 0.03780 \n",
      "[ 63/100] train_loss: 0.05972 valid_loss: 0.07742 test_loss: 0.03625 \n",
      "[ 64/100] train_loss: 0.05916 valid_loss: 0.07809 test_loss: 0.03540 \n",
      "[ 65/100] train_loss: 0.05875 valid_loss: 0.07490 test_loss: 0.03590 \n",
      "[ 66/100] train_loss: 0.06166 valid_loss: 0.07482 test_loss: 0.03691 \n",
      "[ 67/100] train_loss: 0.06195 valid_loss: 0.07553 test_loss: 0.03556 \n",
      "[ 68/100] train_loss: 0.05940 valid_loss: 0.07750 test_loss: 0.03535 \n",
      "[ 69/100] train_loss: 0.05865 valid_loss: 0.07423 test_loss: 0.03667 \n",
      "Validation loss decreased (0.074418 --> 0.074226).  Saving model ...\n",
      "[ 70/100] train_loss: 0.05767 valid_loss: 0.08197 test_loss: 0.03581 \n",
      "[ 71/100] train_loss: 0.05929 valid_loss: 0.07819 test_loss: 0.03676 \n",
      "[ 72/100] train_loss: 0.05615 valid_loss: 0.08490 test_loss: 0.03299 \n",
      "[ 73/100] train_loss: 0.05830 valid_loss: 0.07849 test_loss: 0.03723 \n",
      "[ 74/100] train_loss: 0.05635 valid_loss: 0.08514 test_loss: 0.03561 \n",
      "[ 75/100] train_loss: 0.05875 valid_loss: 0.08291 test_loss: 0.03588 \n",
      "[ 76/100] train_loss: 0.06112 valid_loss: 0.08905 test_loss: 0.03914 \n",
      "[ 77/100] train_loss: 0.06022 valid_loss: 0.08134 test_loss: 0.03342 \n",
      "[ 78/100] train_loss: 0.05738 valid_loss: 0.08193 test_loss: 0.03339 \n",
      "[ 79/100] train_loss: 0.06212 valid_loss: 0.08478 test_loss: 0.03583 \n",
      "[ 80/100] train_loss: 0.05953 valid_loss: 0.07625 test_loss: 0.03155 \n",
      "[ 81/100] train_loss: 0.04923 valid_loss: 0.07818 test_loss: 0.03093 \n",
      "[ 82/100] train_loss: 0.05865 valid_loss: 0.08544 test_loss: 0.03558 \n",
      "[ 83/100] train_loss: 0.05679 valid_loss: 0.07968 test_loss: 0.03288 \n",
      "[ 84/100] train_loss: 0.05589 valid_loss: 0.08303 test_loss: 0.03387 \n",
      "[ 85/100] train_loss: 0.05796 valid_loss: 0.07674 test_loss: 0.03147 \n",
      "[ 86/100] train_loss: 0.06045 valid_loss: 0.08733 test_loss: 0.03781 \n",
      "[ 87/100] train_loss: 0.05678 valid_loss: 0.07682 test_loss: 0.03166 \n",
      "[ 88/100] train_loss: 0.05683 valid_loss: 0.07541 test_loss: 0.03832 \n",
      "[ 89/100] train_loss: 0.05912 valid_loss: 0.07968 test_loss: 0.03162 \n",
      "[ 90/100] train_loss: 0.05603 valid_loss: 0.08355 test_loss: 0.03649 \n",
      "[ 91/100] train_loss: 0.05700 valid_loss: 0.07696 test_loss: 0.03127 \n",
      "[ 92/100] train_loss: 0.05659 valid_loss: 0.07754 test_loss: 0.03143 \n",
      "[ 93/100] train_loss: 0.06022 valid_loss: 0.08340 test_loss: 0.03622 \n",
      "[ 94/100] train_loss: 0.05713 valid_loss: 0.08318 test_loss: 0.02913 \n",
      "[ 95/100] train_loss: 0.05721 valid_loss: 0.07560 test_loss: 0.03491 \n",
      "[ 96/100] train_loss: 0.06418 valid_loss: 0.08139 test_loss: 0.03374 \n",
      "[ 97/100] train_loss: 0.05637 valid_loss: 0.08790 test_loss: 0.03294 \n",
      "[ 98/100] train_loss: 0.05323 valid_loss: 0.08049 test_loss: 0.03017 \n",
      "[ 99/100] train_loss: 0.05814 valid_loss: 0.07765 test_loss: 0.03370 \n",
      "[100/100] train_loss: 0.05789 valid_loss: 0.07524 test_loss: 0.03300 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = Epochs\n",
    "\n",
    "train_loader = dl_train_seen\n",
    "valid_loader = dl_valid_seen\n",
    "test_loader = dl_test_seen\n",
    "\n",
    "#i = 0\n",
    "for i in range(jb):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = TTRNet(1,4,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-5)\n",
    "    criterion = FocalLoss(pos_weight=torch.tensor([1.23,1.5,1,1]), gamma=2)\n",
    "    fn = './seen/UKDALE_seen_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sh-_NRsnPkdC"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzBMEXOuEDrj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL 0\n",
      "[  1/100] train_loss: 0.18451 valid_loss: 0.19308 test_loss: 0.18622 \n",
      "Validation loss decreased (inf --> 0.193082).  Saving model ...\n",
      "[  2/100] train_loss: 0.18240 valid_loss: 0.19235 test_loss: 0.18493 \n",
      "Validation loss decreased (0.193082 --> 0.192349).  Saving model ...\n",
      "[  3/100] train_loss: 0.18158 valid_loss: 0.19101 test_loss: 0.18304 \n",
      "Validation loss decreased (0.192349 --> 0.191006).  Saving model ...\n",
      "[  4/100] train_loss: 0.17850 valid_loss: 0.18919 test_loss: 0.18064 \n",
      "Validation loss decreased (0.191006 --> 0.189193).  Saving model ...\n",
      "[  5/100] train_loss: 0.17667 valid_loss: 0.18729 test_loss: 0.17780 \n",
      "Validation loss decreased (0.189193 --> 0.187287).  Saving model ...\n",
      "[  6/100] train_loss: 0.17482 valid_loss: 0.18494 test_loss: 0.17506 \n",
      "Validation loss decreased (0.187287 --> 0.184936).  Saving model ...\n",
      "[  7/100] train_loss: 0.17310 valid_loss: 0.18238 test_loss: 0.17209 \n",
      "Validation loss decreased (0.184936 --> 0.182376).  Saving model ...\n",
      "[  8/100] train_loss: 0.16981 valid_loss: 0.17935 test_loss: 0.16887 \n",
      "Validation loss decreased (0.182376 --> 0.179351).  Saving model ...\n",
      "[  9/100] train_loss: 0.16772 valid_loss: 0.17693 test_loss: 0.16558 \n",
      "Validation loss decreased (0.179351 --> 0.176930).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16528 valid_loss: 0.17376 test_loss: 0.16232 \n",
      "Validation loss decreased (0.176930 --> 0.173757).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16139 valid_loss: 0.17154 test_loss: 0.15894 \n",
      "Validation loss decreased (0.173757 --> 0.171538).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15769 valid_loss: 0.16696 test_loss: 0.15540 \n",
      "Validation loss decreased (0.171538 --> 0.166965).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15429 valid_loss: 0.16205 test_loss: 0.15204 \n",
      "Validation loss decreased (0.166965 --> 0.162050).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15243 valid_loss: 0.15999 test_loss: 0.14808 \n",
      "Validation loss decreased (0.162050 --> 0.159989).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14807 valid_loss: 0.15653 test_loss: 0.14456 \n",
      "Validation loss decreased (0.159989 --> 0.156530).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14439 valid_loss: 0.15166 test_loss: 0.14078 \n",
      "Validation loss decreased (0.156530 --> 0.151658).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13965 valid_loss: 0.14962 test_loss: 0.13725 \n",
      "Validation loss decreased (0.151658 --> 0.149619).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13562 valid_loss: 0.14566 test_loss: 0.13404 \n",
      "Validation loss decreased (0.149619 --> 0.145658).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13401 valid_loss: 0.13997 test_loss: 0.12876 \n",
      "Validation loss decreased (0.145658 --> 0.139968).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12803 valid_loss: 0.13894 test_loss: 0.12565 \n",
      "Validation loss decreased (0.139968 --> 0.138942).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12607 valid_loss: 0.13881 test_loss: 0.12304 \n",
      "Validation loss decreased (0.138942 --> 0.138813).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11883 valid_loss: 0.12900 test_loss: 0.11835 \n",
      "Validation loss decreased (0.138813 --> 0.128997).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12300 valid_loss: 0.12567 test_loss: 0.11581 \n",
      "Validation loss decreased (0.128997 --> 0.125667).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11499 valid_loss: 0.12636 test_loss: 0.11271 \n",
      "[ 25/100] train_loss: 0.11627 valid_loss: 0.12067 test_loss: 0.10887 \n",
      "Validation loss decreased (0.125667 --> 0.120669).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11148 valid_loss: 0.11484 test_loss: 0.10555 \n",
      "Validation loss decreased (0.120669 --> 0.114840).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11100 valid_loss: 0.11028 test_loss: 0.10343 \n",
      "Validation loss decreased (0.114840 --> 0.110284).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10231 valid_loss: 0.11410 test_loss: 0.10129 \n",
      "[ 29/100] train_loss: 0.09941 valid_loss: 0.11452 test_loss: 0.09928 \n",
      "[ 30/100] train_loss: 0.09586 valid_loss: 0.10502 test_loss: 0.09495 \n",
      "Validation loss decreased (0.110284 --> 0.105021).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09571 valid_loss: 0.10910 test_loss: 0.09437 \n",
      "[ 32/100] train_loss: 0.09447 valid_loss: 0.09908 test_loss: 0.09119 \n",
      "Validation loss decreased (0.105021 --> 0.099080).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09547 valid_loss: 0.10236 test_loss: 0.08959 \n",
      "[ 34/100] train_loss: 0.09530 valid_loss: 0.10822 test_loss: 0.09003 \n",
      "[ 35/100] train_loss: 0.08571 valid_loss: 0.10005 test_loss: 0.08626 \n",
      "[ 36/100] train_loss: 0.09680 valid_loss: 0.09892 test_loss: 0.08495 \n",
      "Validation loss decreased (0.099080 --> 0.098924).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09188 valid_loss: 0.09349 test_loss: 0.08396 \n",
      "Validation loss decreased (0.098924 --> 0.093486).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08286 valid_loss: 0.10179 test_loss: 0.08416 \n",
      "[ 39/100] train_loss: 0.09077 valid_loss: 0.09884 test_loss: 0.08221 \n",
      "[ 40/100] train_loss: 0.08668 valid_loss: 0.08871 test_loss: 0.08044 \n",
      "Validation loss decreased (0.093486 --> 0.088711).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08635 valid_loss: 0.09583 test_loss: 0.07980 \n",
      "[ 42/100] train_loss: 0.08170 valid_loss: 0.09661 test_loss: 0.07944 \n",
      "[ 43/100] train_loss: 0.08482 valid_loss: 0.09105 test_loss: 0.07747 \n",
      "[ 44/100] train_loss: 0.07938 valid_loss: 0.08833 test_loss: 0.07693 \n",
      "Validation loss decreased (0.088711 --> 0.088334).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08079 valid_loss: 0.08835 test_loss: 0.07603 \n",
      "[ 46/100] train_loss: 0.08204 valid_loss: 0.09573 test_loss: 0.07711 \n",
      "[ 47/100] train_loss: 0.08054 valid_loss: 0.08711 test_loss: 0.07444 \n",
      "Validation loss decreased (0.088334 --> 0.087108).  Saving model ...\n",
      "[ 48/100] train_loss: 0.08032 valid_loss: 0.09645 test_loss: 0.07586 \n",
      "[ 49/100] train_loss: 0.07966 valid_loss: 0.08577 test_loss: 0.07288 \n",
      "Validation loss decreased (0.087108 --> 0.085773).  Saving model ...\n",
      "[ 50/100] train_loss: 0.08681 valid_loss: 0.08643 test_loss: 0.07353 \n",
      "[ 51/100] train_loss: 0.08008 valid_loss: 0.09330 test_loss: 0.07406 \n",
      "[ 52/100] train_loss: 0.08343 valid_loss: 0.08712 test_loss: 0.07241 \n",
      "[ 53/100] train_loss: 0.08457 valid_loss: 0.08753 test_loss: 0.07208 \n",
      "[ 54/100] train_loss: 0.07875 valid_loss: 0.09261 test_loss: 0.07276 \n",
      "[ 55/100] train_loss: 0.08555 valid_loss: 0.08839 test_loss: 0.07116 \n",
      "[ 56/100] train_loss: 0.08360 valid_loss: 0.08484 test_loss: 0.07203 \n",
      "Validation loss decreased (0.085773 --> 0.084841).  Saving model ...\n",
      "[ 57/100] train_loss: 0.08222 valid_loss: 0.09014 test_loss: 0.07251 \n",
      "[ 58/100] train_loss: 0.07403 valid_loss: 0.08972 test_loss: 0.07118 \n",
      "[ 59/100] train_loss: 0.08243 valid_loss: 0.08517 test_loss: 0.06962 \n",
      "[ 60/100] train_loss: 0.07279 valid_loss: 0.09327 test_loss: 0.07112 \n",
      "[ 61/100] train_loss: 0.07995 valid_loss: 0.08598 test_loss: 0.06943 \n",
      "[ 62/100] train_loss: 0.08129 valid_loss: 0.08647 test_loss: 0.06928 \n",
      "[ 63/100] train_loss: 0.07465 valid_loss: 0.09409 test_loss: 0.07043 \n",
      "[ 64/100] train_loss: 0.07640 valid_loss: 0.08356 test_loss: 0.06735 \n",
      "Validation loss decreased (0.084841 --> 0.083556).  Saving model ...\n",
      "[ 65/100] train_loss: 0.07625 valid_loss: 0.08554 test_loss: 0.06794 \n",
      "[ 66/100] train_loss: 0.07047 valid_loss: 0.09284 test_loss: 0.06924 \n",
      "[ 67/100] train_loss: 0.06893 valid_loss: 0.08099 test_loss: 0.06717 \n",
      "Validation loss decreased (0.083556 --> 0.080985).  Saving model ...\n",
      "[ 68/100] train_loss: 0.07243 valid_loss: 0.08728 test_loss: 0.06698 \n",
      "[ 69/100] train_loss: 0.07905 valid_loss: 0.08929 test_loss: 0.06742 \n",
      "[ 70/100] train_loss: 0.07482 valid_loss: 0.08880 test_loss: 0.06806 \n",
      "[ 71/100] train_loss: 0.07156 valid_loss: 0.09378 test_loss: 0.06853 \n",
      "[ 72/100] train_loss: 0.06974 valid_loss: 0.08309 test_loss: 0.06506 \n",
      "[ 73/100] train_loss: 0.07327 valid_loss: 0.07954 test_loss: 0.06738 \n",
      "Validation loss decreased (0.080985 --> 0.079542).  Saving model ...\n",
      "[ 74/100] train_loss: 0.07731 valid_loss: 0.10361 test_loss: 0.07207 \n",
      "[ 75/100] train_loss: 0.07660 valid_loss: 0.10102 test_loss: 0.07062 \n",
      "[ 76/100] train_loss: 0.07204 valid_loss: 0.07836 test_loss: 0.06513 \n",
      "Validation loss decreased (0.079542 --> 0.078359).  Saving model ...\n",
      "[ 77/100] train_loss: 0.07634 valid_loss: 0.08397 test_loss: 0.06456 \n",
      "[ 78/100] train_loss: 0.07063 valid_loss: 0.10001 test_loss: 0.06918 \n",
      "[ 79/100] train_loss: 0.07447 valid_loss: 0.08211 test_loss: 0.06473 \n",
      "[ 80/100] train_loss: 0.07194 valid_loss: 0.08271 test_loss: 0.06497 \n",
      "[ 81/100] train_loss: 0.06818 valid_loss: 0.09462 test_loss: 0.06659 \n",
      "[ 82/100] train_loss: 0.07505 valid_loss: 0.08737 test_loss: 0.06438 \n",
      "[ 83/100] train_loss: 0.06353 valid_loss: 0.08233 test_loss: 0.06400 \n",
      "[ 84/100] train_loss: 0.07073 valid_loss: 0.08616 test_loss: 0.06392 \n",
      "[ 85/100] train_loss: 0.07234 valid_loss: 0.08981 test_loss: 0.06398 \n",
      "[ 86/100] train_loss: 0.06775 valid_loss: 0.08226 test_loss: 0.06276 \n",
      "[ 87/100] train_loss: 0.06741 valid_loss: 0.08678 test_loss: 0.06326 \n",
      "[ 88/100] train_loss: 0.06871 valid_loss: 0.08411 test_loss: 0.06320 \n",
      "[ 89/100] train_loss: 0.06742 valid_loss: 0.08118 test_loss: 0.06374 \n",
      "[ 90/100] train_loss: 0.07258 valid_loss: 0.09856 test_loss: 0.06710 \n",
      "[ 91/100] train_loss: 0.06187 valid_loss: 0.08414 test_loss: 0.06268 \n",
      "[ 92/100] train_loss: 0.07245 valid_loss: 0.08301 test_loss: 0.06217 \n",
      "[ 93/100] train_loss: 0.07227 valid_loss: 0.09258 test_loss: 0.06426 \n",
      "[ 94/100] train_loss: 0.06929 valid_loss: 0.08488 test_loss: 0.06275 \n",
      "[ 95/100] train_loss: 0.06171 valid_loss: 0.07997 test_loss: 0.06178 \n",
      "[ 96/100] train_loss: 0.07222 valid_loss: 0.08656 test_loss: 0.06204 \n",
      "[ 97/100] train_loss: 0.06962 valid_loss: 0.08307 test_loss: 0.06286 \n",
      "[ 98/100] train_loss: 0.06610 valid_loss: 0.08851 test_loss: 0.06296 \n",
      "[ 99/100] train_loss: 0.07156 valid_loss: 0.09508 test_loss: 0.06453 \n",
      "[100/100] train_loss: 0.07301 valid_loss: 0.08792 test_loss: 0.06227 \n",
      "TRAINING MODEL 1\n",
      "[  1/100] train_loss: 0.18703 valid_loss: 0.19526 test_loss: 0.18624 \n",
      "Validation loss decreased (inf --> 0.195262).  Saving model ...\n",
      "[  2/100] train_loss: 0.18448 valid_loss: 0.19384 test_loss: 0.18503 \n",
      "Validation loss decreased (0.195262 --> 0.193842).  Saving model ...\n",
      "[  3/100] train_loss: 0.18178 valid_loss: 0.19193 test_loss: 0.18321 \n",
      "Validation loss decreased (0.193842 --> 0.191931).  Saving model ...\n",
      "[  4/100] train_loss: 0.18065 valid_loss: 0.18934 test_loss: 0.18077 \n",
      "Validation loss decreased (0.191931 --> 0.189345).  Saving model ...\n",
      "[  5/100] train_loss: 0.17767 valid_loss: 0.18690 test_loss: 0.17814 \n",
      "Validation loss decreased (0.189345 --> 0.186902).  Saving model ...\n",
      "[  6/100] train_loss: 0.17408 valid_loss: 0.18393 test_loss: 0.17521 \n",
      "Validation loss decreased (0.186902 --> 0.183926).  Saving model ...\n",
      "[  7/100] train_loss: 0.17308 valid_loss: 0.18099 test_loss: 0.17214 \n",
      "Validation loss decreased (0.183926 --> 0.180993).  Saving model ...\n",
      "[  8/100] train_loss: 0.16967 valid_loss: 0.17791 test_loss: 0.16912 \n",
      "Validation loss decreased (0.180993 --> 0.177913).  Saving model ...\n",
      "[  9/100] train_loss: 0.16891 valid_loss: 0.17595 test_loss: 0.16649 \n",
      "Validation loss decreased (0.177913 --> 0.175951).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16633 valid_loss: 0.17257 test_loss: 0.16367 \n",
      "Validation loss decreased (0.175951 --> 0.172565).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16350 valid_loss: 0.16842 test_loss: 0.16043 \n",
      "Validation loss decreased (0.172565 --> 0.168422).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15848 valid_loss: 0.16685 test_loss: 0.15843 \n",
      "Validation loss decreased (0.168422 --> 0.166846).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15796 valid_loss: 0.16376 test_loss: 0.15574 \n",
      "Validation loss decreased (0.166846 --> 0.163756).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15308 valid_loss: 0.16063 test_loss: 0.15160 \n",
      "Validation loss decreased (0.163756 --> 0.160627).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14854 valid_loss: 0.15797 test_loss: 0.14889 \n",
      "Validation loss decreased (0.160627 --> 0.157966).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14639 valid_loss: 0.15218 test_loss: 0.14493 \n",
      "Validation loss decreased (0.157966 --> 0.152184).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14388 valid_loss: 0.14703 test_loss: 0.14108 \n",
      "Validation loss decreased (0.152184 --> 0.147027).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13739 valid_loss: 0.14397 test_loss: 0.13887 \n",
      "Validation loss decreased (0.147027 --> 0.143968).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13505 valid_loss: 0.14397 test_loss: 0.13536 \n",
      "[ 20/100] train_loss: 0.13329 valid_loss: 0.13713 test_loss: 0.12998 \n",
      "Validation loss decreased (0.143968 --> 0.137126).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12437 valid_loss: 0.13276 test_loss: 0.12737 \n",
      "Validation loss decreased (0.137126 --> 0.132765).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12558 valid_loss: 0.13136 test_loss: 0.12310 \n",
      "Validation loss decreased (0.132765 --> 0.131359).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12162 valid_loss: 0.13005 test_loss: 0.11874 \n",
      "Validation loss decreased (0.131359 --> 0.130045).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11633 valid_loss: 0.12048 test_loss: 0.11486 \n",
      "Validation loss decreased (0.130045 --> 0.120478).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11347 valid_loss: 0.11906 test_loss: 0.11295 \n",
      "Validation loss decreased (0.120478 --> 0.119064).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11386 valid_loss: 0.11810 test_loss: 0.11006 \n",
      "Validation loss decreased (0.119064 --> 0.118095).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10558 valid_loss: 0.11094 test_loss: 0.10672 \n",
      "Validation loss decreased (0.118095 --> 0.110939).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10660 valid_loss: 0.11201 test_loss: 0.10380 \n",
      "[ 29/100] train_loss: 0.10425 valid_loss: 0.11618 test_loss: 0.10333 \n",
      "[ 30/100] train_loss: 0.09690 valid_loss: 0.11035 test_loss: 0.09967 \n",
      "Validation loss decreased (0.110939 --> 0.110346).  Saving model ...\n",
      "[ 31/100] train_loss: 0.10095 valid_loss: 0.11134 test_loss: 0.09825 \n",
      "[ 32/100] train_loss: 0.09592 valid_loss: 0.10085 test_loss: 0.09591 \n",
      "Validation loss decreased (0.110346 --> 0.100853).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09623 valid_loss: 0.09878 test_loss: 0.09233 \n",
      "Validation loss decreased (0.100853 --> 0.098779).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09559 valid_loss: 0.10157 test_loss: 0.09051 \n",
      "[ 35/100] train_loss: 0.09102 valid_loss: 0.09744 test_loss: 0.08945 \n",
      "Validation loss decreased (0.098779 --> 0.097440).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09141 valid_loss: 0.09980 test_loss: 0.08842 \n",
      "[ 37/100] train_loss: 0.08548 valid_loss: 0.09555 test_loss: 0.08648 \n",
      "Validation loss decreased (0.097440 --> 0.095547).  Saving model ...\n",
      "[ 38/100] train_loss: 0.09348 valid_loss: 0.09647 test_loss: 0.08481 \n",
      "[ 39/100] train_loss: 0.09128 valid_loss: 0.09883 test_loss: 0.08456 \n",
      "[ 40/100] train_loss: 0.08313 valid_loss: 0.09411 test_loss: 0.08342 \n",
      "Validation loss decreased (0.095547 --> 0.094112).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08461 valid_loss: 0.09144 test_loss: 0.08182 \n",
      "Validation loss decreased (0.094112 --> 0.091441).  Saving model ...\n",
      "[ 42/100] train_loss: 0.08536 valid_loss: 0.09603 test_loss: 0.08140 \n",
      "[ 43/100] train_loss: 0.08279 valid_loss: 0.09524 test_loss: 0.08041 \n",
      "[ 44/100] train_loss: 0.08410 valid_loss: 0.09058 test_loss: 0.07846 \n",
      "Validation loss decreased (0.091441 --> 0.090576).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08697 valid_loss: 0.09486 test_loss: 0.07901 \n",
      "[ 46/100] train_loss: 0.08072 valid_loss: 0.09361 test_loss: 0.07754 \n",
      "[ 47/100] train_loss: 0.07710 valid_loss: 0.09090 test_loss: 0.07624 \n",
      "[ 48/100] train_loss: 0.08137 valid_loss: 0.09072 test_loss: 0.07519 \n",
      "[ 49/100] train_loss: 0.08396 valid_loss: 0.09031 test_loss: 0.07488 \n",
      "Validation loss decreased (0.090576 --> 0.090314).  Saving model ...\n",
      "[ 50/100] train_loss: 0.07778 valid_loss: 0.09269 test_loss: 0.07543 \n",
      "[ 51/100] train_loss: 0.08415 valid_loss: 0.08888 test_loss: 0.07619 \n",
      "Validation loss decreased (0.090314 --> 0.088884).  Saving model ...\n",
      "[ 52/100] train_loss: 0.07735 valid_loss: 0.08960 test_loss: 0.07424 \n",
      "[ 53/100] train_loss: 0.08155 valid_loss: 0.09015 test_loss: 0.07300 \n",
      "[ 54/100] train_loss: 0.07558 valid_loss: 0.08542 test_loss: 0.07234 \n",
      "Validation loss decreased (0.088884 --> 0.085420).  Saving model ...\n",
      "[ 55/100] train_loss: 0.07605 valid_loss: 0.08596 test_loss: 0.07209 \n",
      "[ 56/100] train_loss: 0.07793 valid_loss: 0.09092 test_loss: 0.07197 \n",
      "[ 57/100] train_loss: 0.06954 valid_loss: 0.09255 test_loss: 0.07227 \n",
      "[ 58/100] train_loss: 0.07354 valid_loss: 0.09218 test_loss: 0.07234 \n",
      "[ 59/100] train_loss: 0.07725 valid_loss: 0.08545 test_loss: 0.07079 \n",
      "[ 60/100] train_loss: 0.06935 valid_loss: 0.08790 test_loss: 0.07093 \n",
      "[ 61/100] train_loss: 0.06700 valid_loss: 0.08814 test_loss: 0.06924 \n",
      "[ 62/100] train_loss: 0.07632 valid_loss: 0.08286 test_loss: 0.06822 \n",
      "Validation loss decreased (0.085420 --> 0.082861).  Saving model ...\n",
      "[ 63/100] train_loss: 0.07909 valid_loss: 0.08286 test_loss: 0.07189 \n",
      "[ 64/100] train_loss: 0.07800 valid_loss: 0.08964 test_loss: 0.07018 \n",
      "[ 65/100] train_loss: 0.07758 valid_loss: 0.08596 test_loss: 0.06931 \n",
      "[ 66/100] train_loss: 0.07492 valid_loss: 0.08699 test_loss: 0.06834 \n",
      "[ 67/100] train_loss: 0.07441 valid_loss: 0.08606 test_loss: 0.06731 \n",
      "[ 68/100] train_loss: 0.06957 valid_loss: 0.08809 test_loss: 0.06695 \n",
      "[ 69/100] train_loss: 0.07523 valid_loss: 0.08174 test_loss: 0.06725 \n",
      "Validation loss decreased (0.082861 --> 0.081744).  Saving model ...\n",
      "[ 70/100] train_loss: 0.07543 valid_loss: 0.08350 test_loss: 0.06879 \n",
      "[ 71/100] train_loss: 0.06758 valid_loss: 0.08471 test_loss: 0.06708 \n",
      "[ 72/100] train_loss: 0.07064 valid_loss: 0.08457 test_loss: 0.06573 \n",
      "[ 73/100] train_loss: 0.06322 valid_loss: 0.08511 test_loss: 0.06525 \n",
      "[ 74/100] train_loss: 0.06895 valid_loss: 0.08678 test_loss: 0.06543 \n",
      "[ 75/100] train_loss: 0.06670 valid_loss: 0.08532 test_loss: 0.06687 \n",
      "[ 76/100] train_loss: 0.07205 valid_loss: 0.08866 test_loss: 0.06686 \n",
      "[ 77/100] train_loss: 0.06917 valid_loss: 0.08433 test_loss: 0.06543 \n",
      "[ 78/100] train_loss: 0.06925 valid_loss: 0.08334 test_loss: 0.06429 \n",
      "[ 79/100] train_loss: 0.07197 valid_loss: 0.08529 test_loss: 0.06461 \n",
      "[ 80/100] train_loss: 0.06691 valid_loss: 0.08649 test_loss: 0.06539 \n",
      "[ 81/100] train_loss: 0.06752 valid_loss: 0.08098 test_loss: 0.06499 \n",
      "Validation loss decreased (0.081744 --> 0.080984).  Saving model ...\n",
      "[ 82/100] train_loss: 0.06547 valid_loss: 0.08735 test_loss: 0.06504 \n",
      "[ 83/100] train_loss: 0.06710 valid_loss: 0.08360 test_loss: 0.06474 \n",
      "[ 84/100] train_loss: 0.06752 valid_loss: 0.08127 test_loss: 0.06380 \n",
      "[ 85/100] train_loss: 0.06184 valid_loss: 0.08967 test_loss: 0.06451 \n",
      "[ 86/100] train_loss: 0.06873 valid_loss: 0.08827 test_loss: 0.06369 \n",
      "[ 87/100] train_loss: 0.07197 valid_loss: 0.08088 test_loss: 0.06599 \n",
      "Validation loss decreased (0.080984 --> 0.080878).  Saving model ...\n",
      "[ 88/100] train_loss: 0.06410 valid_loss: 0.08758 test_loss: 0.06444 \n",
      "[ 89/100] train_loss: 0.07095 valid_loss: 0.09336 test_loss: 0.06519 \n",
      "[ 90/100] train_loss: 0.07104 valid_loss: 0.07763 test_loss: 0.06419 \n",
      "Validation loss decreased (0.080878 --> 0.077626).  Saving model ...\n",
      "[ 91/100] train_loss: 0.06506 valid_loss: 0.08403 test_loss: 0.06362 \n",
      "[ 92/100] train_loss: 0.06511 valid_loss: 0.08991 test_loss: 0.06473 \n",
      "[ 93/100] train_loss: 0.07410 valid_loss: 0.08294 test_loss: 0.06510 \n",
      "[ 94/100] train_loss: 0.07106 valid_loss: 0.08488 test_loss: 0.06664 \n",
      "[ 95/100] train_loss: 0.06593 valid_loss: 0.08934 test_loss: 0.06412 \n",
      "[ 96/100] train_loss: 0.06566 valid_loss: 0.08513 test_loss: 0.06231 \n",
      "[ 97/100] train_loss: 0.06842 valid_loss: 0.07921 test_loss: 0.06300 \n",
      "[ 98/100] train_loss: 0.07975 valid_loss: 0.08542 test_loss: 0.06474 \n",
      "[ 99/100] train_loss: 0.06523 valid_loss: 0.08634 test_loss: 0.06536 \n",
      "[100/100] train_loss: 0.06443 valid_loss: 0.08758 test_loss: 0.06217 \n",
      "TRAINING MODEL 2\n",
      "[  1/100] train_loss: 0.18720 valid_loss: 0.19495 test_loss: 0.18603 \n",
      "Validation loss decreased (inf --> 0.194948).  Saving model ...\n",
      "[  2/100] train_loss: 0.18363 valid_loss: 0.19345 test_loss: 0.18467 \n",
      "Validation loss decreased (0.194948 --> 0.193452).  Saving model ...\n",
      "[  3/100] train_loss: 0.18110 valid_loss: 0.19184 test_loss: 0.18279 \n",
      "Validation loss decreased (0.193452 --> 0.191837).  Saving model ...\n",
      "[  4/100] train_loss: 0.17894 valid_loss: 0.18969 test_loss: 0.18026 \n",
      "Validation loss decreased (0.191837 --> 0.189690).  Saving model ...\n",
      "[  5/100] train_loss: 0.17569 valid_loss: 0.18758 test_loss: 0.17724 \n",
      "Validation loss decreased (0.189690 --> 0.187578).  Saving model ...\n",
      "[  6/100] train_loss: 0.17368 valid_loss: 0.18596 test_loss: 0.17426 \n",
      "Validation loss decreased (0.187578 --> 0.185957).  Saving model ...\n",
      "[  7/100] train_loss: 0.17147 valid_loss: 0.18373 test_loss: 0.17130 \n",
      "Validation loss decreased (0.185957 --> 0.183731).  Saving model ...\n",
      "[  8/100] train_loss: 0.16940 valid_loss: 0.18056 test_loss: 0.16798 \n",
      "Validation loss decreased (0.183731 --> 0.180564).  Saving model ...\n",
      "[  9/100] train_loss: 0.16510 valid_loss: 0.17746 test_loss: 0.16422 \n",
      "Validation loss decreased (0.180564 --> 0.177463).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16245 valid_loss: 0.17420 test_loss: 0.16017 \n",
      "Validation loss decreased (0.177463 --> 0.174201).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15847 valid_loss: 0.17038 test_loss: 0.15657 \n",
      "Validation loss decreased (0.174201 --> 0.170381).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15596 valid_loss: 0.16817 test_loss: 0.15359 \n",
      "Validation loss decreased (0.170381 --> 0.168174).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15143 valid_loss: 0.16373 test_loss: 0.15052 \n",
      "Validation loss decreased (0.168174 --> 0.163732).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14468 valid_loss: 0.16066 test_loss: 0.14768 \n",
      "Validation loss decreased (0.163732 --> 0.160655).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14251 valid_loss: 0.15809 test_loss: 0.14446 \n",
      "Validation loss decreased (0.160655 --> 0.158093).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13956 valid_loss: 0.15324 test_loss: 0.13921 \n",
      "Validation loss decreased (0.158093 --> 0.153238).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13616 valid_loss: 0.14518 test_loss: 0.13523 \n",
      "Validation loss decreased (0.153238 --> 0.145176).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13082 valid_loss: 0.14431 test_loss: 0.13225 \n",
      "Validation loss decreased (0.145176 --> 0.144305).  Saving model ...\n",
      "[ 19/100] train_loss: 0.12731 valid_loss: 0.14716 test_loss: 0.13042 \n",
      "[ 20/100] train_loss: 0.12039 valid_loss: 0.13485 test_loss: 0.12491 \n",
      "Validation loss decreased (0.144305 --> 0.134853).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12032 valid_loss: 0.12988 test_loss: 0.11796 \n",
      "Validation loss decreased (0.134853 --> 0.129881).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11529 valid_loss: 0.12827 test_loss: 0.11459 \n",
      "Validation loss decreased (0.129881 --> 0.128268).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11092 valid_loss: 0.12266 test_loss: 0.11127 \n",
      "Validation loss decreased (0.128268 --> 0.122657).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11051 valid_loss: 0.11807 test_loss: 0.10808 \n",
      "Validation loss decreased (0.122657 --> 0.118073).  Saving model ...\n",
      "[ 25/100] train_loss: 0.10666 valid_loss: 0.11940 test_loss: 0.10594 \n",
      "[ 26/100] train_loss: 0.10298 valid_loss: 0.12486 test_loss: 0.10586 \n",
      "[ 27/100] train_loss: 0.10614 valid_loss: 0.10997 test_loss: 0.09974 \n",
      "Validation loss decreased (0.118073 --> 0.109970).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10331 valid_loss: 0.10533 test_loss: 0.09726 \n",
      "Validation loss decreased (0.109970 --> 0.105334).  Saving model ...\n",
      "[ 29/100] train_loss: 0.09776 valid_loss: 0.10592 test_loss: 0.09545 \n",
      "[ 30/100] train_loss: 0.09623 valid_loss: 0.10460 test_loss: 0.09304 \n",
      "Validation loss decreased (0.105334 --> 0.104603).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09288 valid_loss: 0.10027 test_loss: 0.09065 \n",
      "Validation loss decreased (0.104603 --> 0.100273).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09326 valid_loss: 0.10227 test_loss: 0.08936 \n",
      "[ 33/100] train_loss: 0.09307 valid_loss: 0.10643 test_loss: 0.08932 \n",
      "[ 34/100] train_loss: 0.08995 valid_loss: 0.09748 test_loss: 0.08589 \n",
      "Validation loss decreased (0.100273 --> 0.097482).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09250 valid_loss: 0.09377 test_loss: 0.08480 \n",
      "Validation loss decreased (0.097482 --> 0.093768).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09339 valid_loss: 0.09476 test_loss: 0.08376 \n",
      "[ 37/100] train_loss: 0.08731 valid_loss: 0.09722 test_loss: 0.08313 \n",
      "[ 38/100] train_loss: 0.08434 valid_loss: 0.09588 test_loss: 0.08187 \n",
      "[ 39/100] train_loss: 0.08791 valid_loss: 0.09717 test_loss: 0.08031 \n",
      "[ 40/100] train_loss: 0.09036 valid_loss: 0.08716 test_loss: 0.08005 \n",
      "Validation loss decreased (0.093768 --> 0.087161).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08412 valid_loss: 0.09364 test_loss: 0.07875 \n",
      "[ 42/100] train_loss: 0.07885 valid_loss: 0.09277 test_loss: 0.07737 \n",
      "[ 43/100] train_loss: 0.07962 valid_loss: 0.08966 test_loss: 0.07605 \n",
      "[ 44/100] train_loss: 0.08723 valid_loss: 0.09190 test_loss: 0.07571 \n",
      "[ 45/100] train_loss: 0.08087 valid_loss: 0.09491 test_loss: 0.07615 \n",
      "[ 46/100] train_loss: 0.08553 valid_loss: 0.08424 test_loss: 0.07518 \n",
      "Validation loss decreased (0.087161 --> 0.084238).  Saving model ...\n",
      "[ 47/100] train_loss: 0.08416 valid_loss: 0.08456 test_loss: 0.07541 \n",
      "[ 48/100] train_loss: 0.08350 valid_loss: 0.08667 test_loss: 0.07513 \n",
      "[ 49/100] train_loss: 0.07457 valid_loss: 0.08929 test_loss: 0.07288 \n",
      "[ 50/100] train_loss: 0.07538 valid_loss: 0.08482 test_loss: 0.07158 \n",
      "[ 51/100] train_loss: 0.07886 valid_loss: 0.08799 test_loss: 0.07145 \n",
      "[ 52/100] train_loss: 0.07368 valid_loss: 0.08809 test_loss: 0.07117 \n",
      "[ 53/100] train_loss: 0.07035 valid_loss: 0.08874 test_loss: 0.07064 \n",
      "[ 54/100] train_loss: 0.07814 valid_loss: 0.08247 test_loss: 0.07069 \n",
      "Validation loss decreased (0.084238 --> 0.082467).  Saving model ...\n",
      "[ 55/100] train_loss: 0.07536 valid_loss: 0.08631 test_loss: 0.07015 \n",
      "[ 56/100] train_loss: 0.08119 valid_loss: 0.08944 test_loss: 0.06967 \n",
      "[ 57/100] train_loss: 0.07576 valid_loss: 0.08860 test_loss: 0.06950 \n",
      "[ 58/100] train_loss: 0.07265 valid_loss: 0.08691 test_loss: 0.06863 \n",
      "[ 59/100] train_loss: 0.07682 valid_loss: 0.08430 test_loss: 0.06828 \n",
      "[ 60/100] train_loss: 0.07697 valid_loss: 0.08432 test_loss: 0.06800 \n",
      "[ 61/100] train_loss: 0.07544 valid_loss: 0.08597 test_loss: 0.06835 \n",
      "[ 62/100] train_loss: 0.07229 valid_loss: 0.08676 test_loss: 0.06778 \n",
      "[ 63/100] train_loss: 0.07626 valid_loss: 0.08822 test_loss: 0.06738 \n",
      "[ 64/100] train_loss: 0.07346 valid_loss: 0.08666 test_loss: 0.06688 \n",
      "[ 65/100] train_loss: 0.07334 valid_loss: 0.08755 test_loss: 0.06699 \n",
      "[ 66/100] train_loss: 0.07379 valid_loss: 0.08043 test_loss: 0.06654 \n",
      "Validation loss decreased (0.082467 --> 0.080434).  Saving model ...\n",
      "[ 67/100] train_loss: 0.07416 valid_loss: 0.08188 test_loss: 0.06648 \n",
      "[ 68/100] train_loss: 0.07458 valid_loss: 0.09469 test_loss: 0.06843 \n",
      "[ 69/100] train_loss: 0.07500 valid_loss: 0.08100 test_loss: 0.06552 \n",
      "[ 70/100] train_loss: 0.07458 valid_loss: 0.07771 test_loss: 0.06705 \n",
      "Validation loss decreased (0.080434 --> 0.077712).  Saving model ...\n",
      "[ 71/100] train_loss: 0.07787 valid_loss: 0.08783 test_loss: 0.06664 \n",
      "[ 72/100] train_loss: 0.06937 valid_loss: 0.08696 test_loss: 0.06597 \n",
      "[ 73/100] train_loss: 0.07805 valid_loss: 0.08542 test_loss: 0.06515 \n",
      "[ 74/100] train_loss: 0.07406 valid_loss: 0.08911 test_loss: 0.06577 \n",
      "[ 75/100] train_loss: 0.06634 valid_loss: 0.08283 test_loss: 0.06424 \n",
      "[ 76/100] train_loss: 0.06689 valid_loss: 0.07868 test_loss: 0.06466 \n",
      "[ 77/100] train_loss: 0.07017 valid_loss: 0.07743 test_loss: 0.06628 \n",
      "Validation loss decreased (0.077712 --> 0.077432).  Saving model ...\n",
      "[ 78/100] train_loss: 0.06686 valid_loss: 0.08693 test_loss: 0.06427 \n",
      "[ 79/100] train_loss: 0.06272 valid_loss: 0.08823 test_loss: 0.06435 \n",
      "[ 80/100] train_loss: 0.06421 valid_loss: 0.08192 test_loss: 0.06304 \n",
      "[ 81/100] train_loss: 0.07131 valid_loss: 0.08037 test_loss: 0.06306 \n",
      "[ 82/100] train_loss: 0.07515 valid_loss: 0.08971 test_loss: 0.06478 \n",
      "[ 83/100] train_loss: 0.06752 valid_loss: 0.08007 test_loss: 0.06352 \n",
      "[ 84/100] train_loss: 0.07085 valid_loss: 0.08403 test_loss: 0.06376 \n",
      "[ 85/100] train_loss: 0.07207 valid_loss: 0.08952 test_loss: 0.06499 \n",
      "[ 86/100] train_loss: 0.06202 valid_loss: 0.08168 test_loss: 0.06289 \n",
      "[ 87/100] train_loss: 0.06801 valid_loss: 0.08316 test_loss: 0.06263 \n",
      "[ 88/100] train_loss: 0.06840 valid_loss: 0.08441 test_loss: 0.06298 \n",
      "[ 89/100] train_loss: 0.06601 valid_loss: 0.08258 test_loss: 0.06283 \n",
      "[ 90/100] train_loss: 0.07494 valid_loss: 0.08696 test_loss: 0.06307 \n",
      "[ 91/100] train_loss: 0.07467 valid_loss: 0.08702 test_loss: 0.06348 \n",
      "[ 92/100] train_loss: 0.06429 valid_loss: 0.08151 test_loss: 0.06165 \n",
      "[ 93/100] train_loss: 0.06859 valid_loss: 0.08437 test_loss: 0.06171 \n",
      "[ 94/100] train_loss: 0.06863 valid_loss: 0.08423 test_loss: 0.06140 \n",
      "[ 95/100] train_loss: 0.06824 valid_loss: 0.08607 test_loss: 0.06173 \n",
      "[ 96/100] train_loss: 0.06450 valid_loss: 0.08344 test_loss: 0.06130 \n",
      "[ 97/100] train_loss: 0.06459 valid_loss: 0.08369 test_loss: 0.06081 \n",
      "[ 98/100] train_loss: 0.06730 valid_loss: 0.08001 test_loss: 0.06072 \n",
      "[ 99/100] train_loss: 0.07271 valid_loss: 0.07520 test_loss: 0.06328 \n",
      "Validation loss decreased (0.077432 --> 0.075199).  Saving model ...\n",
      "[100/100] train_loss: 0.06833 valid_loss: 0.08446 test_loss: 0.06125 \n",
      "TRAINING MODEL 3\n",
      "[  1/100] train_loss: 0.18404 valid_loss: 0.19222 test_loss: 0.18601 \n",
      "Validation loss decreased (inf --> 0.192222).  Saving model ...\n",
      "[  2/100] train_loss: 0.18345 valid_loss: 0.19135 test_loss: 0.18487 \n",
      "Validation loss decreased (0.192222 --> 0.191352).  Saving model ...\n",
      "[  3/100] train_loss: 0.18157 valid_loss: 0.18977 test_loss: 0.18308 \n",
      "Validation loss decreased (0.191352 --> 0.189767).  Saving model ...\n",
      "[  4/100] train_loss: 0.17936 valid_loss: 0.18768 test_loss: 0.18048 \n",
      "Validation loss decreased (0.189767 --> 0.187685).  Saving model ...\n",
      "[  5/100] train_loss: 0.17668 valid_loss: 0.18539 test_loss: 0.17748 \n",
      "Validation loss decreased (0.187685 --> 0.185386).  Saving model ...\n",
      "[  6/100] train_loss: 0.17375 valid_loss: 0.18284 test_loss: 0.17465 \n",
      "Validation loss decreased (0.185386 --> 0.182843).  Saving model ...\n",
      "[  7/100] train_loss: 0.17153 valid_loss: 0.18066 test_loss: 0.17163 \n",
      "Validation loss decreased (0.182843 --> 0.180662).  Saving model ...\n",
      "[  8/100] train_loss: 0.17048 valid_loss: 0.17827 test_loss: 0.16860 \n",
      "Validation loss decreased (0.180662 --> 0.178269).  Saving model ...\n",
      "[  9/100] train_loss: 0.16710 valid_loss: 0.17552 test_loss: 0.16577 \n",
      "Validation loss decreased (0.178269 --> 0.175519).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16590 valid_loss: 0.17244 test_loss: 0.16242 \n",
      "Validation loss decreased (0.175519 --> 0.172444).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16139 valid_loss: 0.17024 test_loss: 0.15913 \n",
      "Validation loss decreased (0.172444 --> 0.170240).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15900 valid_loss: 0.16679 test_loss: 0.15619 \n",
      "Validation loss decreased (0.170240 --> 0.166788).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15649 valid_loss: 0.16410 test_loss: 0.15302 \n",
      "Validation loss decreased (0.166788 --> 0.164103).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15326 valid_loss: 0.16069 test_loss: 0.14978 \n",
      "Validation loss decreased (0.164103 --> 0.160689).  Saving model ...\n",
      "[ 15/100] train_loss: 0.15061 valid_loss: 0.15647 test_loss: 0.14585 \n",
      "Validation loss decreased (0.160689 --> 0.156470).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14497 valid_loss: 0.15483 test_loss: 0.14273 \n",
      "Validation loss decreased (0.156470 --> 0.154834).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14066 valid_loss: 0.15180 test_loss: 0.13930 \n",
      "Validation loss decreased (0.154834 --> 0.151796).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13913 valid_loss: 0.14695 test_loss: 0.13602 \n",
      "Validation loss decreased (0.151796 --> 0.146947).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13515 valid_loss: 0.14335 test_loss: 0.13275 \n",
      "Validation loss decreased (0.146947 --> 0.143354).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13230 valid_loss: 0.14098 test_loss: 0.12924 \n",
      "Validation loss decreased (0.143354 --> 0.140982).  Saving model ...\n",
      "[ 21/100] train_loss: 0.13053 valid_loss: 0.13444 test_loss: 0.12578 \n",
      "Validation loss decreased (0.140982 --> 0.134440).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12629 valid_loss: 0.13420 test_loss: 0.12205 \n",
      "Validation loss decreased (0.134440 --> 0.134198).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11967 valid_loss: 0.13205 test_loss: 0.11936 \n",
      "Validation loss decreased (0.134198 --> 0.132055).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11523 valid_loss: 0.12686 test_loss: 0.11573 \n",
      "Validation loss decreased (0.132055 --> 0.126863).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11381 valid_loss: 0.12411 test_loss: 0.11220 \n",
      "Validation loss decreased (0.126863 --> 0.124112).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11315 valid_loss: 0.12030 test_loss: 0.10927 \n",
      "Validation loss decreased (0.124112 --> 0.120303).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11393 valid_loss: 0.12165 test_loss: 0.10656 \n",
      "[ 28/100] train_loss: 0.10814 valid_loss: 0.11638 test_loss: 0.10397 \n",
      "Validation loss decreased (0.120303 --> 0.116381).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10288 valid_loss: 0.10951 test_loss: 0.10197 \n",
      "Validation loss decreased (0.116381 --> 0.109514).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10541 valid_loss: 0.11327 test_loss: 0.09864 \n",
      "[ 31/100] train_loss: 0.10080 valid_loss: 0.10914 test_loss: 0.09587 \n",
      "Validation loss decreased (0.109514 --> 0.109143).  Saving model ...\n",
      "[ 32/100] train_loss: 0.10053 valid_loss: 0.10352 test_loss: 0.09335 \n",
      "Validation loss decreased (0.109143 --> 0.103524).  Saving model ...\n",
      "[ 33/100] train_loss: 0.10280 valid_loss: 0.10405 test_loss: 0.09194 \n",
      "[ 34/100] train_loss: 0.09456 valid_loss: 0.10623 test_loss: 0.09102 \n",
      "[ 35/100] train_loss: 0.09277 valid_loss: 0.10203 test_loss: 0.08870 \n",
      "Validation loss decreased (0.103524 --> 0.102035).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09361 valid_loss: 0.09643 test_loss: 0.08778 \n",
      "Validation loss decreased (0.102035 --> 0.096429).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09383 valid_loss: 0.10233 test_loss: 0.08632 \n",
      "[ 38/100] train_loss: 0.09363 valid_loss: 0.09794 test_loss: 0.08433 \n",
      "[ 39/100] train_loss: 0.09318 valid_loss: 0.09472 test_loss: 0.08379 \n",
      "Validation loss decreased (0.096429 --> 0.094717).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08739 valid_loss: 0.09730 test_loss: 0.08244 \n",
      "[ 41/100] train_loss: 0.09383 valid_loss: 0.09657 test_loss: 0.08123 \n",
      "[ 42/100] train_loss: 0.08455 valid_loss: 0.09796 test_loss: 0.08049 \n",
      "[ 43/100] train_loss: 0.08250 valid_loss: 0.09112 test_loss: 0.07868 \n",
      "Validation loss decreased (0.094717 --> 0.091118).  Saving model ...\n",
      "[ 44/100] train_loss: 0.08793 valid_loss: 0.09088 test_loss: 0.07792 \n",
      "Validation loss decreased (0.091118 --> 0.090884).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08716 valid_loss: 0.08999 test_loss: 0.07776 \n",
      "Validation loss decreased (0.090884 --> 0.089990).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08666 valid_loss: 0.08938 test_loss: 0.07738 \n",
      "Validation loss decreased (0.089990 --> 0.089375).  Saving model ...\n",
      "[ 47/100] train_loss: 0.07918 valid_loss: 0.09607 test_loss: 0.07706 \n",
      "[ 48/100] train_loss: 0.08209 valid_loss: 0.09133 test_loss: 0.07533 \n",
      "[ 49/100] train_loss: 0.07762 valid_loss: 0.08826 test_loss: 0.07414 \n",
      "Validation loss decreased (0.089375 --> 0.088261).  Saving model ...\n",
      "[ 50/100] train_loss: 0.07992 valid_loss: 0.09703 test_loss: 0.07585 \n",
      "[ 51/100] train_loss: 0.07946 valid_loss: 0.08823 test_loss: 0.07286 \n",
      "Validation loss decreased (0.088261 --> 0.088230).  Saving model ...\n",
      "[ 52/100] train_loss: 0.08219 valid_loss: 0.08525 test_loss: 0.07297 \n",
      "Validation loss decreased (0.088230 --> 0.085252).  Saving model ...\n",
      "[ 53/100] train_loss: 0.08264 valid_loss: 0.08684 test_loss: 0.07290 \n",
      "[ 54/100] train_loss: 0.07149 valid_loss: 0.09354 test_loss: 0.07346 \n",
      "[ 55/100] train_loss: 0.07746 valid_loss: 0.09186 test_loss: 0.07260 \n",
      "[ 56/100] train_loss: 0.07525 valid_loss: 0.08832 test_loss: 0.07115 \n",
      "[ 57/100] train_loss: 0.07705 valid_loss: 0.08894 test_loss: 0.07091 \n",
      "[ 58/100] train_loss: 0.07361 valid_loss: 0.08619 test_loss: 0.07039 \n",
      "[ 59/100] train_loss: 0.07587 valid_loss: 0.08406 test_loss: 0.07052 \n",
      "Validation loss decreased (0.085252 --> 0.084062).  Saving model ...\n",
      "[ 60/100] train_loss: 0.07159 valid_loss: 0.08937 test_loss: 0.07011 \n",
      "[ 61/100] train_loss: 0.08006 valid_loss: 0.08908 test_loss: 0.06985 \n",
      "[ 62/100] train_loss: 0.07442 valid_loss: 0.08685 test_loss: 0.06945 \n",
      "[ 63/100] train_loss: 0.08022 valid_loss: 0.08613 test_loss: 0.06919 \n",
      "[ 64/100] train_loss: 0.07298 valid_loss: 0.08883 test_loss: 0.06881 \n",
      "[ 65/100] train_loss: 0.07707 valid_loss: 0.08807 test_loss: 0.06835 \n",
      "[ 66/100] train_loss: 0.07736 valid_loss: 0.08446 test_loss: 0.06783 \n",
      "[ 67/100] train_loss: 0.07897 valid_loss: 0.09046 test_loss: 0.06824 \n",
      "[ 68/100] train_loss: 0.07131 valid_loss: 0.08831 test_loss: 0.06747 \n",
      "[ 69/100] train_loss: 0.07682 valid_loss: 0.08892 test_loss: 0.06736 \n",
      "[ 70/100] train_loss: 0.07644 valid_loss: 0.08585 test_loss: 0.06708 \n",
      "[ 71/100] train_loss: 0.07353 valid_loss: 0.08680 test_loss: 0.06737 \n",
      "[ 72/100] train_loss: 0.07657 valid_loss: 0.09309 test_loss: 0.06851 \n",
      "[ 73/100] train_loss: 0.07709 valid_loss: 0.09055 test_loss: 0.06793 \n",
      "[ 74/100] train_loss: 0.07428 valid_loss: 0.08540 test_loss: 0.06626 \n",
      "[ 75/100] train_loss: 0.07114 valid_loss: 0.09410 test_loss: 0.06783 \n",
      "[ 76/100] train_loss: 0.07230 valid_loss: 0.08206 test_loss: 0.06635 \n",
      "Validation loss decreased (0.084062 --> 0.082056).  Saving model ...\n",
      "[ 77/100] train_loss: 0.07137 valid_loss: 0.08431 test_loss: 0.06626 \n",
      "[ 78/100] train_loss: 0.06998 valid_loss: 0.09417 test_loss: 0.06755 \n",
      "[ 79/100] train_loss: 0.07449 valid_loss: 0.08454 test_loss: 0.06550 \n",
      "[ 80/100] train_loss: 0.07048 valid_loss: 0.08239 test_loss: 0.06756 \n",
      "[ 81/100] train_loss: 0.07126 valid_loss: 0.08802 test_loss: 0.06652 \n",
      "[ 82/100] train_loss: 0.07037 valid_loss: 0.09847 test_loss: 0.06750 \n",
      "[ 83/100] train_loss: 0.06852 valid_loss: 0.09268 test_loss: 0.06547 \n",
      "[ 84/100] train_loss: 0.07656 valid_loss: 0.08090 test_loss: 0.06470 \n",
      "Validation loss decreased (0.082056 --> 0.080902).  Saving model ...\n",
      "[ 85/100] train_loss: 0.07234 valid_loss: 0.08756 test_loss: 0.06555 \n",
      "[ 86/100] train_loss: 0.07770 valid_loss: 0.09069 test_loss: 0.06556 \n",
      "[ 87/100] train_loss: 0.06453 valid_loss: 0.08595 test_loss: 0.06458 \n",
      "[ 88/100] train_loss: 0.06387 valid_loss: 0.09060 test_loss: 0.06431 \n",
      "[ 89/100] train_loss: 0.07030 valid_loss: 0.08681 test_loss: 0.06350 \n",
      "[ 90/100] train_loss: 0.07025 valid_loss: 0.08737 test_loss: 0.06361 \n",
      "[ 91/100] train_loss: 0.06072 valid_loss: 0.08719 test_loss: 0.06303 \n",
      "[ 92/100] train_loss: 0.07019 valid_loss: 0.08594 test_loss: 0.06265 \n",
      "[ 93/100] train_loss: 0.07431 valid_loss: 0.09143 test_loss: 0.06416 \n",
      "[ 94/100] train_loss: 0.07321 valid_loss: 0.08683 test_loss: 0.06347 \n",
      "[ 95/100] train_loss: 0.06716 valid_loss: 0.08422 test_loss: 0.06297 \n",
      "[ 96/100] train_loss: 0.06645 valid_loss: 0.08987 test_loss: 0.06280 \n",
      "[ 97/100] train_loss: 0.07212 valid_loss: 0.09018 test_loss: 0.06277 \n",
      "[ 98/100] train_loss: 0.06728 valid_loss: 0.08178 test_loss: 0.06251 \n",
      "[ 99/100] train_loss: 0.06491 valid_loss: 0.08801 test_loss: 0.06225 \n",
      "[100/100] train_loss: 0.06719 valid_loss: 0.08979 test_loss: 0.06231 \n",
      "TRAINING MODEL 4\n",
      "[  1/100] train_loss: 0.18605 valid_loss: 0.19362 test_loss: 0.18587 \n",
      "Validation loss decreased (inf --> 0.193624).  Saving model ...\n",
      "[  2/100] train_loss: 0.18264 valid_loss: 0.19284 test_loss: 0.18469 \n",
      "Validation loss decreased (0.193624 --> 0.192841).  Saving model ...\n",
      "[  3/100] train_loss: 0.17994 valid_loss: 0.19183 test_loss: 0.18303 \n",
      "Validation loss decreased (0.192841 --> 0.191825).  Saving model ...\n",
      "[  4/100] train_loss: 0.18112 valid_loss: 0.19013 test_loss: 0.18077 \n",
      "Validation loss decreased (0.191825 --> 0.190134).  Saving model ...\n",
      "[  5/100] train_loss: 0.17780 valid_loss: 0.18745 test_loss: 0.17794 \n",
      "Validation loss decreased (0.190134 --> 0.187452).  Saving model ...\n",
      "[  6/100] train_loss: 0.17579 valid_loss: 0.18470 test_loss: 0.17484 \n",
      "Validation loss decreased (0.187452 --> 0.184696).  Saving model ...\n",
      "[  7/100] train_loss: 0.17217 valid_loss: 0.18198 test_loss: 0.17177 \n",
      "Validation loss decreased (0.184696 --> 0.181980).  Saving model ...\n",
      "[  8/100] train_loss: 0.17153 valid_loss: 0.17877 test_loss: 0.16847 \n",
      "Validation loss decreased (0.181980 --> 0.178769).  Saving model ...\n",
      "[  9/100] train_loss: 0.16710 valid_loss: 0.17610 test_loss: 0.16515 \n",
      "Validation loss decreased (0.178769 --> 0.176098).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16517 valid_loss: 0.17233 test_loss: 0.16171 \n",
      "Validation loss decreased (0.176098 --> 0.172328).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16074 valid_loss: 0.16910 test_loss: 0.15906 \n",
      "Validation loss decreased (0.172328 --> 0.169097).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15756 valid_loss: 0.16620 test_loss: 0.15496 \n",
      "Validation loss decreased (0.169097 --> 0.166195).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15178 valid_loss: 0.16298 test_loss: 0.15103 \n",
      "Validation loss decreased (0.166195 --> 0.162979).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15035 valid_loss: 0.16224 test_loss: 0.14872 \n",
      "Validation loss decreased (0.162979 --> 0.162243).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14613 valid_loss: 0.15532 test_loss: 0.14563 \n",
      "Validation loss decreased (0.162243 --> 0.155319).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14377 valid_loss: 0.14838 test_loss: 0.14040 \n",
      "Validation loss decreased (0.155319 --> 0.148379).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13541 valid_loss: 0.15098 test_loss: 0.13800 \n",
      "[ 18/100] train_loss: 0.13856 valid_loss: 0.14674 test_loss: 0.13623 \n",
      "Validation loss decreased (0.148379 --> 0.146739).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13005 valid_loss: 0.13587 test_loss: 0.12819 \n",
      "Validation loss decreased (0.146739 --> 0.135874).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12531 valid_loss: 0.13539 test_loss: 0.12428 \n",
      "Validation loss decreased (0.135874 --> 0.135391).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12381 valid_loss: 0.13112 test_loss: 0.12033 \n",
      "Validation loss decreased (0.135391 --> 0.131118).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11910 valid_loss: 0.12812 test_loss: 0.11722 \n",
      "Validation loss decreased (0.131118 --> 0.128120).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11599 valid_loss: 0.12623 test_loss: 0.11459 \n",
      "Validation loss decreased (0.128120 --> 0.126229).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11955 valid_loss: 0.11650 test_loss: 0.11247 \n",
      "Validation loss decreased (0.126229 --> 0.116505).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11492 valid_loss: 0.11721 test_loss: 0.10912 \n",
      "[ 26/100] train_loss: 0.11195 valid_loss: 0.11946 test_loss: 0.10688 \n",
      "[ 27/100] train_loss: 0.10530 valid_loss: 0.11362 test_loss: 0.10313 \n",
      "Validation loss decreased (0.116505 --> 0.113624).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10291 valid_loss: 0.10635 test_loss: 0.10087 \n",
      "Validation loss decreased (0.113624 --> 0.106347).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10530 valid_loss: 0.10864 test_loss: 0.09885 \n",
      "[ 30/100] train_loss: 0.09364 valid_loss: 0.10812 test_loss: 0.09683 \n",
      "[ 31/100] train_loss: 0.09788 valid_loss: 0.10237 test_loss: 0.09351 \n",
      "Validation loss decreased (0.106347 --> 0.102374).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09722 valid_loss: 0.10190 test_loss: 0.09181 \n",
      "Validation loss decreased (0.102374 --> 0.101900).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09091 valid_loss: 0.09861 test_loss: 0.09013 \n",
      "Validation loss decreased (0.101900 --> 0.098607).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09168 valid_loss: 0.10241 test_loss: 0.08950 \n",
      "[ 35/100] train_loss: 0.09090 valid_loss: 0.09660 test_loss: 0.08695 \n",
      "Validation loss decreased (0.098607 --> 0.096605).  Saving model ...\n",
      "[ 36/100] train_loss: 0.08726 valid_loss: 0.09515 test_loss: 0.08526 \n",
      "Validation loss decreased (0.096605 --> 0.095151).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09267 valid_loss: 0.09103 test_loss: 0.08410 \n",
      "Validation loss decreased (0.095151 --> 0.091035).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08761 valid_loss: 0.09396 test_loss: 0.08296 \n",
      "[ 39/100] train_loss: 0.09065 valid_loss: 0.09134 test_loss: 0.08127 \n",
      "[ 40/100] train_loss: 0.08647 valid_loss: 0.08967 test_loss: 0.08028 \n",
      "Validation loss decreased (0.091035 --> 0.089667).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08234 valid_loss: 0.08955 test_loss: 0.07928 \n",
      "Validation loss decreased (0.089667 --> 0.089554).  Saving model ...\n",
      "[ 42/100] train_loss: 0.08720 valid_loss: 0.08762 test_loss: 0.07888 \n",
      "Validation loss decreased (0.089554 --> 0.087620).  Saving model ...\n",
      "[ 43/100] train_loss: 0.09531 valid_loss: 0.09285 test_loss: 0.07846 \n",
      "[ 44/100] train_loss: 0.08458 valid_loss: 0.08716 test_loss: 0.07768 \n",
      "Validation loss decreased (0.087620 --> 0.087160).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08356 valid_loss: 0.08839 test_loss: 0.07698 \n",
      "[ 46/100] train_loss: 0.07785 valid_loss: 0.08968 test_loss: 0.07601 \n",
      "[ 47/100] train_loss: 0.08598 valid_loss: 0.09285 test_loss: 0.07621 \n",
      "[ 48/100] train_loss: 0.08321 valid_loss: 0.09158 test_loss: 0.07549 \n",
      "[ 49/100] train_loss: 0.08159 valid_loss: 0.08613 test_loss: 0.07438 \n",
      "Validation loss decreased (0.087160 --> 0.086134).  Saving model ...\n",
      "[ 50/100] train_loss: 0.08110 valid_loss: 0.08862 test_loss: 0.07431 \n",
      "[ 51/100] train_loss: 0.07871 valid_loss: 0.08587 test_loss: 0.07308 \n",
      "Validation loss decreased (0.086134 --> 0.085874).  Saving model ...\n",
      "[ 52/100] train_loss: 0.07942 valid_loss: 0.08288 test_loss: 0.07284 \n",
      "Validation loss decreased (0.085874 --> 0.082884).  Saving model ...\n",
      "[ 53/100] train_loss: 0.07795 valid_loss: 0.08440 test_loss: 0.07225 \n",
      "[ 54/100] train_loss: 0.07978 valid_loss: 0.08434 test_loss: 0.07226 \n",
      "[ 55/100] train_loss: 0.07857 valid_loss: 0.08598 test_loss: 0.07188 \n",
      "[ 56/100] train_loss: 0.07687 valid_loss: 0.08687 test_loss: 0.07125 \n",
      "[ 57/100] train_loss: 0.07586 valid_loss: 0.08013 test_loss: 0.07078 \n",
      "Validation loss decreased (0.082884 --> 0.080129).  Saving model ...\n",
      "[ 58/100] train_loss: 0.08189 valid_loss: 0.08444 test_loss: 0.07080 \n",
      "[ 59/100] train_loss: 0.07563 valid_loss: 0.09341 test_loss: 0.07219 \n",
      "[ 60/100] train_loss: 0.07959 valid_loss: 0.08100 test_loss: 0.06934 \n",
      "[ 61/100] train_loss: 0.07773 valid_loss: 0.08135 test_loss: 0.06979 \n",
      "[ 62/100] train_loss: 0.07153 valid_loss: 0.08544 test_loss: 0.06944 \n",
      "[ 63/100] train_loss: 0.07897 valid_loss: 0.09086 test_loss: 0.06996 \n",
      "[ 64/100] train_loss: 0.07194 valid_loss: 0.08250 test_loss: 0.06845 \n",
      "[ 65/100] train_loss: 0.07060 valid_loss: 0.08162 test_loss: 0.06869 \n",
      "[ 66/100] train_loss: 0.06707 valid_loss: 0.08718 test_loss: 0.06834 \n",
      "[ 67/100] train_loss: 0.07159 valid_loss: 0.08688 test_loss: 0.06772 \n",
      "[ 68/100] train_loss: 0.08185 valid_loss: 0.08029 test_loss: 0.06733 \n",
      "[ 69/100] train_loss: 0.07790 valid_loss: 0.08802 test_loss: 0.06821 \n",
      "[ 70/100] train_loss: 0.06753 valid_loss: 0.08631 test_loss: 0.06710 \n",
      "[ 71/100] train_loss: 0.07673 valid_loss: 0.07977 test_loss: 0.06684 \n",
      "Validation loss decreased (0.080129 --> 0.079774).  Saving model ...\n",
      "[ 72/100] train_loss: 0.07152 valid_loss: 0.08577 test_loss: 0.06652 \n",
      "[ 73/100] train_loss: 0.07051 valid_loss: 0.08506 test_loss: 0.06643 \n",
      "[ 74/100] train_loss: 0.08005 valid_loss: 0.08677 test_loss: 0.06629 \n",
      "[ 75/100] train_loss: 0.07327 valid_loss: 0.08053 test_loss: 0.06521 \n",
      "[ 76/100] train_loss: 0.06600 valid_loss: 0.07991 test_loss: 0.06498 \n",
      "[ 77/100] train_loss: 0.07622 valid_loss: 0.08942 test_loss: 0.06597 \n",
      "[ 78/100] train_loss: 0.07856 valid_loss: 0.08852 test_loss: 0.06614 \n",
      "[ 79/100] train_loss: 0.07464 valid_loss: 0.08017 test_loss: 0.06535 \n",
      "[ 80/100] train_loss: 0.07010 valid_loss: 0.08197 test_loss: 0.06513 \n",
      "[ 81/100] train_loss: 0.07599 valid_loss: 0.08442 test_loss: 0.06471 \n",
      "[ 82/100] train_loss: 0.07060 valid_loss: 0.08044 test_loss: 0.06399 \n",
      "[ 83/100] train_loss: 0.07236 valid_loss: 0.08359 test_loss: 0.06436 \n",
      "[ 84/100] train_loss: 0.06670 valid_loss: 0.08721 test_loss: 0.06416 \n",
      "[ 85/100] train_loss: 0.07332 valid_loss: 0.07836 test_loss: 0.06293 \n",
      "Validation loss decreased (0.079774 --> 0.078356).  Saving model ...\n",
      "[ 86/100] train_loss: 0.06831 valid_loss: 0.08299 test_loss: 0.06304 \n",
      "[ 87/100] train_loss: 0.06283 valid_loss: 0.08562 test_loss: 0.06319 \n",
      "[ 88/100] train_loss: 0.07024 valid_loss: 0.08238 test_loss: 0.06274 \n",
      "[ 89/100] train_loss: 0.07004 valid_loss: 0.08126 test_loss: 0.06259 \n",
      "[ 90/100] train_loss: 0.07000 valid_loss: 0.08441 test_loss: 0.06271 \n",
      "[ 91/100] train_loss: 0.06090 valid_loss: 0.09052 test_loss: 0.06377 \n",
      "[ 92/100] train_loss: 0.07043 valid_loss: 0.08445 test_loss: 0.06230 \n",
      "[ 93/100] train_loss: 0.07419 valid_loss: 0.07786 test_loss: 0.06342 \n",
      "Validation loss decreased (0.078356 --> 0.077856).  Saving model ...\n",
      "[ 94/100] train_loss: 0.07001 valid_loss: 0.08413 test_loss: 0.06343 \n",
      "[ 95/100] train_loss: 0.06644 valid_loss: 0.08971 test_loss: 0.06325 \n",
      "[ 96/100] train_loss: 0.06582 valid_loss: 0.08938 test_loss: 0.06265 \n",
      "[ 97/100] train_loss: 0.06771 valid_loss: 0.08316 test_loss: 0.06182 \n",
      "[ 98/100] train_loss: 0.06272 valid_loss: 0.07933 test_loss: 0.06303 \n",
      "[ 99/100] train_loss: 0.07159 valid_loss: 0.09496 test_loss: 0.06451 \n",
      "[100/100] train_loss: 0.06462 valid_loss: 0.08834 test_loss: 0.06184 \n",
      "TRAINING MODEL 5\n",
      "[  1/100] train_loss: 0.18687 valid_loss: 0.18562 test_loss: 0.18788 \n",
      "Validation loss decreased (inf --> 0.185619).  Saving model ...\n",
      "[  2/100] train_loss: 0.18457 valid_loss: 0.18476 test_loss: 0.18649 \n",
      "Validation loss decreased (0.185619 --> 0.184762).  Saving model ...\n",
      "[  3/100] train_loss: 0.18349 valid_loss: 0.18380 test_loss: 0.18434 \n",
      "Validation loss decreased (0.184762 --> 0.183805).  Saving model ...\n",
      "[  4/100] train_loss: 0.18016 valid_loss: 0.18248 test_loss: 0.18142 \n",
      "Validation loss decreased (0.183805 --> 0.182482).  Saving model ...\n",
      "[  5/100] train_loss: 0.17792 valid_loss: 0.18028 test_loss: 0.17777 \n",
      "Validation loss decreased (0.182482 --> 0.180278).  Saving model ...\n",
      "[  6/100] train_loss: 0.17540 valid_loss: 0.17780 test_loss: 0.17351 \n",
      "Validation loss decreased (0.180278 --> 0.177799).  Saving model ...\n",
      "[  7/100] train_loss: 0.17219 valid_loss: 0.17464 test_loss: 0.16958 \n",
      "Validation loss decreased (0.177799 --> 0.174639).  Saving model ...\n",
      "[  8/100] train_loss: 0.16942 valid_loss: 0.17196 test_loss: 0.16605 \n",
      "Validation loss decreased (0.174639 --> 0.171961).  Saving model ...\n",
      "[  9/100] train_loss: 0.16586 valid_loss: 0.16828 test_loss: 0.16187 \n",
      "Validation loss decreased (0.171961 --> 0.168276).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16129 valid_loss: 0.16463 test_loss: 0.15750 \n",
      "Validation loss decreased (0.168276 --> 0.164628).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15749 valid_loss: 0.16015 test_loss: 0.15276 \n",
      "Validation loss decreased (0.164628 --> 0.160150).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15239 valid_loss: 0.15711 test_loss: 0.14912 \n",
      "Validation loss decreased (0.160150 --> 0.157111).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14910 valid_loss: 0.15627 test_loss: 0.14584 \n",
      "Validation loss decreased (0.157111 --> 0.156274).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14377 valid_loss: 0.15148 test_loss: 0.14194 \n",
      "Validation loss decreased (0.156274 --> 0.151481).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14333 valid_loss: 0.14630 test_loss: 0.13741 \n",
      "Validation loss decreased (0.151481 --> 0.146302).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13949 valid_loss: 0.14180 test_loss: 0.13329 \n",
      "Validation loss decreased (0.146302 --> 0.141796).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13420 valid_loss: 0.13731 test_loss: 0.12993 \n",
      "Validation loss decreased (0.141796 --> 0.137315).  Saving model ...\n",
      "[ 18/100] train_loss: 0.12849 valid_loss: 0.14011 test_loss: 0.12658 \n",
      "[ 19/100] train_loss: 0.12653 valid_loss: 0.13315 test_loss: 0.12257 \n",
      "Validation loss decreased (0.137315 --> 0.133152).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12629 valid_loss: 0.12560 test_loss: 0.11855 \n",
      "Validation loss decreased (0.133152 --> 0.125596).  Saving model ...\n",
      "[ 21/100] train_loss: 0.11828 valid_loss: 0.13554 test_loss: 0.11756 \n",
      "[ 22/100] train_loss: 0.11791 valid_loss: 0.12767 test_loss: 0.11317 \n",
      "[ 23/100] train_loss: 0.11209 valid_loss: 0.11757 test_loss: 0.10885 \n",
      "Validation loss decreased (0.125596 --> 0.117570).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11097 valid_loss: 0.11081 test_loss: 0.10634 \n",
      "Validation loss decreased (0.117570 --> 0.110808).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11054 valid_loss: 0.11170 test_loss: 0.10262 \n",
      "[ 26/100] train_loss: 0.10871 valid_loss: 0.11102 test_loss: 0.10066 \n",
      "[ 27/100] train_loss: 0.10068 valid_loss: 0.11075 test_loss: 0.09900 \n",
      "Validation loss decreased (0.110808 --> 0.110749).  Saving model ...\n",
      "[ 28/100] train_loss: 0.09963 valid_loss: 0.10313 test_loss: 0.09602 \n",
      "Validation loss decreased (0.110749 --> 0.103133).  Saving model ...\n",
      "[ 29/100] train_loss: 0.09931 valid_loss: 0.10720 test_loss: 0.09411 \n",
      "[ 30/100] train_loss: 0.09434 valid_loss: 0.10366 test_loss: 0.09198 \n",
      "[ 31/100] train_loss: 0.09150 valid_loss: 0.09807 test_loss: 0.09030 \n",
      "Validation loss decreased (0.103133 --> 0.098067).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09354 valid_loss: 0.09911 test_loss: 0.08829 \n",
      "[ 33/100] train_loss: 0.09384 valid_loss: 0.09747 test_loss: 0.08660 \n",
      "Validation loss decreased (0.098067 --> 0.097471).  Saving model ...\n",
      "[ 34/100] train_loss: 0.08974 valid_loss: 0.09685 test_loss: 0.08516 \n",
      "Validation loss decreased (0.097471 --> 0.096850).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09266 valid_loss: 0.09980 test_loss: 0.08460 \n",
      "[ 36/100] train_loss: 0.08885 valid_loss: 0.09546 test_loss: 0.08290 \n",
      "Validation loss decreased (0.096850 --> 0.095462).  Saving model ...\n",
      "[ 37/100] train_loss: 0.08992 valid_loss: 0.09342 test_loss: 0.08175 \n",
      "Validation loss decreased (0.095462 --> 0.093418).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08922 valid_loss: 0.09171 test_loss: 0.08080 \n",
      "Validation loss decreased (0.093418 --> 0.091711).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08648 valid_loss: 0.09287 test_loss: 0.07987 \n",
      "[ 40/100] train_loss: 0.09272 valid_loss: 0.08754 test_loss: 0.07924 \n",
      "Validation loss decreased (0.091711 --> 0.087537).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08800 valid_loss: 0.08917 test_loss: 0.07791 \n",
      "[ 42/100] train_loss: 0.08859 valid_loss: 0.09612 test_loss: 0.07856 \n",
      "[ 43/100] train_loss: 0.08511 valid_loss: 0.08754 test_loss: 0.07576 \n",
      "[ 44/100] train_loss: 0.08233 valid_loss: 0.08494 test_loss: 0.07563 \n",
      "Validation loss decreased (0.087537 --> 0.084937).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08164 valid_loss: 0.08830 test_loss: 0.07469 \n",
      "[ 46/100] train_loss: 0.08266 valid_loss: 0.09028 test_loss: 0.07458 \n",
      "[ 47/100] train_loss: 0.08567 valid_loss: 0.08663 test_loss: 0.07396 \n",
      "[ 48/100] train_loss: 0.08163 valid_loss: 0.08595 test_loss: 0.07405 \n",
      "[ 49/100] train_loss: 0.08187 valid_loss: 0.09114 test_loss: 0.07432 \n",
      "[ 50/100] train_loss: 0.08261 valid_loss: 0.08580 test_loss: 0.07247 \n",
      "[ 51/100] train_loss: 0.07973 valid_loss: 0.09080 test_loss: 0.07295 \n",
      "[ 52/100] train_loss: 0.08214 valid_loss: 0.08529 test_loss: 0.07183 \n",
      "[ 53/100] train_loss: 0.07275 valid_loss: 0.08349 test_loss: 0.07151 \n",
      "Validation loss decreased (0.084937 --> 0.083494).  Saving model ...\n",
      "[ 54/100] train_loss: 0.07514 valid_loss: 0.08785 test_loss: 0.07157 \n",
      "[ 55/100] train_loss: 0.07873 valid_loss: 0.08756 test_loss: 0.07122 \n",
      "[ 56/100] train_loss: 0.08059 valid_loss: 0.08536 test_loss: 0.07079 \n",
      "[ 57/100] train_loss: 0.08234 valid_loss: 0.09079 test_loss: 0.07157 \n",
      "[ 58/100] train_loss: 0.08059 valid_loss: 0.08266 test_loss: 0.06945 \n",
      "Validation loss decreased (0.083494 --> 0.082664).  Saving model ...\n",
      "[ 59/100] train_loss: 0.07772 valid_loss: 0.08313 test_loss: 0.06906 \n",
      "[ 60/100] train_loss: 0.08156 valid_loss: 0.08582 test_loss: 0.06934 \n",
      "[ 61/100] train_loss: 0.07361 valid_loss: 0.08141 test_loss: 0.06908 \n",
      "Validation loss decreased (0.082664 --> 0.081410).  Saving model ...\n",
      "[ 62/100] train_loss: 0.06908 valid_loss: 0.08691 test_loss: 0.06841 \n",
      "[ 63/100] train_loss: 0.08695 valid_loss: 0.08339 test_loss: 0.06784 \n",
      "[ 64/100] train_loss: 0.07092 valid_loss: 0.08469 test_loss: 0.06800 \n",
      "[ 65/100] train_loss: 0.07281 valid_loss: 0.08662 test_loss: 0.06795 \n",
      "[ 66/100] train_loss: 0.07095 valid_loss: 0.08986 test_loss: 0.06835 \n",
      "[ 67/100] train_loss: 0.07543 valid_loss: 0.07868 test_loss: 0.06702 \n",
      "Validation loss decreased (0.081410 --> 0.078682).  Saving model ...\n",
      "[ 68/100] train_loss: 0.07049 valid_loss: 0.08086 test_loss: 0.06678 \n",
      "[ 69/100] train_loss: 0.07792 valid_loss: 0.09058 test_loss: 0.06806 \n",
      "[ 70/100] train_loss: 0.08300 valid_loss: 0.08527 test_loss: 0.06705 \n",
      "[ 71/100] train_loss: 0.07888 valid_loss: 0.07937 test_loss: 0.06750 \n",
      "[ 72/100] train_loss: 0.06593 valid_loss: 0.08538 test_loss: 0.06642 \n",
      "[ 73/100] train_loss: 0.07001 valid_loss: 0.09262 test_loss: 0.06809 \n",
      "[ 74/100] train_loss: 0.07694 valid_loss: 0.07880 test_loss: 0.06551 \n",
      "[ 75/100] train_loss: 0.06810 valid_loss: 0.08892 test_loss: 0.06716 \n",
      "[ 76/100] train_loss: 0.07915 valid_loss: 0.08305 test_loss: 0.06563 \n",
      "[ 77/100] train_loss: 0.06763 valid_loss: 0.07994 test_loss: 0.06642 \n",
      "[ 78/100] train_loss: 0.07612 valid_loss: 0.08741 test_loss: 0.06646 \n",
      "[ 79/100] train_loss: 0.07802 valid_loss: 0.08598 test_loss: 0.06549 \n",
      "[ 80/100] train_loss: 0.07696 valid_loss: 0.07960 test_loss: 0.06447 \n",
      "[ 81/100] train_loss: 0.07230 valid_loss: 0.09260 test_loss: 0.06737 \n",
      "[ 82/100] train_loss: 0.07197 valid_loss: 0.08092 test_loss: 0.06407 \n",
      "[ 83/100] train_loss: 0.06976 valid_loss: 0.07714 test_loss: 0.06475 \n",
      "Validation loss decreased (0.078682 --> 0.077145).  Saving model ...\n",
      "[ 84/100] train_loss: 0.06226 valid_loss: 0.08079 test_loss: 0.06438 \n",
      "[ 85/100] train_loss: 0.07325 valid_loss: 0.08612 test_loss: 0.06482 \n",
      "[ 86/100] train_loss: 0.06578 valid_loss: 0.08560 test_loss: 0.06419 \n",
      "[ 87/100] train_loss: 0.07179 valid_loss: 0.07963 test_loss: 0.06402 \n",
      "[ 88/100] train_loss: 0.06983 valid_loss: 0.09506 test_loss: 0.06668 \n",
      "[ 89/100] train_loss: 0.07657 valid_loss: 0.08476 test_loss: 0.06351 \n",
      "[ 90/100] train_loss: 0.07096 valid_loss: 0.07506 test_loss: 0.06389 \n",
      "Validation loss decreased (0.077145 --> 0.075059).  Saving model ...\n",
      "[ 91/100] train_loss: 0.07228 valid_loss: 0.07863 test_loss: 0.06305 \n",
      "[ 92/100] train_loss: 0.07336 valid_loss: 0.08889 test_loss: 0.06438 \n",
      "[ 93/100] train_loss: 0.06944 valid_loss: 0.07708 test_loss: 0.06262 \n",
      "[ 94/100] train_loss: 0.06405 valid_loss: 0.07952 test_loss: 0.06219 \n",
      "[ 95/100] train_loss: 0.06769 valid_loss: 0.08455 test_loss: 0.06291 \n",
      "[ 96/100] train_loss: 0.07227 valid_loss: 0.08698 test_loss: 0.06325 \n",
      "[ 97/100] train_loss: 0.06756 valid_loss: 0.07736 test_loss: 0.06230 \n",
      "[ 98/100] train_loss: 0.06190 valid_loss: 0.08303 test_loss: 0.06231 \n",
      "[ 99/100] train_loss: 0.06547 valid_loss: 0.08474 test_loss: 0.06271 \n",
      "[100/100] train_loss: 0.06214 valid_loss: 0.08054 test_loss: 0.06184 \n",
      "TRAINING MODEL 6\n",
      "[  1/100] train_loss: 0.18504 valid_loss: 0.19486 test_loss: 0.18694 \n",
      "Validation loss decreased (inf --> 0.194860).  Saving model ...\n",
      "[  2/100] train_loss: 0.18447 valid_loss: 0.19431 test_loss: 0.18567 \n",
      "Validation loss decreased (0.194860 --> 0.194314).  Saving model ...\n",
      "[  3/100] train_loss: 0.18289 valid_loss: 0.19302 test_loss: 0.18400 \n",
      "Validation loss decreased (0.194314 --> 0.193024).  Saving model ...\n",
      "[  4/100] train_loss: 0.18103 valid_loss: 0.19165 test_loss: 0.18190 \n",
      "Validation loss decreased (0.193024 --> 0.191654).  Saving model ...\n",
      "[  5/100] train_loss: 0.17926 valid_loss: 0.18982 test_loss: 0.17959 \n",
      "Validation loss decreased (0.191654 --> 0.189823).  Saving model ...\n",
      "[  6/100] train_loss: 0.17734 valid_loss: 0.18806 test_loss: 0.17706 \n",
      "Validation loss decreased (0.189823 --> 0.188058).  Saving model ...\n",
      "[  7/100] train_loss: 0.17667 valid_loss: 0.18577 test_loss: 0.17424 \n",
      "Validation loss decreased (0.188058 --> 0.185770).  Saving model ...\n",
      "[  8/100] train_loss: 0.17268 valid_loss: 0.18385 test_loss: 0.17145 \n",
      "Validation loss decreased (0.185770 --> 0.183847).  Saving model ...\n",
      "[  9/100] train_loss: 0.16880 valid_loss: 0.18249 test_loss: 0.16859 \n",
      "Validation loss decreased (0.183847 --> 0.182488).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16746 valid_loss: 0.17923 test_loss: 0.16536 \n",
      "Validation loss decreased (0.182488 --> 0.179234).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16402 valid_loss: 0.17624 test_loss: 0.16159 \n",
      "Validation loss decreased (0.179234 --> 0.176237).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16122 valid_loss: 0.17512 test_loss: 0.15912 \n",
      "Validation loss decreased (0.176237 --> 0.175121).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15633 valid_loss: 0.17168 test_loss: 0.15641 \n",
      "Validation loss decreased (0.175121 --> 0.171678).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15193 valid_loss: 0.17521 test_loss: 0.15670 \n",
      "[ 15/100] train_loss: 0.14762 valid_loss: 0.17126 test_loss: 0.15251 \n",
      "Validation loss decreased (0.171678 --> 0.171260).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14368 valid_loss: 0.16530 test_loss: 0.14867 \n",
      "Validation loss decreased (0.171260 --> 0.165295).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14274 valid_loss: 0.15570 test_loss: 0.14042 \n",
      "Validation loss decreased (0.165295 --> 0.155703).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13719 valid_loss: 0.15656 test_loss: 0.13779 \n",
      "[ 19/100] train_loss: 0.13392 valid_loss: 0.15207 test_loss: 0.13562 \n",
      "Validation loss decreased (0.155703 --> 0.152073).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13065 valid_loss: 0.14882 test_loss: 0.13124 \n",
      "Validation loss decreased (0.152073 --> 0.148816).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12852 valid_loss: 0.14556 test_loss: 0.12789 \n",
      "Validation loss decreased (0.148816 --> 0.145560).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12283 valid_loss: 0.13883 test_loss: 0.12376 \n",
      "Validation loss decreased (0.145560 --> 0.138832).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12128 valid_loss: 0.13477 test_loss: 0.11979 \n",
      "Validation loss decreased (0.138832 --> 0.134770).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11706 valid_loss: 0.13701 test_loss: 0.11908 \n",
      "[ 25/100] train_loss: 0.11686 valid_loss: 0.13586 test_loss: 0.11559 \n",
      "[ 26/100] train_loss: 0.11305 valid_loss: 0.12026 test_loss: 0.10828 \n",
      "Validation loss decreased (0.134770 --> 0.120256).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10519 valid_loss: 0.12092 test_loss: 0.10588 \n",
      "[ 28/100] train_loss: 0.10786 valid_loss: 0.11639 test_loss: 0.10332 \n",
      "Validation loss decreased (0.120256 --> 0.116394).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10397 valid_loss: 0.11221 test_loss: 0.10163 \n",
      "Validation loss decreased (0.116394 --> 0.112207).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10337 valid_loss: 0.12531 test_loss: 0.10271 \n",
      "[ 31/100] train_loss: 0.09961 valid_loss: 0.11999 test_loss: 0.09877 \n",
      "[ 32/100] train_loss: 0.09871 valid_loss: 0.10140 test_loss: 0.09512 \n",
      "Validation loss decreased (0.112207 --> 0.101398).  Saving model ...\n",
      "[ 33/100] train_loss: 0.08969 valid_loss: 0.10229 test_loss: 0.09301 \n",
      "[ 34/100] train_loss: 0.09611 valid_loss: 0.11518 test_loss: 0.09281 \n",
      "[ 35/100] train_loss: 0.09653 valid_loss: 0.10070 test_loss: 0.08742 \n",
      "Validation loss decreased (0.101398 --> 0.100704).  Saving model ...\n",
      "[ 36/100] train_loss: 0.08825 valid_loss: 0.09977 test_loss: 0.08613 \n",
      "Validation loss decreased (0.100704 --> 0.099766).  Saving model ...\n",
      "[ 37/100] train_loss: 0.08769 valid_loss: 0.10248 test_loss: 0.08547 \n",
      "[ 38/100] train_loss: 0.08995 valid_loss: 0.09866 test_loss: 0.08367 \n",
      "Validation loss decreased (0.099766 --> 0.098659).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08965 valid_loss: 0.09621 test_loss: 0.08264 \n",
      "Validation loss decreased (0.098659 --> 0.096206).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08931 valid_loss: 0.09660 test_loss: 0.08133 \n",
      "[ 41/100] train_loss: 0.08882 valid_loss: 0.09884 test_loss: 0.08085 \n",
      "[ 42/100] train_loss: 0.08843 valid_loss: 0.09557 test_loss: 0.07979 \n",
      "Validation loss decreased (0.096206 --> 0.095566).  Saving model ...\n",
      "[ 43/100] train_loss: 0.08224 valid_loss: 0.09433 test_loss: 0.07848 \n",
      "Validation loss decreased (0.095566 --> 0.094330).  Saving model ...\n",
      "[ 44/100] train_loss: 0.08854 valid_loss: 0.09507 test_loss: 0.07812 \n",
      "[ 45/100] train_loss: 0.08056 valid_loss: 0.10398 test_loss: 0.07966 \n",
      "[ 46/100] train_loss: 0.08560 valid_loss: 0.09545 test_loss: 0.07607 \n",
      "[ 47/100] train_loss: 0.08360 valid_loss: 0.09271 test_loss: 0.07524 \n",
      "Validation loss decreased (0.094330 --> 0.092709).  Saving model ...\n",
      "[ 48/100] train_loss: 0.08258 valid_loss: 0.09567 test_loss: 0.07560 \n",
      "[ 49/100] train_loss: 0.07831 valid_loss: 0.09761 test_loss: 0.07558 \n",
      "[ 50/100] train_loss: 0.07983 valid_loss: 0.09093 test_loss: 0.07328 \n",
      "Validation loss decreased (0.092709 --> 0.090932).  Saving model ...\n",
      "[ 51/100] train_loss: 0.08966 valid_loss: 0.08781 test_loss: 0.07287 \n",
      "Validation loss decreased (0.090932 --> 0.087810).  Saving model ...\n",
      "[ 52/100] train_loss: 0.08046 valid_loss: 0.09466 test_loss: 0.07386 \n",
      "[ 53/100] train_loss: 0.08414 valid_loss: 0.08971 test_loss: 0.07230 \n",
      "[ 54/100] train_loss: 0.07476 valid_loss: 0.08620 test_loss: 0.07059 \n",
      "Validation loss decreased (0.087810 --> 0.086199).  Saving model ...\n",
      "[ 55/100] train_loss: 0.07561 valid_loss: 0.08741 test_loss: 0.07044 \n",
      "[ 56/100] train_loss: 0.07571 valid_loss: 0.09976 test_loss: 0.07369 \n",
      "[ 57/100] train_loss: 0.08488 valid_loss: 0.09168 test_loss: 0.07058 \n",
      "[ 58/100] train_loss: 0.07425 valid_loss: 0.08260 test_loss: 0.07276 \n",
      "Validation loss decreased (0.086199 --> 0.082603).  Saving model ...\n",
      "[ 59/100] train_loss: 0.07946 valid_loss: 0.09153 test_loss: 0.07049 \n",
      "[ 60/100] train_loss: 0.07849 valid_loss: 0.09958 test_loss: 0.07285 \n",
      "[ 61/100] train_loss: 0.07534 valid_loss: 0.09712 test_loss: 0.07134 \n",
      "[ 62/100] train_loss: 0.08172 valid_loss: 0.08660 test_loss: 0.06820 \n",
      "[ 63/100] train_loss: 0.07030 valid_loss: 0.08563 test_loss: 0.06824 \n",
      "[ 64/100] train_loss: 0.07289 valid_loss: 0.09337 test_loss: 0.06905 \n",
      "[ 65/100] train_loss: 0.07951 valid_loss: 0.08764 test_loss: 0.06799 \n",
      "[ 66/100] train_loss: 0.07714 valid_loss: 0.08920 test_loss: 0.06865 \n",
      "[ 67/100] train_loss: 0.08027 valid_loss: 0.08853 test_loss: 0.06766 \n",
      "[ 68/100] train_loss: 0.07458 valid_loss: 0.08962 test_loss: 0.06751 \n",
      "[ 69/100] train_loss: 0.07544 valid_loss: 0.09287 test_loss: 0.06820 \n",
      "[ 70/100] train_loss: 0.07478 valid_loss: 0.08443 test_loss: 0.06625 \n",
      "[ 71/100] train_loss: 0.07607 valid_loss: 0.09294 test_loss: 0.06820 \n",
      "[ 72/100] train_loss: 0.07484 valid_loss: 0.08475 test_loss: 0.06614 \n",
      "[ 73/100] train_loss: 0.07014 valid_loss: 0.08258 test_loss: 0.06559 \n",
      "Validation loss decreased (0.082603 --> 0.082583).  Saving model ...\n",
      "[ 74/100] train_loss: 0.06732 valid_loss: 0.09448 test_loss: 0.06784 \n",
      "[ 75/100] train_loss: 0.07464 valid_loss: 0.08652 test_loss: 0.06538 \n",
      "[ 76/100] train_loss: 0.07236 valid_loss: 0.08631 test_loss: 0.06565 \n",
      "[ 77/100] train_loss: 0.07273 valid_loss: 0.08738 test_loss: 0.06565 \n",
      "[ 78/100] train_loss: 0.07583 valid_loss: 0.09009 test_loss: 0.06661 \n",
      "[ 79/100] train_loss: 0.06873 valid_loss: 0.08282 test_loss: 0.06417 \n",
      "[ 80/100] train_loss: 0.07131 valid_loss: 0.09167 test_loss: 0.06535 \n",
      "[ 81/100] train_loss: 0.06526 valid_loss: 0.08628 test_loss: 0.06406 \n",
      "[ 82/100] train_loss: 0.06830 valid_loss: 0.08826 test_loss: 0.06431 \n",
      "[ 83/100] train_loss: 0.07417 valid_loss: 0.08955 test_loss: 0.06463 \n",
      "[ 84/100] train_loss: 0.06556 valid_loss: 0.08792 test_loss: 0.06397 \n",
      "[ 85/100] train_loss: 0.07324 valid_loss: 0.08254 test_loss: 0.06404 \n",
      "Validation loss decreased (0.082583 --> 0.082541).  Saving model ...\n",
      "[ 86/100] train_loss: 0.07054 valid_loss: 0.09147 test_loss: 0.06485 \n",
      "[ 87/100] train_loss: 0.07016 valid_loss: 0.08589 test_loss: 0.06280 \n",
      "[ 88/100] train_loss: 0.06801 valid_loss: 0.08806 test_loss: 0.06328 \n",
      "[ 89/100] train_loss: 0.07073 valid_loss: 0.09654 test_loss: 0.06602 \n",
      "[ 90/100] train_loss: 0.07058 valid_loss: 0.08717 test_loss: 0.06419 \n",
      "[ 91/100] train_loss: 0.07275 valid_loss: 0.09660 test_loss: 0.06683 \n",
      "[ 92/100] train_loss: 0.06539 valid_loss: 0.09500 test_loss: 0.06521 \n",
      "[ 93/100] train_loss: 0.06927 valid_loss: 0.08850 test_loss: 0.06285 \n",
      "[ 94/100] train_loss: 0.07166 valid_loss: 0.08371 test_loss: 0.06203 \n",
      "[ 95/100] train_loss: 0.07199 valid_loss: 0.08983 test_loss: 0.06384 \n",
      "[ 96/100] train_loss: 0.06928 valid_loss: 0.09444 test_loss: 0.06481 \n",
      "[ 97/100] train_loss: 0.06883 valid_loss: 0.09265 test_loss: 0.06389 \n",
      "[ 98/100] train_loss: 0.07129 valid_loss: 0.09321 test_loss: 0.06389 \n",
      "[ 99/100] train_loss: 0.06348 valid_loss: 0.07971 test_loss: 0.06107 \n",
      "Validation loss decreased (0.082541 --> 0.079705).  Saving model ...\n",
      "[100/100] train_loss: 0.07263 valid_loss: 0.08726 test_loss: 0.06224 \n",
      "TRAINING MODEL 7\n",
      "[  1/100] train_loss: 0.18864 valid_loss: 0.19733 test_loss: 0.18673 \n",
      "Validation loss decreased (inf --> 0.197329).  Saving model ...\n",
      "[  2/100] train_loss: 0.18463 valid_loss: 0.19595 test_loss: 0.18565 \n",
      "Validation loss decreased (0.197329 --> 0.195947).  Saving model ...\n",
      "[  3/100] train_loss: 0.18144 valid_loss: 0.19429 test_loss: 0.18403 \n",
      "Validation loss decreased (0.195947 --> 0.194290).  Saving model ...\n",
      "[  4/100] train_loss: 0.17964 valid_loss: 0.19237 test_loss: 0.18180 \n",
      "Validation loss decreased (0.194290 --> 0.192372).  Saving model ...\n",
      "[  5/100] train_loss: 0.17880 valid_loss: 0.19000 test_loss: 0.17891 \n",
      "Validation loss decreased (0.192372 --> 0.190002).  Saving model ...\n",
      "[  6/100] train_loss: 0.17655 valid_loss: 0.18731 test_loss: 0.17563 \n",
      "Validation loss decreased (0.190002 --> 0.187311).  Saving model ...\n",
      "[  7/100] train_loss: 0.17418 valid_loss: 0.18494 test_loss: 0.17241 \n",
      "Validation loss decreased (0.187311 --> 0.184942).  Saving model ...\n",
      "[  8/100] train_loss: 0.17232 valid_loss: 0.18225 test_loss: 0.16932 \n",
      "Validation loss decreased (0.184942 --> 0.182247).  Saving model ...\n",
      "[  9/100] train_loss: 0.16969 valid_loss: 0.17884 test_loss: 0.16622 \n",
      "Validation loss decreased (0.182247 --> 0.178843).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16512 valid_loss: 0.17550 test_loss: 0.16338 \n",
      "Validation loss decreased (0.178843 --> 0.175500).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16338 valid_loss: 0.17328 test_loss: 0.15994 \n",
      "Validation loss decreased (0.175500 --> 0.173284).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15728 valid_loss: 0.17120 test_loss: 0.15692 \n",
      "Validation loss decreased (0.173284 --> 0.171203).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15613 valid_loss: 0.16670 test_loss: 0.15345 \n",
      "Validation loss decreased (0.171203 --> 0.166704).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15408 valid_loss: 0.16391 test_loss: 0.14990 \n",
      "Validation loss decreased (0.166704 --> 0.163913).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14932 valid_loss: 0.15877 test_loss: 0.14681 \n",
      "Validation loss decreased (0.163913 --> 0.158771).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14345 valid_loss: 0.15835 test_loss: 0.14331 \n",
      "Validation loss decreased (0.158771 --> 0.158346).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14028 valid_loss: 0.15284 test_loss: 0.13974 \n",
      "Validation loss decreased (0.158346 --> 0.152835).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13936 valid_loss: 0.14772 test_loss: 0.13563 \n",
      "Validation loss decreased (0.152835 --> 0.147718).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13359 valid_loss: 0.14378 test_loss: 0.13208 \n",
      "Validation loss decreased (0.147718 --> 0.143781).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13173 valid_loss: 0.14452 test_loss: 0.12860 \n",
      "[ 21/100] train_loss: 0.12722 valid_loss: 0.14062 test_loss: 0.12553 \n",
      "Validation loss decreased (0.143781 --> 0.140624).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12590 valid_loss: 0.13200 test_loss: 0.12158 \n",
      "Validation loss decreased (0.140624 --> 0.132000).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12209 valid_loss: 0.13178 test_loss: 0.11826 \n",
      "Validation loss decreased (0.132000 --> 0.131782).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11773 valid_loss: 0.12897 test_loss: 0.11605 \n",
      "Validation loss decreased (0.131782 --> 0.128967).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11665 valid_loss: 0.11984 test_loss: 0.11232 \n",
      "Validation loss decreased (0.128967 --> 0.119835).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10940 valid_loss: 0.12518 test_loss: 0.10992 \n",
      "[ 27/100] train_loss: 0.10674 valid_loss: 0.12308 test_loss: 0.10756 \n",
      "[ 28/100] train_loss: 0.10424 valid_loss: 0.11400 test_loss: 0.10404 \n",
      "Validation loss decreased (0.119835 --> 0.113996).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10667 valid_loss: 0.11223 test_loss: 0.10120 \n",
      "Validation loss decreased (0.113996 --> 0.112229).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10199 valid_loss: 0.11147 test_loss: 0.09875 \n",
      "Validation loss decreased (0.112229 --> 0.111470).  Saving model ...\n",
      "[ 31/100] train_loss: 0.10345 valid_loss: 0.10736 test_loss: 0.09590 \n",
      "Validation loss decreased (0.111470 --> 0.107358).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09547 valid_loss: 0.10055 test_loss: 0.09425 \n",
      "Validation loss decreased (0.107358 --> 0.100552).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09268 valid_loss: 0.10349 test_loss: 0.09144 \n",
      "[ 34/100] train_loss: 0.09790 valid_loss: 0.10020 test_loss: 0.08947 \n",
      "Validation loss decreased (0.100552 --> 0.100204).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09041 valid_loss: 0.09782 test_loss: 0.08775 \n",
      "Validation loss decreased (0.100204 --> 0.097825).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09505 valid_loss: 0.10233 test_loss: 0.08769 \n",
      "[ 37/100] train_loss: 0.08797 valid_loss: 0.09599 test_loss: 0.08528 \n",
      "Validation loss decreased (0.097825 --> 0.095986).  Saving model ...\n",
      "[ 38/100] train_loss: 0.09126 valid_loss: 0.09240 test_loss: 0.08398 \n",
      "Validation loss decreased (0.095986 --> 0.092399).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08753 valid_loss: 0.09964 test_loss: 0.08382 \n",
      "[ 40/100] train_loss: 0.08528 valid_loss: 0.09711 test_loss: 0.08207 \n",
      "[ 41/100] train_loss: 0.08370 valid_loss: 0.08944 test_loss: 0.08016 \n",
      "Validation loss decreased (0.092399 --> 0.089443).  Saving model ...\n",
      "[ 42/100] train_loss: 0.09011 valid_loss: 0.08959 test_loss: 0.08024 \n",
      "[ 43/100] train_loss: 0.08483 valid_loss: 0.09702 test_loss: 0.07949 \n",
      "[ 44/100] train_loss: 0.08732 valid_loss: 0.09519 test_loss: 0.07812 \n",
      "[ 45/100] train_loss: 0.08966 valid_loss: 0.08627 test_loss: 0.07632 \n",
      "Validation loss decreased (0.089443 --> 0.086273).  Saving model ...\n",
      "[ 46/100] train_loss: 0.07846 valid_loss: 0.08891 test_loss: 0.07546 \n",
      "[ 47/100] train_loss: 0.08210 valid_loss: 0.09152 test_loss: 0.07509 \n",
      "[ 48/100] train_loss: 0.08332 valid_loss: 0.08333 test_loss: 0.07517 \n",
      "Validation loss decreased (0.086273 --> 0.083326).  Saving model ...\n",
      "[ 49/100] train_loss: 0.08799 valid_loss: 0.09249 test_loss: 0.07583 \n",
      "[ 50/100] train_loss: 0.08400 valid_loss: 0.09086 test_loss: 0.07547 \n",
      "[ 51/100] train_loss: 0.07986 valid_loss: 0.08969 test_loss: 0.07417 \n",
      "[ 52/100] train_loss: 0.08070 valid_loss: 0.08757 test_loss: 0.07312 \n",
      "[ 53/100] train_loss: 0.07943 valid_loss: 0.08592 test_loss: 0.07241 \n",
      "[ 54/100] train_loss: 0.07526 valid_loss: 0.08890 test_loss: 0.07180 \n",
      "[ 55/100] train_loss: 0.07766 valid_loss: 0.08751 test_loss: 0.07120 \n",
      "[ 56/100] train_loss: 0.07589 valid_loss: 0.08469 test_loss: 0.07102 \n",
      "[ 57/100] train_loss: 0.07635 valid_loss: 0.09029 test_loss: 0.07151 \n",
      "[ 58/100] train_loss: 0.06889 valid_loss: 0.08084 test_loss: 0.06959 \n",
      "Validation loss decreased (0.083326 --> 0.080840).  Saving model ...\n",
      "[ 59/100] train_loss: 0.07914 valid_loss: 0.08568 test_loss: 0.06943 \n",
      "[ 60/100] train_loss: 0.07697 valid_loss: 0.09432 test_loss: 0.07242 \n",
      "[ 61/100] train_loss: 0.07182 valid_loss: 0.08622 test_loss: 0.06905 \n",
      "[ 62/100] train_loss: 0.07257 valid_loss: 0.08673 test_loss: 0.06806 \n",
      "[ 63/100] train_loss: 0.07082 valid_loss: 0.08556 test_loss: 0.06778 \n",
      "[ 64/100] train_loss: 0.08056 valid_loss: 0.09435 test_loss: 0.06991 \n",
      "[ 65/100] train_loss: 0.07169 valid_loss: 0.08567 test_loss: 0.06756 \n",
      "[ 66/100] train_loss: 0.07419 valid_loss: 0.08123 test_loss: 0.06772 \n",
      "[ 67/100] train_loss: 0.07398 valid_loss: 0.10263 test_loss: 0.07295 \n",
      "[ 68/100] train_loss: 0.07669 valid_loss: 0.08370 test_loss: 0.06531 \n",
      "[ 69/100] train_loss: 0.07196 valid_loss: 0.08009 test_loss: 0.06593 \n",
      "Validation loss decreased (0.080840 --> 0.080093).  Saving model ...\n",
      "[ 70/100] train_loss: 0.07258 valid_loss: 0.09216 test_loss: 0.06793 \n",
      "[ 71/100] train_loss: 0.07654 valid_loss: 0.09258 test_loss: 0.06842 \n",
      "[ 72/100] train_loss: 0.07408 valid_loss: 0.08316 test_loss: 0.06718 \n",
      "[ 73/100] train_loss: 0.07944 valid_loss: 0.09538 test_loss: 0.06861 \n",
      "[ 74/100] train_loss: 0.07949 valid_loss: 0.08833 test_loss: 0.06609 \n",
      "[ 75/100] train_loss: 0.07370 valid_loss: 0.08532 test_loss: 0.06519 \n",
      "[ 76/100] train_loss: 0.07112 valid_loss: 0.08262 test_loss: 0.06537 \n",
      "[ 77/100] train_loss: 0.06582 valid_loss: 0.08782 test_loss: 0.06633 \n",
      "[ 78/100] train_loss: 0.06642 valid_loss: 0.08919 test_loss: 0.06531 \n",
      "[ 79/100] train_loss: 0.07434 valid_loss: 0.08755 test_loss: 0.06437 \n",
      "[ 80/100] train_loss: 0.06728 valid_loss: 0.08295 test_loss: 0.06402 \n",
      "[ 81/100] train_loss: 0.07140 valid_loss: 0.09089 test_loss: 0.06571 \n",
      "[ 82/100] train_loss: 0.06988 valid_loss: 0.08803 test_loss: 0.06440 \n",
      "[ 83/100] train_loss: 0.06851 valid_loss: 0.08186 test_loss: 0.06327 \n",
      "[ 84/100] train_loss: 0.07062 valid_loss: 0.08391 test_loss: 0.06330 \n",
      "[ 85/100] train_loss: 0.07184 valid_loss: 0.09888 test_loss: 0.06787 \n",
      "[ 86/100] train_loss: 0.07529 valid_loss: 0.08884 test_loss: 0.06418 \n",
      "[ 87/100] train_loss: 0.06982 valid_loss: 0.08158 test_loss: 0.06300 \n",
      "[ 88/100] train_loss: 0.07040 valid_loss: 0.08564 test_loss: 0.06325 \n",
      "[ 89/100] train_loss: 0.07685 valid_loss: 0.09109 test_loss: 0.06507 \n",
      "[ 90/100] train_loss: 0.06563 valid_loss: 0.09811 test_loss: 0.06722 \n",
      "[ 91/100] train_loss: 0.07066 valid_loss: 0.08440 test_loss: 0.06249 \n",
      "[ 92/100] train_loss: 0.06873 valid_loss: 0.08322 test_loss: 0.06220 \n",
      "[ 93/100] train_loss: 0.06703 valid_loss: 0.09427 test_loss: 0.06497 \n",
      "[ 94/100] train_loss: 0.06565 valid_loss: 0.08403 test_loss: 0.06224 \n",
      "[ 95/100] train_loss: 0.06290 valid_loss: 0.08929 test_loss: 0.06365 \n",
      "[ 96/100] train_loss: 0.06706 valid_loss: 0.09661 test_loss: 0.06569 \n",
      "[ 97/100] train_loss: 0.07070 valid_loss: 0.07895 test_loss: 0.06099 \n",
      "Validation loss decreased (0.080093 --> 0.078952).  Saving model ...\n",
      "[ 98/100] train_loss: 0.06673 valid_loss: 0.08805 test_loss: 0.06199 \n",
      "[ 99/100] train_loss: 0.06120 valid_loss: 0.08842 test_loss: 0.06194 \n",
      "[100/100] train_loss: 0.05902 valid_loss: 0.08570 test_loss: 0.06088 \n",
      "TRAINING MODEL 8\n",
      "[  1/100] train_loss: 0.18640 valid_loss: 0.19143 test_loss: 0.18675 \n",
      "Validation loss decreased (inf --> 0.191430).  Saving model ...\n",
      "[  2/100] train_loss: 0.18481 valid_loss: 0.19059 test_loss: 0.18570 \n",
      "Validation loss decreased (0.191430 --> 0.190593).  Saving model ...\n",
      "[  3/100] train_loss: 0.18237 valid_loss: 0.18954 test_loss: 0.18430 \n",
      "Validation loss decreased (0.190593 --> 0.189540).  Saving model ...\n",
      "[  4/100] train_loss: 0.18074 valid_loss: 0.18806 test_loss: 0.18248 \n",
      "Validation loss decreased (0.189540 --> 0.188062).  Saving model ...\n",
      "[  5/100] train_loss: 0.17841 valid_loss: 0.18650 test_loss: 0.18021 \n",
      "Validation loss decreased (0.188062 --> 0.186505).  Saving model ...\n",
      "[  6/100] train_loss: 0.17784 valid_loss: 0.18465 test_loss: 0.17755 \n",
      "Validation loss decreased (0.186505 --> 0.184653).  Saving model ...\n",
      "[  7/100] train_loss: 0.17542 valid_loss: 0.18196 test_loss: 0.17451 \n",
      "Validation loss decreased (0.184653 --> 0.181960).  Saving model ...\n",
      "[  8/100] train_loss: 0.17184 valid_loss: 0.17937 test_loss: 0.17128 \n",
      "Validation loss decreased (0.181960 --> 0.179367).  Saving model ...\n",
      "[  9/100] train_loss: 0.17025 valid_loss: 0.17646 test_loss: 0.16820 \n",
      "Validation loss decreased (0.179367 --> 0.176463).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16786 valid_loss: 0.17351 test_loss: 0.16531 \n",
      "Validation loss decreased (0.176463 --> 0.173505).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16396 valid_loss: 0.17167 test_loss: 0.16191 \n",
      "Validation loss decreased (0.173505 --> 0.171675).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16237 valid_loss: 0.16862 test_loss: 0.15836 \n",
      "Validation loss decreased (0.171675 --> 0.168619).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15727 valid_loss: 0.16463 test_loss: 0.15450 \n",
      "Validation loss decreased (0.168619 --> 0.164633).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15534 valid_loss: 0.16133 test_loss: 0.15090 \n",
      "Validation loss decreased (0.164633 --> 0.161327).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14989 valid_loss: 0.15707 test_loss: 0.14733 \n",
      "Validation loss decreased (0.161327 --> 0.157074).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14477 valid_loss: 0.15491 test_loss: 0.14408 \n",
      "Validation loss decreased (0.157074 --> 0.154911).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14367 valid_loss: 0.15083 test_loss: 0.14083 \n",
      "Validation loss decreased (0.154911 --> 0.150828).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13688 valid_loss: 0.14870 test_loss: 0.13739 \n",
      "Validation loss decreased (0.150828 --> 0.148703).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13530 valid_loss: 0.14214 test_loss: 0.13357 \n",
      "Validation loss decreased (0.148703 --> 0.142145).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13284 valid_loss: 0.14029 test_loss: 0.13019 \n",
      "Validation loss decreased (0.142145 --> 0.140285).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12736 valid_loss: 0.14095 test_loss: 0.12702 \n",
      "[ 22/100] train_loss: 0.12434 valid_loss: 0.13363 test_loss: 0.12372 \n",
      "Validation loss decreased (0.140285 --> 0.133631).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11976 valid_loss: 0.12918 test_loss: 0.12055 \n",
      "Validation loss decreased (0.133631 --> 0.129183).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11875 valid_loss: 0.12630 test_loss: 0.11667 \n",
      "Validation loss decreased (0.129183 --> 0.126297).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11873 valid_loss: 0.12368 test_loss: 0.11315 \n",
      "Validation loss decreased (0.126297 --> 0.123679).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11845 valid_loss: 0.11605 test_loss: 0.11127 \n",
      "Validation loss decreased (0.123679 --> 0.116046).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11038 valid_loss: 0.11896 test_loss: 0.10798 \n",
      "[ 28/100] train_loss: 0.11613 valid_loss: 0.11800 test_loss: 0.10555 \n",
      "[ 29/100] train_loss: 0.10751 valid_loss: 0.11032 test_loss: 0.10330 \n",
      "Validation loss decreased (0.116046 --> 0.110324).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10591 valid_loss: 0.11231 test_loss: 0.10137 \n",
      "[ 31/100] train_loss: 0.10272 valid_loss: 0.11948 test_loss: 0.10146 \n",
      "[ 32/100] train_loss: 0.10528 valid_loss: 0.10351 test_loss: 0.09697 \n",
      "Validation loss decreased (0.110324 --> 0.103506).  Saving model ...\n",
      "[ 33/100] train_loss: 0.10203 valid_loss: 0.10155 test_loss: 0.09583 \n",
      "Validation loss decreased (0.103506 --> 0.101552).  Saving model ...\n",
      "[ 34/100] train_loss: 0.10142 valid_loss: 0.11017 test_loss: 0.09423 \n",
      "[ 35/100] train_loss: 0.10172 valid_loss: 0.10207 test_loss: 0.09143 \n",
      "[ 36/100] train_loss: 0.09524 valid_loss: 0.09847 test_loss: 0.09048 \n",
      "Validation loss decreased (0.101552 --> 0.098470).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09757 valid_loss: 0.10043 test_loss: 0.08908 \n",
      "[ 38/100] train_loss: 0.08757 valid_loss: 0.10377 test_loss: 0.08915 \n",
      "[ 39/100] train_loss: 0.09651 valid_loss: 0.10014 test_loss: 0.08802 \n",
      "[ 40/100] train_loss: 0.09171 valid_loss: 0.09721 test_loss: 0.08587 \n",
      "Validation loss decreased (0.098470 --> 0.097208).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08939 valid_loss: 0.09893 test_loss: 0.08470 \n",
      "[ 42/100] train_loss: 0.08717 valid_loss: 0.09869 test_loss: 0.08354 \n",
      "[ 43/100] train_loss: 0.08269 valid_loss: 0.09339 test_loss: 0.08208 \n",
      "Validation loss decreased (0.097208 --> 0.093393).  Saving model ...\n",
      "[ 44/100] train_loss: 0.08730 valid_loss: 0.09359 test_loss: 0.08069 \n",
      "[ 45/100] train_loss: 0.09347 valid_loss: 0.08992 test_loss: 0.08061 \n",
      "Validation loss decreased (0.093393 --> 0.089919).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08090 valid_loss: 0.09637 test_loss: 0.08024 \n",
      "[ 47/100] train_loss: 0.08315 valid_loss: 0.09538 test_loss: 0.07899 \n",
      "[ 48/100] train_loss: 0.08822 valid_loss: 0.08725 test_loss: 0.07797 \n",
      "Validation loss decreased (0.089919 --> 0.087253).  Saving model ...\n",
      "[ 49/100] train_loss: 0.08649 valid_loss: 0.09235 test_loss: 0.07715 \n",
      "[ 50/100] train_loss: 0.07855 valid_loss: 0.09107 test_loss: 0.07624 \n",
      "[ 51/100] train_loss: 0.07959 valid_loss: 0.09275 test_loss: 0.07581 \n",
      "[ 52/100] train_loss: 0.07856 valid_loss: 0.08468 test_loss: 0.07537 \n",
      "Validation loss decreased (0.087253 --> 0.084675).  Saving model ...\n",
      "[ 53/100] train_loss: 0.07777 valid_loss: 0.08963 test_loss: 0.07437 \n",
      "[ 54/100] train_loss: 0.08825 valid_loss: 0.08943 test_loss: 0.07456 \n",
      "[ 55/100] train_loss: 0.07906 valid_loss: 0.08964 test_loss: 0.07399 \n",
      "[ 56/100] train_loss: 0.07784 valid_loss: 0.08814 test_loss: 0.07317 \n",
      "[ 57/100] train_loss: 0.08193 valid_loss: 0.08802 test_loss: 0.07273 \n",
      "[ 58/100] train_loss: 0.07554 valid_loss: 0.09556 test_loss: 0.07352 \n",
      "[ 59/100] train_loss: 0.07759 valid_loss: 0.09023 test_loss: 0.07193 \n",
      "[ 60/100] train_loss: 0.07390 valid_loss: 0.09356 test_loss: 0.07219 \n",
      "[ 61/100] train_loss: 0.07947 valid_loss: 0.08841 test_loss: 0.07075 \n",
      "[ 62/100] train_loss: 0.07153 valid_loss: 0.08336 test_loss: 0.07088 \n",
      "Validation loss decreased (0.084675 --> 0.083359).  Saving model ...\n",
      "[ 63/100] train_loss: 0.07244 valid_loss: 0.08669 test_loss: 0.07018 \n",
      "[ 64/100] train_loss: 0.07999 valid_loss: 0.09546 test_loss: 0.07152 \n",
      "[ 65/100] train_loss: 0.07983 valid_loss: 0.08731 test_loss: 0.06978 \n",
      "[ 66/100] train_loss: 0.07353 valid_loss: 0.08490 test_loss: 0.06956 \n",
      "[ 67/100] train_loss: 0.07412 valid_loss: 0.08483 test_loss: 0.06904 \n",
      "[ 68/100] train_loss: 0.07286 valid_loss: 0.08981 test_loss: 0.06885 \n",
      "[ 69/100] train_loss: 0.07599 valid_loss: 0.08413 test_loss: 0.06785 \n",
      "[ 70/100] train_loss: 0.07236 valid_loss: 0.08055 test_loss: 0.06832 \n",
      "Validation loss decreased (0.083359 --> 0.080551).  Saving model ...\n",
      "[ 71/100] train_loss: 0.07518 valid_loss: 0.08335 test_loss: 0.06824 \n",
      "[ 72/100] train_loss: 0.07361 valid_loss: 0.08701 test_loss: 0.06764 \n",
      "[ 73/100] train_loss: 0.06535 valid_loss: 0.08600 test_loss: 0.06709 \n",
      "[ 74/100] train_loss: 0.07621 valid_loss: 0.08817 test_loss: 0.06687 \n",
      "[ 75/100] train_loss: 0.06858 valid_loss: 0.09008 test_loss: 0.06693 \n",
      "[ 76/100] train_loss: 0.07191 valid_loss: 0.08355 test_loss: 0.06614 \n",
      "[ 77/100] train_loss: 0.07037 valid_loss: 0.08239 test_loss: 0.06656 \n",
      "[ 78/100] train_loss: 0.07352 valid_loss: 0.09118 test_loss: 0.06702 \n",
      "[ 79/100] train_loss: 0.06803 valid_loss: 0.08823 test_loss: 0.06587 \n",
      "[ 80/100] train_loss: 0.07284 valid_loss: 0.08367 test_loss: 0.06541 \n",
      "[ 81/100] train_loss: 0.06872 valid_loss: 0.08709 test_loss: 0.06574 \n",
      "[ 82/100] train_loss: 0.06912 valid_loss: 0.09175 test_loss: 0.06634 \n",
      "[ 83/100] train_loss: 0.07048 valid_loss: 0.08762 test_loss: 0.06522 \n",
      "[ 84/100] train_loss: 0.07162 valid_loss: 0.08166 test_loss: 0.06618 \n",
      "[ 85/100] train_loss: 0.06843 valid_loss: 0.08850 test_loss: 0.06504 \n",
      "[ 86/100] train_loss: 0.07813 valid_loss: 0.09034 test_loss: 0.06538 \n",
      "[ 87/100] train_loss: 0.07326 valid_loss: 0.08380 test_loss: 0.06519 \n",
      "[ 88/100] train_loss: 0.07540 valid_loss: 0.08252 test_loss: 0.06411 \n",
      "[ 89/100] train_loss: 0.07467 valid_loss: 0.08661 test_loss: 0.06408 \n",
      "[ 90/100] train_loss: 0.07108 valid_loss: 0.08634 test_loss: 0.06485 \n",
      "[ 91/100] train_loss: 0.07052 valid_loss: 0.08605 test_loss: 0.06497 \n",
      "[ 92/100] train_loss: 0.07375 valid_loss: 0.08792 test_loss: 0.06377 \n",
      "[ 93/100] train_loss: 0.06592 valid_loss: 0.08065 test_loss: 0.06395 \n",
      "[ 94/100] train_loss: 0.06923 valid_loss: 0.08566 test_loss: 0.06298 \n",
      "[ 95/100] train_loss: 0.07142 valid_loss: 0.08186 test_loss: 0.06287 \n",
      "[ 96/100] train_loss: 0.06921 valid_loss: 0.08044 test_loss: 0.06333 \n",
      "Validation loss decreased (0.080551 --> 0.080441).  Saving model ...\n",
      "[ 97/100] train_loss: 0.07280 valid_loss: 0.09165 test_loss: 0.06416 \n",
      "[ 98/100] train_loss: 0.06290 valid_loss: 0.08296 test_loss: 0.06239 \n",
      "[ 99/100] train_loss: 0.06890 valid_loss: 0.07862 test_loss: 0.06280 \n",
      "Validation loss decreased (0.080441 --> 0.078623).  Saving model ...\n",
      "[100/100] train_loss: 0.06636 valid_loss: 0.08220 test_loss: 0.06234 \n",
      "TRAINING MODEL 9\n",
      "[  1/100] train_loss: 0.18523 valid_loss: 0.19310 test_loss: 0.18545 \n",
      "Validation loss decreased (inf --> 0.193100).  Saving model ...\n",
      "[  2/100] train_loss: 0.18471 valid_loss: 0.19181 test_loss: 0.18409 \n",
      "Validation loss decreased (0.193100 --> 0.191807).  Saving model ...\n",
      "[  3/100] train_loss: 0.18088 valid_loss: 0.19009 test_loss: 0.18211 \n",
      "Validation loss decreased (0.191807 --> 0.190086).  Saving model ...\n",
      "[  4/100] train_loss: 0.17810 valid_loss: 0.18781 test_loss: 0.17936 \n",
      "Validation loss decreased (0.190086 --> 0.187805).  Saving model ...\n",
      "[  5/100] train_loss: 0.17836 valid_loss: 0.18527 test_loss: 0.17604 \n",
      "Validation loss decreased (0.187805 --> 0.185267).  Saving model ...\n",
      "[  6/100] train_loss: 0.17431 valid_loss: 0.18198 test_loss: 0.17269 \n",
      "Validation loss decreased (0.185267 --> 0.181978).  Saving model ...\n",
      "[  7/100] train_loss: 0.17223 valid_loss: 0.17939 test_loss: 0.16940 \n",
      "Validation loss decreased (0.181978 --> 0.179390).  Saving model ...\n",
      "[  8/100] train_loss: 0.16976 valid_loss: 0.17593 test_loss: 0.16622 \n",
      "Validation loss decreased (0.179390 --> 0.175930).  Saving model ...\n",
      "[  9/100] train_loss: 0.16508 valid_loss: 0.17306 test_loss: 0.16292 \n",
      "Validation loss decreased (0.175930 --> 0.173061).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16173 valid_loss: 0.17035 test_loss: 0.15920 \n",
      "Validation loss decreased (0.173061 --> 0.170352).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15830 valid_loss: 0.16826 test_loss: 0.15592 \n",
      "Validation loss decreased (0.170352 --> 0.168257).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15456 valid_loss: 0.16523 test_loss: 0.15207 \n",
      "Validation loss decreased (0.168257 --> 0.165232).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15163 valid_loss: 0.15938 test_loss: 0.14743 \n",
      "Validation loss decreased (0.165232 --> 0.159381).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14792 valid_loss: 0.15586 test_loss: 0.14423 \n",
      "Validation loss decreased (0.159381 --> 0.155859).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14227 valid_loss: 0.15158 test_loss: 0.14024 \n",
      "Validation loss decreased (0.155859 --> 0.151584).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13922 valid_loss: 0.14993 test_loss: 0.13669 \n",
      "Validation loss decreased (0.151584 --> 0.149928).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13682 valid_loss: 0.14320 test_loss: 0.13195 \n",
      "Validation loss decreased (0.149928 --> 0.143197).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13238 valid_loss: 0.13982 test_loss: 0.12816 \n",
      "Validation loss decreased (0.143197 --> 0.139819).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13000 valid_loss: 0.14089 test_loss: 0.12542 \n",
      "[ 20/100] train_loss: 0.12388 valid_loss: 0.13702 test_loss: 0.12193 \n",
      "Validation loss decreased (0.139819 --> 0.137023).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12324 valid_loss: 0.12588 test_loss: 0.11709 \n",
      "Validation loss decreased (0.137023 --> 0.125876).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11930 valid_loss: 0.12304 test_loss: 0.11436 \n",
      "Validation loss decreased (0.125876 --> 0.123037).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11586 valid_loss: 0.12560 test_loss: 0.11192 \n",
      "[ 24/100] train_loss: 0.11517 valid_loss: 0.12194 test_loss: 0.10835 \n",
      "Validation loss decreased (0.123037 --> 0.121940).  Saving model ...\n",
      "[ 25/100] train_loss: 0.10803 valid_loss: 0.11596 test_loss: 0.10510 \n",
      "Validation loss decreased (0.121940 --> 0.115965).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10672 valid_loss: 0.11892 test_loss: 0.10391 \n",
      "[ 27/100] train_loss: 0.10624 valid_loss: 0.11377 test_loss: 0.10014 \n",
      "Validation loss decreased (0.115965 --> 0.113768).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10522 valid_loss: 0.10582 test_loss: 0.09739 \n",
      "Validation loss decreased (0.113768 --> 0.105819).  Saving model ...\n",
      "[ 29/100] train_loss: 0.09843 valid_loss: 0.10870 test_loss: 0.09529 \n",
      "[ 30/100] train_loss: 0.09948 valid_loss: 0.10978 test_loss: 0.09359 \n",
      "[ 31/100] train_loss: 0.09958 valid_loss: 0.09928 test_loss: 0.09026 \n",
      "Validation loss decreased (0.105819 --> 0.099280).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09371 valid_loss: 0.10386 test_loss: 0.09024 \n",
      "[ 33/100] train_loss: 0.09338 valid_loss: 0.10448 test_loss: 0.08904 \n",
      "[ 34/100] train_loss: 0.09267 valid_loss: 0.09814 test_loss: 0.08584 \n",
      "Validation loss decreased (0.099280 --> 0.098137).  Saving model ...\n",
      "[ 35/100] train_loss: 0.08727 valid_loss: 0.09519 test_loss: 0.08430 \n",
      "Validation loss decreased (0.098137 --> 0.095186).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09025 valid_loss: 0.09722 test_loss: 0.08365 \n",
      "[ 37/100] train_loss: 0.08801 valid_loss: 0.09661 test_loss: 0.08245 \n",
      "[ 38/100] train_loss: 0.09048 valid_loss: 0.10799 test_loss: 0.08530 \n",
      "[ 39/100] train_loss: 0.09092 valid_loss: 0.10190 test_loss: 0.08226 \n",
      "[ 40/100] train_loss: 0.08513 valid_loss: 0.08949 test_loss: 0.07907 \n",
      "Validation loss decreased (0.095186 --> 0.089488).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08525 valid_loss: 0.09152 test_loss: 0.07817 \n",
      "[ 42/100] train_loss: 0.07990 valid_loss: 0.09129 test_loss: 0.07700 \n",
      "[ 43/100] train_loss: 0.08317 valid_loss: 0.09850 test_loss: 0.07759 \n",
      "[ 44/100] train_loss: 0.08949 valid_loss: 0.09207 test_loss: 0.07533 \n",
      "[ 45/100] train_loss: 0.08227 valid_loss: 0.09031 test_loss: 0.07446 \n",
      "[ 46/100] train_loss: 0.07992 valid_loss: 0.09388 test_loss: 0.07469 \n",
      "[ 47/100] train_loss: 0.08486 valid_loss: 0.09645 test_loss: 0.07507 \n",
      "[ 48/100] train_loss: 0.07911 valid_loss: 0.09249 test_loss: 0.07394 \n",
      "[ 49/100] train_loss: 0.08361 valid_loss: 0.09296 test_loss: 0.07367 \n",
      "[ 50/100] train_loss: 0.08777 valid_loss: 0.09576 test_loss: 0.07358 \n",
      "[ 51/100] train_loss: 0.07980 valid_loss: 0.08544 test_loss: 0.07165 \n",
      "Validation loss decreased (0.089488 --> 0.085444).  Saving model ...\n",
      "[ 52/100] train_loss: 0.08135 valid_loss: 0.09653 test_loss: 0.07332 \n",
      "[ 53/100] train_loss: 0.08380 valid_loss: 0.09674 test_loss: 0.07301 \n",
      "[ 54/100] train_loss: 0.07318 valid_loss: 0.08920 test_loss: 0.07020 \n",
      "[ 55/100] train_loss: 0.07868 valid_loss: 0.08375 test_loss: 0.06987 \n",
      "Validation loss decreased (0.085444 --> 0.083748).  Saving model ...\n",
      "[ 56/100] train_loss: 0.08024 valid_loss: 0.08976 test_loss: 0.07035 \n",
      "[ 57/100] train_loss: 0.07495 valid_loss: 0.10160 test_loss: 0.07357 \n",
      "[ 58/100] train_loss: 0.07884 valid_loss: 0.08626 test_loss: 0.06837 \n",
      "[ 59/100] train_loss: 0.07790 valid_loss: 0.08397 test_loss: 0.06852 \n",
      "[ 60/100] train_loss: 0.07935 valid_loss: 0.08852 test_loss: 0.06895 \n",
      "[ 61/100] train_loss: 0.07490 valid_loss: 0.09800 test_loss: 0.07103 \n",
      "[ 62/100] train_loss: 0.07642 valid_loss: 0.09217 test_loss: 0.06861 \n",
      "[ 63/100] train_loss: 0.07347 valid_loss: 0.09081 test_loss: 0.06817 \n",
      "[ 64/100] train_loss: 0.07725 valid_loss: 0.09079 test_loss: 0.06794 \n",
      "[ 65/100] train_loss: 0.07198 valid_loss: 0.09011 test_loss: 0.06758 \n",
      "[ 66/100] train_loss: 0.07478 valid_loss: 0.08790 test_loss: 0.06657 \n",
      "[ 67/100] train_loss: 0.06791 valid_loss: 0.08749 test_loss: 0.06643 \n",
      "[ 68/100] train_loss: 0.07349 valid_loss: 0.09236 test_loss: 0.06733 \n",
      "[ 69/100] train_loss: 0.07119 valid_loss: 0.08425 test_loss: 0.06588 \n",
      "[ 70/100] train_loss: 0.07190 valid_loss: 0.09127 test_loss: 0.06664 \n",
      "[ 71/100] train_loss: 0.07288 valid_loss: 0.09342 test_loss: 0.06729 \n",
      "[ 72/100] train_loss: 0.07608 valid_loss: 0.09466 test_loss: 0.06765 \n",
      "[ 73/100] train_loss: 0.07307 valid_loss: 0.08212 test_loss: 0.06538 \n",
      "Validation loss decreased (0.083748 --> 0.082117).  Saving model ...\n",
      "[ 74/100] train_loss: 0.07319 valid_loss: 0.08777 test_loss: 0.06550 \n",
      "[ 75/100] train_loss: 0.07036 valid_loss: 0.09771 test_loss: 0.06790 \n",
      "[ 76/100] train_loss: 0.07061 valid_loss: 0.08969 test_loss: 0.06501 \n",
      "[ 77/100] train_loss: 0.07352 valid_loss: 0.08184 test_loss: 0.06396 \n",
      "Validation loss decreased (0.082117 --> 0.081835).  Saving model ...\n",
      "[ 78/100] train_loss: 0.06577 valid_loss: 0.08471 test_loss: 0.06388 \n",
      "[ 79/100] train_loss: 0.06666 valid_loss: 0.09368 test_loss: 0.06582 \n",
      "[ 80/100] train_loss: 0.07238 valid_loss: 0.09432 test_loss: 0.06601 \n",
      "[ 81/100] train_loss: 0.07265 valid_loss: 0.09716 test_loss: 0.06702 \n",
      "[ 82/100] train_loss: 0.07184 valid_loss: 0.08605 test_loss: 0.06361 \n",
      "[ 83/100] train_loss: 0.07597 valid_loss: 0.08928 test_loss: 0.06429 \n",
      "[ 84/100] train_loss: 0.07471 valid_loss: 0.08839 test_loss: 0.06406 \n",
      "[ 85/100] train_loss: 0.07624 valid_loss: 0.08651 test_loss: 0.06350 \n",
      "[ 86/100] train_loss: 0.07717 valid_loss: 0.08302 test_loss: 0.06305 \n",
      "[ 87/100] train_loss: 0.06944 valid_loss: 0.09112 test_loss: 0.06442 \n",
      "[ 88/100] train_loss: 0.07147 valid_loss: 0.08610 test_loss: 0.06301 \n",
      "[ 89/100] train_loss: 0.07120 valid_loss: 0.08213 test_loss: 0.06244 \n",
      "[ 90/100] train_loss: 0.06878 valid_loss: 0.08698 test_loss: 0.06321 \n",
      "[ 91/100] train_loss: 0.06438 valid_loss: 0.08620 test_loss: 0.06228 \n",
      "[ 92/100] train_loss: 0.06390 valid_loss: 0.09244 test_loss: 0.06404 \n",
      "[ 93/100] train_loss: 0.06759 valid_loss: 0.07853 test_loss: 0.06142 \n",
      "Validation loss decreased (0.081835 --> 0.078530).  Saving model ...\n",
      "[ 94/100] train_loss: 0.07021 valid_loss: 0.08849 test_loss: 0.06285 \n",
      "[ 95/100] train_loss: 0.07029 valid_loss: 0.08708 test_loss: 0.06229 \n",
      "[ 96/100] train_loss: 0.06121 valid_loss: 0.08352 test_loss: 0.06133 \n",
      "[ 97/100] train_loss: 0.06723 valid_loss: 0.08483 test_loss: 0.06143 \n",
      "[ 98/100] train_loss: 0.06198 valid_loss: 0.09244 test_loss: 0.06331 \n",
      "[ 99/100] train_loss: 0.06851 valid_loss: 0.09051 test_loss: 0.06266 \n",
      "[100/100] train_loss: 0.07033 valid_loss: 0.08388 test_loss: 0.06101 \n",
      "TRAINING MODEL 10\n",
      "[  1/100] train_loss: 0.18611 valid_loss: 0.19218 test_loss: 0.18616 \n",
      "Validation loss decreased (inf --> 0.192177).  Saving model ...\n",
      "[  2/100] train_loss: 0.18301 valid_loss: 0.19084 test_loss: 0.18492 \n",
      "Validation loss decreased (0.192177 --> 0.190836).  Saving model ...\n",
      "[  3/100] train_loss: 0.18033 valid_loss: 0.18954 test_loss: 0.18292 \n",
      "Validation loss decreased (0.190836 --> 0.189537).  Saving model ...\n",
      "[  4/100] train_loss: 0.17947 valid_loss: 0.18769 test_loss: 0.18016 \n",
      "Validation loss decreased (0.189537 --> 0.187686).  Saving model ...\n",
      "[  5/100] train_loss: 0.17578 valid_loss: 0.18523 test_loss: 0.17711 \n",
      "Validation loss decreased (0.187686 --> 0.185227).  Saving model ...\n",
      "[  6/100] train_loss: 0.17391 valid_loss: 0.18256 test_loss: 0.17368 \n",
      "Validation loss decreased (0.185227 --> 0.182559).  Saving model ...\n",
      "[  7/100] train_loss: 0.17049 valid_loss: 0.17955 test_loss: 0.17017 \n",
      "Validation loss decreased (0.182559 --> 0.179547).  Saving model ...\n",
      "[  8/100] train_loss: 0.16737 valid_loss: 0.17669 test_loss: 0.16690 \n",
      "Validation loss decreased (0.179547 --> 0.176686).  Saving model ...\n",
      "[  9/100] train_loss: 0.16490 valid_loss: 0.17465 test_loss: 0.16324 \n",
      "Validation loss decreased (0.176686 --> 0.174650).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16114 valid_loss: 0.17051 test_loss: 0.15938 \n",
      "Validation loss decreased (0.174650 --> 0.170508).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15757 valid_loss: 0.16774 test_loss: 0.15582 \n",
      "Validation loss decreased (0.170508 --> 0.167737).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15391 valid_loss: 0.16408 test_loss: 0.15252 \n",
      "Validation loss decreased (0.167737 --> 0.164084).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15086 valid_loss: 0.16023 test_loss: 0.14859 \n",
      "Validation loss decreased (0.164084 --> 0.160228).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14495 valid_loss: 0.15751 test_loss: 0.14478 \n",
      "Validation loss decreased (0.160228 --> 0.157515).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14436 valid_loss: 0.15679 test_loss: 0.14145 \n",
      "Validation loss decreased (0.157515 --> 0.156793).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13820 valid_loss: 0.14796 test_loss: 0.13705 \n",
      "Validation loss decreased (0.156793 --> 0.147956).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13459 valid_loss: 0.14415 test_loss: 0.13481 \n",
      "Validation loss decreased (0.147956 --> 0.144147).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13095 valid_loss: 0.14317 test_loss: 0.12950 \n",
      "Validation loss decreased (0.144147 --> 0.143174).  Saving model ...\n",
      "[ 19/100] train_loss: 0.12877 valid_loss: 0.14218 test_loss: 0.12595 \n",
      "Validation loss decreased (0.143174 --> 0.142184).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12621 valid_loss: 0.13418 test_loss: 0.12178 \n",
      "Validation loss decreased (0.142184 --> 0.134178).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12148 valid_loss: 0.13161 test_loss: 0.11896 \n",
      "Validation loss decreased (0.134178 --> 0.131611).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11684 valid_loss: 0.12591 test_loss: 0.11462 \n",
      "Validation loss decreased (0.131611 --> 0.125907).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11260 valid_loss: 0.11930 test_loss: 0.11090 \n",
      "Validation loss decreased (0.125907 --> 0.119298).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11258 valid_loss: 0.12277 test_loss: 0.10834 \n",
      "[ 25/100] train_loss: 0.10790 valid_loss: 0.12001 test_loss: 0.10546 \n",
      "[ 26/100] train_loss: 0.10197 valid_loss: 0.11106 test_loss: 0.10187 \n",
      "Validation loss decreased (0.119298 --> 0.111063).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11189 valid_loss: 0.11717 test_loss: 0.10059 \n",
      "[ 28/100] train_loss: 0.10660 valid_loss: 0.11795 test_loss: 0.09945 \n",
      "[ 29/100] train_loss: 0.10102 valid_loss: 0.10170 test_loss: 0.09614 \n",
      "Validation loss decreased (0.111063 --> 0.101704).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10680 valid_loss: 0.10513 test_loss: 0.09346 \n",
      "[ 31/100] train_loss: 0.09732 valid_loss: 0.10670 test_loss: 0.09164 \n",
      "[ 32/100] train_loss: 0.09485 valid_loss: 0.10534 test_loss: 0.08980 \n",
      "[ 33/100] train_loss: 0.08865 valid_loss: 0.10453 test_loss: 0.08828 \n",
      "[ 34/100] train_loss: 0.09444 valid_loss: 0.09597 test_loss: 0.08604 \n",
      "Validation loss decreased (0.101704 --> 0.095974).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09137 valid_loss: 0.09762 test_loss: 0.08457 \n",
      "[ 36/100] train_loss: 0.08964 valid_loss: 0.09964 test_loss: 0.08364 \n",
      "[ 37/100] train_loss: 0.08921 valid_loss: 0.09319 test_loss: 0.08203 \n",
      "Validation loss decreased (0.095974 --> 0.093190).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08691 valid_loss: 0.09981 test_loss: 0.08171 \n",
      "[ 39/100] train_loss: 0.08873 valid_loss: 0.09285 test_loss: 0.07959 \n",
      "Validation loss decreased (0.093190 --> 0.092846).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08675 valid_loss: 0.09020 test_loss: 0.07899 \n",
      "Validation loss decreased (0.092846 --> 0.090199).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08564 valid_loss: 0.09876 test_loss: 0.07928 \n",
      "[ 42/100] train_loss: 0.08434 valid_loss: 0.09178 test_loss: 0.07717 \n",
      "[ 43/100] train_loss: 0.07797 valid_loss: 0.09203 test_loss: 0.07576 \n",
      "[ 44/100] train_loss: 0.08382 valid_loss: 0.09317 test_loss: 0.07538 \n",
      "[ 45/100] train_loss: 0.07870 valid_loss: 0.08561 test_loss: 0.07524 \n",
      "Validation loss decreased (0.090199 --> 0.085608).  Saving model ...\n",
      "[ 46/100] train_loss: 0.07989 valid_loss: 0.08726 test_loss: 0.07436 \n",
      "[ 47/100] train_loss: 0.08571 valid_loss: 0.10571 test_loss: 0.07820 \n",
      "[ 48/100] train_loss: 0.08542 valid_loss: 0.09119 test_loss: 0.07336 \n",
      "[ 49/100] train_loss: 0.07492 valid_loss: 0.08477 test_loss: 0.07355 \n",
      "Validation loss decreased (0.085608 --> 0.084765).  Saving model ...\n",
      "[ 50/100] train_loss: 0.08415 valid_loss: 0.09274 test_loss: 0.07262 \n",
      "[ 51/100] train_loss: 0.08198 valid_loss: 0.08848 test_loss: 0.07146 \n",
      "[ 52/100] train_loss: 0.08229 valid_loss: 0.09307 test_loss: 0.07196 \n",
      "[ 53/100] train_loss: 0.08499 valid_loss: 0.09515 test_loss: 0.07229 \n",
      "[ 54/100] train_loss: 0.07647 valid_loss: 0.09042 test_loss: 0.07077 \n",
      "[ 55/100] train_loss: 0.07698 valid_loss: 0.09050 test_loss: 0.07074 \n",
      "[ 56/100] train_loss: 0.07628 valid_loss: 0.08817 test_loss: 0.06994 \n",
      "[ 57/100] train_loss: 0.07384 valid_loss: 0.08827 test_loss: 0.06985 \n",
      "[ 58/100] train_loss: 0.07753 valid_loss: 0.09048 test_loss: 0.07018 \n",
      "[ 59/100] train_loss: 0.07404 valid_loss: 0.08686 test_loss: 0.06893 \n",
      "[ 60/100] train_loss: 0.07644 valid_loss: 0.09144 test_loss: 0.06870 \n",
      "[ 61/100] train_loss: 0.07670 valid_loss: 0.08753 test_loss: 0.06823 \n",
      "[ 62/100] train_loss: 0.07035 valid_loss: 0.09050 test_loss: 0.06836 \n",
      "[ 63/100] train_loss: 0.07122 valid_loss: 0.09564 test_loss: 0.06939 \n",
      "[ 64/100] train_loss: 0.07475 valid_loss: 0.08588 test_loss: 0.06691 \n",
      "[ 65/100] train_loss: 0.07724 valid_loss: 0.09681 test_loss: 0.06948 \n",
      "[ 66/100] train_loss: 0.07395 valid_loss: 0.08588 test_loss: 0.06696 \n",
      "[ 67/100] train_loss: 0.06841 valid_loss: 0.08625 test_loss: 0.06689 \n",
      "[ 68/100] train_loss: 0.06867 valid_loss: 0.09429 test_loss: 0.06793 \n",
      "[ 69/100] train_loss: 0.06260 valid_loss: 0.08907 test_loss: 0.06651 \n",
      "[ 70/100] train_loss: 0.07130 valid_loss: 0.09198 test_loss: 0.06676 \n",
      "[ 71/100] train_loss: 0.07439 valid_loss: 0.08436 test_loss: 0.06581 \n",
      "Validation loss decreased (0.084765 --> 0.084363).  Saving model ...\n",
      "[ 72/100] train_loss: 0.07042 valid_loss: 0.08436 test_loss: 0.06738 \n",
      "Validation loss decreased (0.084363 --> 0.084357).  Saving model ...\n",
      "[ 73/100] train_loss: 0.07332 valid_loss: 0.09569 test_loss: 0.06778 \n",
      "[ 74/100] train_loss: 0.06810 valid_loss: 0.09640 test_loss: 0.06760 \n",
      "[ 75/100] train_loss: 0.06900 valid_loss: 0.08424 test_loss: 0.06497 \n",
      "Validation loss decreased (0.084357 --> 0.084235).  Saving model ...\n",
      "[ 76/100] train_loss: 0.06772 valid_loss: 0.08783 test_loss: 0.06543 \n",
      "[ 77/100] train_loss: 0.07677 valid_loss: 0.09308 test_loss: 0.06669 \n",
      "[ 78/100] train_loss: 0.07048 valid_loss: 0.09292 test_loss: 0.06655 \n",
      "[ 79/100] train_loss: 0.07135 valid_loss: 0.08383 test_loss: 0.06537 \n",
      "Validation loss decreased (0.084235 --> 0.083827).  Saving model ...\n",
      "[ 80/100] train_loss: 0.07291 valid_loss: 0.08579 test_loss: 0.06468 \n",
      "[ 81/100] train_loss: 0.06865 valid_loss: 0.09079 test_loss: 0.06496 \n",
      "[ 82/100] train_loss: 0.06795 valid_loss: 0.08753 test_loss: 0.06411 \n",
      "[ 83/100] train_loss: 0.06950 valid_loss: 0.08742 test_loss: 0.06439 \n",
      "[ 84/100] train_loss: 0.07152 valid_loss: 0.08528 test_loss: 0.06430 \n",
      "[ 85/100] train_loss: 0.07121 valid_loss: 0.09277 test_loss: 0.06563 \n",
      "[ 86/100] train_loss: 0.06429 valid_loss: 0.08997 test_loss: 0.06421 \n",
      "[ 87/100] train_loss: 0.07029 valid_loss: 0.08817 test_loss: 0.06329 \n",
      "[ 88/100] train_loss: 0.07087 valid_loss: 0.08620 test_loss: 0.06308 \n",
      "[ 89/100] train_loss: 0.06937 valid_loss: 0.08668 test_loss: 0.06305 \n",
      "[ 90/100] train_loss: 0.06514 valid_loss: 0.09087 test_loss: 0.06367 \n",
      "[ 91/100] train_loss: 0.07158 valid_loss: 0.09965 test_loss: 0.06607 \n",
      "[ 92/100] train_loss: 0.07044 valid_loss: 0.08566 test_loss: 0.06256 \n",
      "[ 93/100] train_loss: 0.07098 valid_loss: 0.09236 test_loss: 0.06393 \n",
      "[ 94/100] train_loss: 0.06873 valid_loss: 0.09548 test_loss: 0.06458 \n",
      "[ 95/100] train_loss: 0.06430 valid_loss: 0.08089 test_loss: 0.06242 \n",
      "Validation loss decreased (0.083827 --> 0.080889).  Saving model ...\n",
      "[ 96/100] train_loss: 0.07317 valid_loss: 0.09294 test_loss: 0.06376 \n",
      "[ 97/100] train_loss: 0.07265 valid_loss: 0.09645 test_loss: 0.06532 \n",
      "[ 98/100] train_loss: 0.06693 valid_loss: 0.09173 test_loss: 0.06325 \n",
      "[ 99/100] train_loss: 0.06695 valid_loss: 0.08332 test_loss: 0.06189 \n",
      "[100/100] train_loss: 0.06992 valid_loss: 0.10361 test_loss: 0.06707 \n",
      "TRAINING MODEL 11\n",
      "[  1/100] train_loss: 0.18464 valid_loss: 0.19287 test_loss: 0.18495 \n",
      "Validation loss decreased (inf --> 0.192868).  Saving model ...\n",
      "[  2/100] train_loss: 0.18238 valid_loss: 0.19154 test_loss: 0.18332 \n",
      "Validation loss decreased (0.192868 --> 0.191539).  Saving model ...\n",
      "[  3/100] train_loss: 0.18011 valid_loss: 0.18963 test_loss: 0.18116 \n",
      "Validation loss decreased (0.191539 --> 0.189629).  Saving model ...\n",
      "[  4/100] train_loss: 0.17781 valid_loss: 0.18754 test_loss: 0.17852 \n",
      "Validation loss decreased (0.189629 --> 0.187542).  Saving model ...\n",
      "[  5/100] train_loss: 0.17513 valid_loss: 0.18453 test_loss: 0.17546 \n",
      "Validation loss decreased (0.187542 --> 0.184535).  Saving model ...\n",
      "[  6/100] train_loss: 0.17298 valid_loss: 0.18189 test_loss: 0.17255 \n",
      "Validation loss decreased (0.184535 --> 0.181892).  Saving model ...\n",
      "[  7/100] train_loss: 0.17109 valid_loss: 0.17850 test_loss: 0.16932 \n",
      "Validation loss decreased (0.181892 --> 0.178496).  Saving model ...\n",
      "[  8/100] train_loss: 0.16776 valid_loss: 0.17577 test_loss: 0.16627 \n",
      "Validation loss decreased (0.178496 --> 0.175772).  Saving model ...\n",
      "[  9/100] train_loss: 0.16494 valid_loss: 0.17361 test_loss: 0.16371 \n",
      "Validation loss decreased (0.175772 --> 0.173608).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16198 valid_loss: 0.16807 test_loss: 0.15952 \n",
      "Validation loss decreased (0.173608 --> 0.168071).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15971 valid_loss: 0.16742 test_loss: 0.15730 \n",
      "Validation loss decreased (0.168071 --> 0.167423).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15691 valid_loss: 0.16440 test_loss: 0.15457 \n",
      "Validation loss decreased (0.167423 --> 0.164404).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15379 valid_loss: 0.16182 test_loss: 0.15316 \n",
      "Validation loss decreased (0.164404 --> 0.161822).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15096 valid_loss: 0.16123 test_loss: 0.15147 \n",
      "Validation loss decreased (0.161822 --> 0.161233).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14601 valid_loss: 0.15763 test_loss: 0.14781 \n",
      "Validation loss decreased (0.161233 --> 0.157626).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14516 valid_loss: 0.15473 test_loss: 0.14358 \n",
      "Validation loss decreased (0.157626 --> 0.154729).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14219 valid_loss: 0.14829 test_loss: 0.13954 \n",
      "Validation loss decreased (0.154729 --> 0.148288).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13784 valid_loss: 0.14359 test_loss: 0.13674 \n",
      "Validation loss decreased (0.148288 --> 0.143591).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13443 valid_loss: 0.13871 test_loss: 0.13307 \n",
      "Validation loss decreased (0.143591 --> 0.138714).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13401 valid_loss: 0.13928 test_loss: 0.12959 \n",
      "[ 21/100] train_loss: 0.12876 valid_loss: 0.13258 test_loss: 0.12654 \n",
      "Validation loss decreased (0.138714 --> 0.132578).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12849 valid_loss: 0.13193 test_loss: 0.12416 \n",
      "Validation loss decreased (0.132578 --> 0.131928).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12127 valid_loss: 0.12783 test_loss: 0.12124 \n",
      "Validation loss decreased (0.131928 --> 0.127826).  Saving model ...\n",
      "[ 24/100] train_loss: 0.12164 valid_loss: 0.12770 test_loss: 0.11903 \n",
      "Validation loss decreased (0.127826 --> 0.127700).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11955 valid_loss: 0.12427 test_loss: 0.11603 \n",
      "Validation loss decreased (0.127700 --> 0.124266).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10921 valid_loss: 0.12252 test_loss: 0.11386 \n",
      "Validation loss decreased (0.124266 --> 0.122521).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11038 valid_loss: 0.11672 test_loss: 0.11042 \n",
      "Validation loss decreased (0.122521 --> 0.116721).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10718 valid_loss: 0.11826 test_loss: 0.10844 \n",
      "[ 29/100] train_loss: 0.10631 valid_loss: 0.10806 test_loss: 0.10458 \n",
      "Validation loss decreased (0.116721 --> 0.108063).  Saving model ...\n",
      "[ 30/100] train_loss: 0.11033 valid_loss: 0.11056 test_loss: 0.10284 \n",
      "[ 31/100] train_loss: 0.10200 valid_loss: 0.10842 test_loss: 0.10053 \n",
      "[ 32/100] train_loss: 0.10193 valid_loss: 0.10987 test_loss: 0.09935 \n",
      "[ 33/100] train_loss: 0.10155 valid_loss: 0.10311 test_loss: 0.09641 \n",
      "Validation loss decreased (0.108063 --> 0.103112).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09626 valid_loss: 0.10222 test_loss: 0.09422 \n",
      "Validation loss decreased (0.103112 --> 0.102218).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09030 valid_loss: 0.10042 test_loss: 0.09184 \n",
      "Validation loss decreased (0.102218 --> 0.100416).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09323 valid_loss: 0.09544 test_loss: 0.08930 \n",
      "Validation loss decreased (0.100416 --> 0.095439).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09307 valid_loss: 0.10175 test_loss: 0.08927 \n",
      "[ 38/100] train_loss: 0.08974 valid_loss: 0.09365 test_loss: 0.08615 \n",
      "Validation loss decreased (0.095439 --> 0.093653).  Saving model ...\n",
      "[ 39/100] train_loss: 0.09099 valid_loss: 0.09729 test_loss: 0.08539 \n",
      "[ 40/100] train_loss: 0.09016 valid_loss: 0.10192 test_loss: 0.08566 \n",
      "[ 41/100] train_loss: 0.08877 valid_loss: 0.08744 test_loss: 0.08190 \n",
      "Validation loss decreased (0.093653 --> 0.087437).  Saving model ...\n",
      "[ 42/100] train_loss: 0.09011 valid_loss: 0.08896 test_loss: 0.08167 \n",
      "[ 43/100] train_loss: 0.08335 valid_loss: 0.09701 test_loss: 0.08163 \n",
      "[ 44/100] train_loss: 0.08370 valid_loss: 0.08807 test_loss: 0.07870 \n",
      "[ 45/100] train_loss: 0.08850 valid_loss: 0.09623 test_loss: 0.07942 \n",
      "[ 46/100] train_loss: 0.08495 valid_loss: 0.09090 test_loss: 0.07745 \n",
      "[ 47/100] train_loss: 0.08480 valid_loss: 0.08319 test_loss: 0.07661 \n",
      "Validation loss decreased (0.087437 --> 0.083186).  Saving model ...\n",
      "[ 48/100] train_loss: 0.09549 valid_loss: 0.08539 test_loss: 0.07578 \n",
      "[ 49/100] train_loss: 0.08873 valid_loss: 0.09563 test_loss: 0.07717 \n",
      "[ 50/100] train_loss: 0.07955 valid_loss: 0.08868 test_loss: 0.07497 \n",
      "[ 51/100] train_loss: 0.08613 valid_loss: 0.08716 test_loss: 0.07343 \n",
      "[ 52/100] train_loss: 0.07423 valid_loss: 0.09217 test_loss: 0.07406 \n",
      "[ 53/100] train_loss: 0.08132 valid_loss: 0.08989 test_loss: 0.07318 \n",
      "[ 54/100] train_loss: 0.07572 valid_loss: 0.08581 test_loss: 0.07198 \n",
      "[ 55/100] train_loss: 0.08444 valid_loss: 0.09538 test_loss: 0.07396 \n",
      "[ 56/100] train_loss: 0.07629 valid_loss: 0.08839 test_loss: 0.07170 \n",
      "[ 57/100] train_loss: 0.07831 valid_loss: 0.08957 test_loss: 0.07170 \n",
      "[ 58/100] train_loss: 0.07550 valid_loss: 0.08562 test_loss: 0.07068 \n",
      "[ 59/100] train_loss: 0.07555 valid_loss: 0.08833 test_loss: 0.07049 \n",
      "[ 60/100] train_loss: 0.07430 valid_loss: 0.08544 test_loss: 0.06929 \n",
      "[ 61/100] train_loss: 0.07989 valid_loss: 0.08207 test_loss: 0.06940 \n",
      "Validation loss decreased (0.083186 --> 0.082075).  Saving model ...\n",
      "[ 62/100] train_loss: 0.07852 valid_loss: 0.08718 test_loss: 0.06949 \n",
      "[ 63/100] train_loss: 0.08499 valid_loss: 0.08435 test_loss: 0.06940 \n",
      "[ 64/100] train_loss: 0.07783 valid_loss: 0.08527 test_loss: 0.07093 \n",
      "[ 65/100] train_loss: 0.07571 valid_loss: 0.08990 test_loss: 0.06944 \n",
      "[ 66/100] train_loss: 0.07823 valid_loss: 0.08768 test_loss: 0.06840 \n",
      "[ 67/100] train_loss: 0.07888 valid_loss: 0.09350 test_loss: 0.06958 \n",
      "[ 68/100] train_loss: 0.07367 valid_loss: 0.08305 test_loss: 0.06740 \n",
      "[ 69/100] train_loss: 0.07192 valid_loss: 0.08873 test_loss: 0.06733 \n",
      "[ 70/100] train_loss: 0.07506 valid_loss: 0.09110 test_loss: 0.06764 \n",
      "[ 71/100] train_loss: 0.07453 valid_loss: 0.08337 test_loss: 0.06653 \n",
      "[ 72/100] train_loss: 0.06828 valid_loss: 0.09225 test_loss: 0.06780 \n",
      "[ 73/100] train_loss: 0.07160 valid_loss: 0.08610 test_loss: 0.06599 \n",
      "[ 74/100] train_loss: 0.07668 valid_loss: 0.08241 test_loss: 0.06563 \n",
      "[ 75/100] train_loss: 0.07066 valid_loss: 0.08268 test_loss: 0.06565 \n",
      "[ 76/100] train_loss: 0.06779 valid_loss: 0.09640 test_loss: 0.06851 \n",
      "[ 77/100] train_loss: 0.06547 valid_loss: 0.08275 test_loss: 0.06513 \n",
      "[ 78/100] train_loss: 0.07316 valid_loss: 0.08679 test_loss: 0.06509 \n",
      "[ 79/100] train_loss: 0.06688 valid_loss: 0.09215 test_loss: 0.06621 \n",
      "[ 80/100] train_loss: 0.06883 valid_loss: 0.08339 test_loss: 0.06419 \n",
      "[ 81/100] train_loss: 0.06908 valid_loss: 0.09738 test_loss: 0.06774 \n",
      "[ 82/100] train_loss: 0.07114 valid_loss: 0.08416 test_loss: 0.06401 \n",
      "[ 83/100] train_loss: 0.06953 valid_loss: 0.08704 test_loss: 0.06466 \n",
      "[ 84/100] train_loss: 0.07495 valid_loss: 0.09977 test_loss: 0.06824 \n",
      "[ 85/100] train_loss: 0.06861 valid_loss: 0.08385 test_loss: 0.06398 \n",
      "[ 86/100] train_loss: 0.06567 valid_loss: 0.07693 test_loss: 0.06528 \n",
      "Validation loss decreased (0.082075 --> 0.076928).  Saving model ...\n",
      "[ 87/100] train_loss: 0.06841 valid_loss: 0.09891 test_loss: 0.06688 \n",
      "[ 88/100] train_loss: 0.06734 valid_loss: 0.09125 test_loss: 0.06452 \n",
      "[ 89/100] train_loss: 0.07089 valid_loss: 0.08378 test_loss: 0.06266 \n",
      "[ 90/100] train_loss: 0.06947 valid_loss: 0.08977 test_loss: 0.06382 \n",
      "[ 91/100] train_loss: 0.06444 valid_loss: 0.08287 test_loss: 0.06279 \n",
      "[ 92/100] train_loss: 0.06905 valid_loss: 0.08484 test_loss: 0.06308 \n",
      "[ 93/100] train_loss: 0.06193 valid_loss: 0.09150 test_loss: 0.06430 \n",
      "[ 94/100] train_loss: 0.06838 valid_loss: 0.08467 test_loss: 0.06263 \n",
      "[ 95/100] train_loss: 0.06562 valid_loss: 0.08494 test_loss: 0.06254 \n",
      "[ 96/100] train_loss: 0.07356 valid_loss: 0.09345 test_loss: 0.06449 \n",
      "[ 97/100] train_loss: 0.06865 valid_loss: 0.09203 test_loss: 0.06392 \n",
      "[ 98/100] train_loss: 0.06647 valid_loss: 0.09281 test_loss: 0.06406 \n",
      "[ 99/100] train_loss: 0.06654 valid_loss: 0.08275 test_loss: 0.06215 \n",
      "[100/100] train_loss: 0.06474 valid_loss: 0.08275 test_loss: 0.06170 \n",
      "TRAINING MODEL 12\n",
      "[  1/100] train_loss: 0.18728 valid_loss: 0.20083 test_loss: 0.18587 \n",
      "Validation loss decreased (inf --> 0.200832).  Saving model ...\n",
      "[  2/100] train_loss: 0.18261 valid_loss: 0.19936 test_loss: 0.18451 \n",
      "Validation loss decreased (0.200832 --> 0.199362).  Saving model ...\n",
      "[  3/100] train_loss: 0.17913 valid_loss: 0.19766 test_loss: 0.18249 \n",
      "Validation loss decreased (0.199362 --> 0.197662).  Saving model ...\n",
      "[  4/100] train_loss: 0.18130 valid_loss: 0.19570 test_loss: 0.17962 \n",
      "Validation loss decreased (0.197662 --> 0.195700).  Saving model ...\n",
      "[  5/100] train_loss: 0.17725 valid_loss: 0.19285 test_loss: 0.17649 \n",
      "Validation loss decreased (0.195700 --> 0.192849).  Saving model ...\n",
      "[  6/100] train_loss: 0.17467 valid_loss: 0.19001 test_loss: 0.17306 \n",
      "Validation loss decreased (0.192849 --> 0.190011).  Saving model ...\n",
      "[  7/100] train_loss: 0.17119 valid_loss: 0.18754 test_loss: 0.16916 \n",
      "Validation loss decreased (0.190011 --> 0.187542).  Saving model ...\n",
      "[  8/100] train_loss: 0.16931 valid_loss: 0.18429 test_loss: 0.16571 \n",
      "Validation loss decreased (0.187542 --> 0.184292).  Saving model ...\n",
      "[  9/100] train_loss: 0.16527 valid_loss: 0.18139 test_loss: 0.16228 \n",
      "Validation loss decreased (0.184292 --> 0.181389).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16240 valid_loss: 0.17830 test_loss: 0.15873 \n",
      "Validation loss decreased (0.181389 --> 0.178303).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16271 valid_loss: 0.17370 test_loss: 0.15460 \n",
      "Validation loss decreased (0.178303 --> 0.173702).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15446 valid_loss: 0.16966 test_loss: 0.15118 \n",
      "Validation loss decreased (0.173702 --> 0.169658).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14970 valid_loss: 0.16497 test_loss: 0.14703 \n",
      "Validation loss decreased (0.169658 --> 0.164967).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14405 valid_loss: 0.16066 test_loss: 0.14253 \n",
      "Validation loss decreased (0.164967 --> 0.160655).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14185 valid_loss: 0.15790 test_loss: 0.13851 \n",
      "Validation loss decreased (0.160655 --> 0.157903).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14171 valid_loss: 0.15090 test_loss: 0.13389 \n",
      "Validation loss decreased (0.157903 --> 0.150903).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13629 valid_loss: 0.14720 test_loss: 0.13055 \n",
      "Validation loss decreased (0.150903 --> 0.147197).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13186 valid_loss: 0.14730 test_loss: 0.12676 \n",
      "[ 19/100] train_loss: 0.12771 valid_loss: 0.14013 test_loss: 0.12259 \n",
      "Validation loss decreased (0.147197 --> 0.140128).  Saving model ...\n",
      "[ 20/100] train_loss: 0.11982 valid_loss: 0.13483 test_loss: 0.11875 \n",
      "Validation loss decreased (0.140128 --> 0.134829).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12402 valid_loss: 0.13025 test_loss: 0.11562 \n",
      "Validation loss decreased (0.134829 --> 0.130252).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11591 valid_loss: 0.13022 test_loss: 0.11280 \n",
      "Validation loss decreased (0.130252 --> 0.130221).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11029 valid_loss: 0.12226 test_loss: 0.10860 \n",
      "Validation loss decreased (0.130221 --> 0.122261).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11131 valid_loss: 0.12286 test_loss: 0.10612 \n",
      "[ 25/100] train_loss: 0.10618 valid_loss: 0.11890 test_loss: 0.10261 \n",
      "Validation loss decreased (0.122261 --> 0.118902).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10774 valid_loss: 0.11096 test_loss: 0.09949 \n",
      "Validation loss decreased (0.118902 --> 0.110963).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10635 valid_loss: 0.11533 test_loss: 0.09810 \n",
      "[ 28/100] train_loss: 0.10557 valid_loss: 0.11891 test_loss: 0.09773 \n",
      "[ 29/100] train_loss: 0.09499 valid_loss: 0.10349 test_loss: 0.09325 \n",
      "Validation loss decreased (0.110963 --> 0.103493).  Saving model ...\n",
      "[ 30/100] train_loss: 0.09746 valid_loss: 0.10165 test_loss: 0.09205 \n",
      "Validation loss decreased (0.103493 --> 0.101653).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09955 valid_loss: 0.11025 test_loss: 0.09123 \n",
      "[ 32/100] train_loss: 0.10070 valid_loss: 0.10718 test_loss: 0.08933 \n",
      "[ 33/100] train_loss: 0.09021 valid_loss: 0.10287 test_loss: 0.08710 \n",
      "[ 34/100] train_loss: 0.09066 valid_loss: 0.10622 test_loss: 0.08644 \n",
      "[ 35/100] train_loss: 0.08557 valid_loss: 0.09654 test_loss: 0.08280 \n",
      "Validation loss decreased (0.101653 --> 0.096536).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09377 valid_loss: 0.09679 test_loss: 0.08220 \n",
      "[ 37/100] train_loss: 0.08860 valid_loss: 0.09552 test_loss: 0.08205 \n",
      "Validation loss decreased (0.096536 --> 0.095520).  Saving model ...\n",
      "[ 38/100] train_loss: 0.07990 valid_loss: 0.09959 test_loss: 0.08097 \n",
      "[ 39/100] train_loss: 0.08314 valid_loss: 0.09665 test_loss: 0.07913 \n",
      "[ 40/100] train_loss: 0.08686 valid_loss: 0.09273 test_loss: 0.07769 \n",
      "Validation loss decreased (0.095520 --> 0.092732).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08202 valid_loss: 0.08926 test_loss: 0.07702 \n",
      "Validation loss decreased (0.092732 --> 0.089256).  Saving model ...\n",
      "[ 42/100] train_loss: 0.08597 valid_loss: 0.09282 test_loss: 0.07602 \n",
      "[ 43/100] train_loss: 0.07961 valid_loss: 0.09320 test_loss: 0.07553 \n",
      "[ 44/100] train_loss: 0.08327 valid_loss: 0.08735 test_loss: 0.07503 \n",
      "Validation loss decreased (0.089256 --> 0.087350).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08238 valid_loss: 0.08913 test_loss: 0.07445 \n",
      "[ 46/100] train_loss: 0.07977 valid_loss: 0.09032 test_loss: 0.07359 \n",
      "[ 47/100] train_loss: 0.08122 valid_loss: 0.09131 test_loss: 0.07311 \n",
      "[ 48/100] train_loss: 0.08056 valid_loss: 0.08937 test_loss: 0.07219 \n",
      "[ 49/100] train_loss: 0.08729 valid_loss: 0.08892 test_loss: 0.07191 \n",
      "[ 50/100] train_loss: 0.07231 valid_loss: 0.08688 test_loss: 0.07148 \n",
      "Validation loss decreased (0.087350 --> 0.086880).  Saving model ...\n",
      "[ 51/100] train_loss: 0.07818 valid_loss: 0.08771 test_loss: 0.07102 \n",
      "[ 52/100] train_loss: 0.08257 valid_loss: 0.08957 test_loss: 0.07107 \n",
      "[ 53/100] train_loss: 0.07801 valid_loss: 0.09142 test_loss: 0.07112 \n",
      "[ 54/100] train_loss: 0.07990 valid_loss: 0.09545 test_loss: 0.07196 \n",
      "[ 55/100] train_loss: 0.07517 valid_loss: 0.08574 test_loss: 0.06936 \n",
      "Validation loss decreased (0.086880 --> 0.085739).  Saving model ...\n",
      "[ 56/100] train_loss: 0.07896 valid_loss: 0.08780 test_loss: 0.06936 \n",
      "[ 57/100] train_loss: 0.07227 valid_loss: 0.09163 test_loss: 0.06951 \n",
      "[ 58/100] train_loss: 0.07660 valid_loss: 0.08617 test_loss: 0.06824 \n",
      "[ 59/100] train_loss: 0.06879 valid_loss: 0.09067 test_loss: 0.06857 \n",
      "[ 60/100] train_loss: 0.07378 valid_loss: 0.08397 test_loss: 0.06732 \n",
      "Validation loss decreased (0.085739 --> 0.083970).  Saving model ...\n",
      "[ 61/100] train_loss: 0.07491 valid_loss: 0.08291 test_loss: 0.06751 \n",
      "Validation loss decreased (0.083970 --> 0.082912).  Saving model ...\n",
      "[ 62/100] train_loss: 0.07269 valid_loss: 0.09546 test_loss: 0.06971 \n",
      "[ 63/100] train_loss: 0.08119 valid_loss: 0.08989 test_loss: 0.06839 \n",
      "[ 64/100] train_loss: 0.07080 valid_loss: 0.08187 test_loss: 0.06892 \n",
      "Validation loss decreased (0.082912 --> 0.081867).  Saving model ...\n",
      "[ 65/100] train_loss: 0.07198 valid_loss: 0.08803 test_loss: 0.06706 \n",
      "[ 66/100] train_loss: 0.07378 valid_loss: 0.08971 test_loss: 0.06694 \n",
      "[ 67/100] train_loss: 0.07299 valid_loss: 0.09248 test_loss: 0.06740 \n",
      "[ 68/100] train_loss: 0.07154 valid_loss: 0.08697 test_loss: 0.06620 \n",
      "[ 69/100] train_loss: 0.07618 valid_loss: 0.08843 test_loss: 0.06616 \n",
      "[ 70/100] train_loss: 0.07428 valid_loss: 0.09400 test_loss: 0.06735 \n",
      "[ 71/100] train_loss: 0.06920 valid_loss: 0.08364 test_loss: 0.06544 \n",
      "[ 72/100] train_loss: 0.07936 valid_loss: 0.08483 test_loss: 0.06573 \n",
      "[ 73/100] train_loss: 0.07695 valid_loss: 0.09550 test_loss: 0.06720 \n",
      "[ 74/100] train_loss: 0.07226 valid_loss: 0.08591 test_loss: 0.06495 \n",
      "[ 75/100] train_loss: 0.06812 valid_loss: 0.08932 test_loss: 0.06494 \n",
      "[ 76/100] train_loss: 0.07414 valid_loss: 0.08584 test_loss: 0.06453 \n",
      "[ 77/100] train_loss: 0.06843 valid_loss: 0.09598 test_loss: 0.06663 \n",
      "[ 78/100] train_loss: 0.07027 valid_loss: 0.08710 test_loss: 0.06465 \n",
      "[ 79/100] train_loss: 0.06783 valid_loss: 0.08535 test_loss: 0.06448 \n",
      "[ 80/100] train_loss: 0.06767 valid_loss: 0.08807 test_loss: 0.06481 \n",
      "[ 81/100] train_loss: 0.07292 valid_loss: 0.08489 test_loss: 0.06443 \n",
      "[ 82/100] train_loss: 0.07072 valid_loss: 0.09056 test_loss: 0.06500 \n",
      "[ 83/100] train_loss: 0.06775 valid_loss: 0.08695 test_loss: 0.06323 \n",
      "[ 84/100] train_loss: 0.06854 valid_loss: 0.09087 test_loss: 0.06360 \n",
      "[ 85/100] train_loss: 0.06988 valid_loss: 0.08573 test_loss: 0.06223 \n",
      "[ 86/100] train_loss: 0.06591 valid_loss: 0.08339 test_loss: 0.06219 \n",
      "[ 87/100] train_loss: 0.06498 valid_loss: 0.08621 test_loss: 0.06261 \n",
      "[ 88/100] train_loss: 0.07175 valid_loss: 0.09155 test_loss: 0.06376 \n",
      "[ 89/100] train_loss: 0.06454 valid_loss: 0.08923 test_loss: 0.06293 \n",
      "[ 90/100] train_loss: 0.07015 valid_loss: 0.08033 test_loss: 0.06173 \n",
      "Validation loss decreased (0.081867 --> 0.080333).  Saving model ...\n",
      "[ 91/100] train_loss: 0.06447 valid_loss: 0.09494 test_loss: 0.06401 \n",
      "[ 92/100] train_loss: 0.06629 valid_loss: 0.09042 test_loss: 0.06237 \n",
      "[ 93/100] train_loss: 0.06767 valid_loss: 0.08267 test_loss: 0.06121 \n",
      "[ 94/100] train_loss: 0.06590 valid_loss: 0.09520 test_loss: 0.06384 \n",
      "[ 95/100] train_loss: 0.06987 valid_loss: 0.09078 test_loss: 0.06256 \n",
      "[ 96/100] train_loss: 0.07301 valid_loss: 0.08406 test_loss: 0.06133 \n",
      "[ 97/100] train_loss: 0.06961 valid_loss: 0.08083 test_loss: 0.06176 \n",
      "[ 98/100] train_loss: 0.06308 valid_loss: 0.09431 test_loss: 0.06317 \n",
      "[ 99/100] train_loss: 0.06928 valid_loss: 0.09145 test_loss: 0.06206 \n",
      "[100/100] train_loss: 0.07049 valid_loss: 0.07842 test_loss: 0.06060 \n",
      "Validation loss decreased (0.080333 --> 0.078417).  Saving model ...\n",
      "TRAINING MODEL 13\n",
      "[  1/100] train_loss: 0.18370 valid_loss: 0.19392 test_loss: 0.18554 \n",
      "Validation loss decreased (inf --> 0.193918).  Saving model ...\n",
      "[  2/100] train_loss: 0.18281 valid_loss: 0.19246 test_loss: 0.18388 \n",
      "Validation loss decreased (0.193918 --> 0.192460).  Saving model ...\n",
      "[  3/100] train_loss: 0.17935 valid_loss: 0.19049 test_loss: 0.18146 \n",
      "Validation loss decreased (0.192460 --> 0.190493).  Saving model ...\n",
      "[  4/100] train_loss: 0.17753 valid_loss: 0.18804 test_loss: 0.17798 \n",
      "Validation loss decreased (0.190493 --> 0.188044).  Saving model ...\n",
      "[  5/100] train_loss: 0.17464 valid_loss: 0.18509 test_loss: 0.17376 \n",
      "Validation loss decreased (0.188044 --> 0.185090).  Saving model ...\n",
      "[  6/100] train_loss: 0.16987 valid_loss: 0.18280 test_loss: 0.16970 \n",
      "Validation loss decreased (0.185090 --> 0.182795).  Saving model ...\n",
      "[  7/100] train_loss: 0.16912 valid_loss: 0.18005 test_loss: 0.16573 \n",
      "Validation loss decreased (0.182795 --> 0.180052).  Saving model ...\n",
      "[  8/100] train_loss: 0.16554 valid_loss: 0.17702 test_loss: 0.16245 \n",
      "Validation loss decreased (0.180052 --> 0.177021).  Saving model ...\n",
      "[  9/100] train_loss: 0.15935 valid_loss: 0.17479 test_loss: 0.15940 \n",
      "Validation loss decreased (0.177021 --> 0.174788).  Saving model ...\n",
      "[ 10/100] train_loss: 0.15668 valid_loss: 0.17268 test_loss: 0.15624 \n",
      "Validation loss decreased (0.174788 --> 0.172678).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15787 valid_loss: 0.16741 test_loss: 0.15239 \n",
      "Validation loss decreased (0.172678 --> 0.167408).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15047 valid_loss: 0.16446 test_loss: 0.14912 \n",
      "Validation loss decreased (0.167408 --> 0.164458).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14708 valid_loss: 0.16290 test_loss: 0.14509 \n",
      "Validation loss decreased (0.164458 --> 0.162896).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14742 valid_loss: 0.15960 test_loss: 0.14138 \n",
      "Validation loss decreased (0.162896 --> 0.159603).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14292 valid_loss: 0.15454 test_loss: 0.13826 \n",
      "Validation loss decreased (0.159603 --> 0.154535).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14051 valid_loss: 0.15246 test_loss: 0.13405 \n",
      "Validation loss decreased (0.154535 --> 0.152456).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13763 valid_loss: 0.14952 test_loss: 0.13128 \n",
      "Validation loss decreased (0.152456 --> 0.149525).  Saving model ...\n",
      "[ 18/100] train_loss: 0.12806 valid_loss: 0.14587 test_loss: 0.12869 \n",
      "Validation loss decreased (0.149525 --> 0.145873).  Saving model ...\n",
      "[ 19/100] train_loss: 0.12801 valid_loss: 0.14188 test_loss: 0.12470 \n",
      "Validation loss decreased (0.145873 --> 0.141884).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12783 valid_loss: 0.13629 test_loss: 0.12105 \n",
      "Validation loss decreased (0.141884 --> 0.136292).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12178 valid_loss: 0.13483 test_loss: 0.11814 \n",
      "Validation loss decreased (0.136292 --> 0.134833).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11799 valid_loss: 0.13322 test_loss: 0.11515 \n",
      "Validation loss decreased (0.134833 --> 0.133222).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11576 valid_loss: 0.13124 test_loss: 0.11243 \n",
      "Validation loss decreased (0.133222 --> 0.131236).  Saving model ...\n",
      "[ 24/100] train_loss: 0.10968 valid_loss: 0.12140 test_loss: 0.10943 \n",
      "Validation loss decreased (0.131236 --> 0.121399).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11184 valid_loss: 0.11998 test_loss: 0.10631 \n",
      "Validation loss decreased (0.121399 --> 0.119983).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10581 valid_loss: 0.11823 test_loss: 0.10350 \n",
      "Validation loss decreased (0.119983 --> 0.118230).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11031 valid_loss: 0.11735 test_loss: 0.10138 \n",
      "Validation loss decreased (0.118230 --> 0.117352).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10961 valid_loss: 0.12173 test_loss: 0.10063 \n",
      "[ 29/100] train_loss: 0.10461 valid_loss: 0.11291 test_loss: 0.09692 \n",
      "Validation loss decreased (0.117352 --> 0.112915).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10031 valid_loss: 0.10910 test_loss: 0.09472 \n",
      "Validation loss decreased (0.112915 --> 0.109098).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09833 valid_loss: 0.10938 test_loss: 0.09268 \n",
      "[ 32/100] train_loss: 0.09125 valid_loss: 0.11174 test_loss: 0.09209 \n",
      "[ 33/100] train_loss: 0.09401 valid_loss: 0.10600 test_loss: 0.08940 \n",
      "Validation loss decreased (0.109098 --> 0.105998).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09597 valid_loss: 0.10035 test_loss: 0.08808 \n",
      "Validation loss decreased (0.105998 --> 0.100345).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09893 valid_loss: 0.10359 test_loss: 0.08665 \n",
      "[ 36/100] train_loss: 0.08835 valid_loss: 0.10049 test_loss: 0.08517 \n",
      "[ 37/100] train_loss: 0.08751 valid_loss: 0.09546 test_loss: 0.08390 \n",
      "Validation loss decreased (0.100345 --> 0.095463).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08721 valid_loss: 0.09279 test_loss: 0.08345 \n",
      "Validation loss decreased (0.095463 --> 0.092790).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08390 valid_loss: 0.10737 test_loss: 0.08426 \n",
      "[ 40/100] train_loss: 0.08730 valid_loss: 0.10023 test_loss: 0.08121 \n",
      "[ 41/100] train_loss: 0.08686 valid_loss: 0.08923 test_loss: 0.07991 \n",
      "Validation loss decreased (0.092790 --> 0.089228).  Saving model ...\n",
      "[ 42/100] train_loss: 0.07993 valid_loss: 0.08979 test_loss: 0.07930 \n",
      "[ 43/100] train_loss: 0.08469 valid_loss: 0.09322 test_loss: 0.07755 \n",
      "[ 44/100] train_loss: 0.08030 valid_loss: 0.09819 test_loss: 0.07795 \n",
      "[ 45/100] train_loss: 0.08905 valid_loss: 0.08579 test_loss: 0.07737 \n",
      "Validation loss decreased (0.089228 --> 0.085789).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08161 valid_loss: 0.09229 test_loss: 0.07549 \n",
      "[ 47/100] train_loss: 0.08243 valid_loss: 0.09202 test_loss: 0.07490 \n",
      "[ 48/100] train_loss: 0.09043 valid_loss: 0.08657 test_loss: 0.07522 \n",
      "[ 49/100] train_loss: 0.08041 valid_loss: 0.09093 test_loss: 0.07387 \n",
      "[ 50/100] train_loss: 0.08190 valid_loss: 0.09668 test_loss: 0.07478 \n",
      "[ 51/100] train_loss: 0.08293 valid_loss: 0.09393 test_loss: 0.07382 \n",
      "[ 52/100] train_loss: 0.07855 valid_loss: 0.08519 test_loss: 0.07285 \n",
      "Validation loss decreased (0.085789 --> 0.085190).  Saving model ...\n",
      "[ 53/100] train_loss: 0.08093 valid_loss: 0.08671 test_loss: 0.07199 \n",
      "[ 54/100] train_loss: 0.08071 valid_loss: 0.09033 test_loss: 0.07190 \n",
      "[ 55/100] train_loss: 0.07501 valid_loss: 0.08660 test_loss: 0.07123 \n",
      "[ 56/100] train_loss: 0.08106 valid_loss: 0.08845 test_loss: 0.07051 \n",
      "[ 57/100] train_loss: 0.08277 valid_loss: 0.08617 test_loss: 0.07032 \n",
      "[ 58/100] train_loss: 0.08396 valid_loss: 0.09396 test_loss: 0.07138 \n",
      "[ 59/100] train_loss: 0.07520 valid_loss: 0.08847 test_loss: 0.07030 \n",
      "[ 60/100] train_loss: 0.07409 valid_loss: 0.08892 test_loss: 0.06995 \n",
      "[ 61/100] train_loss: 0.08062 valid_loss: 0.09004 test_loss: 0.06964 \n",
      "[ 62/100] train_loss: 0.07281 valid_loss: 0.08882 test_loss: 0.06903 \n",
      "[ 63/100] train_loss: 0.08117 valid_loss: 0.09054 test_loss: 0.06907 \n",
      "[ 64/100] train_loss: 0.07193 valid_loss: 0.08604 test_loss: 0.06826 \n",
      "[ 65/100] train_loss: 0.07563 valid_loss: 0.08675 test_loss: 0.06841 \n",
      "[ 66/100] train_loss: 0.07527 valid_loss: 0.09540 test_loss: 0.06963 \n",
      "[ 67/100] train_loss: 0.07312 valid_loss: 0.08573 test_loss: 0.06734 \n",
      "[ 68/100] train_loss: 0.07409 valid_loss: 0.08646 test_loss: 0.06795 \n",
      "[ 69/100] train_loss: 0.07378 valid_loss: 0.09606 test_loss: 0.06912 \n",
      "[ 70/100] train_loss: 0.07239 valid_loss: 0.08587 test_loss: 0.06726 \n",
      "[ 71/100] train_loss: 0.07438 valid_loss: 0.08997 test_loss: 0.06708 \n",
      "[ 72/100] train_loss: 0.07295 valid_loss: 0.09434 test_loss: 0.06812 \n",
      "[ 73/100] train_loss: 0.07184 valid_loss: 0.08227 test_loss: 0.06613 \n",
      "Validation loss decreased (0.085190 --> 0.082274).  Saving model ...\n",
      "[ 74/100] train_loss: 0.07231 valid_loss: 0.08292 test_loss: 0.06619 \n",
      "[ 75/100] train_loss: 0.07234 valid_loss: 0.09488 test_loss: 0.06775 \n",
      "[ 76/100] train_loss: 0.07270 valid_loss: 0.08900 test_loss: 0.06542 \n",
      "[ 77/100] train_loss: 0.07526 valid_loss: 0.08906 test_loss: 0.06556 \n",
      "[ 78/100] train_loss: 0.07634 valid_loss: 0.08822 test_loss: 0.06634 \n",
      "[ 79/100] train_loss: 0.06740 valid_loss: 0.08471 test_loss: 0.06673 \n",
      "[ 80/100] train_loss: 0.06966 valid_loss: 0.09180 test_loss: 0.06612 \n",
      "[ 81/100] train_loss: 0.07794 valid_loss: 0.09082 test_loss: 0.06570 \n",
      "[ 82/100] train_loss: 0.06632 valid_loss: 0.08470 test_loss: 0.06461 \n",
      "[ 83/100] train_loss: 0.06890 valid_loss: 0.08291 test_loss: 0.06437 \n",
      "[ 84/100] train_loss: 0.06887 valid_loss: 0.09084 test_loss: 0.06512 \n",
      "[ 85/100] train_loss: 0.07254 valid_loss: 0.08861 test_loss: 0.06547 \n",
      "[ 86/100] train_loss: 0.07521 valid_loss: 0.08868 test_loss: 0.06489 \n",
      "[ 87/100] train_loss: 0.06799 valid_loss: 0.09177 test_loss: 0.06489 \n",
      "[ 88/100] train_loss: 0.07079 valid_loss: 0.08344 test_loss: 0.06303 \n",
      "[ 89/100] train_loss: 0.07096 valid_loss: 0.07996 test_loss: 0.06330 \n",
      "Validation loss decreased (0.082274 --> 0.079955).  Saving model ...\n",
      "[ 90/100] train_loss: 0.06997 valid_loss: 0.09369 test_loss: 0.06440 \n",
      "[ 91/100] train_loss: 0.07662 valid_loss: 0.08731 test_loss: 0.06338 \n",
      "[ 92/100] train_loss: 0.06694 valid_loss: 0.08912 test_loss: 0.06363 \n",
      "[ 93/100] train_loss: 0.06616 valid_loss: 0.08964 test_loss: 0.06314 \n",
      "[ 94/100] train_loss: 0.07714 valid_loss: 0.08793 test_loss: 0.06285 \n",
      "[ 95/100] train_loss: 0.06531 valid_loss: 0.08249 test_loss: 0.06240 \n",
      "[ 96/100] train_loss: 0.07098 valid_loss: 0.08378 test_loss: 0.06242 \n",
      "[ 97/100] train_loss: 0.06783 valid_loss: 0.09339 test_loss: 0.06402 \n",
      "[ 98/100] train_loss: 0.06974 valid_loss: 0.08678 test_loss: 0.06244 \n",
      "[ 99/100] train_loss: 0.06825 valid_loss: 0.08991 test_loss: 0.06255 \n",
      "[100/100] train_loss: 0.06636 valid_loss: 0.08484 test_loss: 0.06158 \n",
      "TRAINING MODEL 14\n",
      "[  1/100] train_loss: 0.18868 valid_loss: 0.19281 test_loss: 0.18565 \n",
      "Validation loss decreased (inf --> 0.192809).  Saving model ...\n",
      "[  2/100] train_loss: 0.18461 valid_loss: 0.19201 test_loss: 0.18487 \n",
      "Validation loss decreased (0.192809 --> 0.192011).  Saving model ...\n",
      "[  3/100] train_loss: 0.18377 valid_loss: 0.19121 test_loss: 0.18376 \n",
      "Validation loss decreased (0.192011 --> 0.191212).  Saving model ...\n",
      "[  4/100] train_loss: 0.18088 valid_loss: 0.18994 test_loss: 0.18209 \n",
      "Validation loss decreased (0.191212 --> 0.189935).  Saving model ...\n",
      "[  5/100] train_loss: 0.17998 valid_loss: 0.18847 test_loss: 0.17975 \n",
      "Validation loss decreased (0.189935 --> 0.188469).  Saving model ...\n",
      "[  6/100] train_loss: 0.17855 valid_loss: 0.18625 test_loss: 0.17709 \n",
      "Validation loss decreased (0.188469 --> 0.186254).  Saving model ...\n",
      "[  7/100] train_loss: 0.17527 valid_loss: 0.18373 test_loss: 0.17433 \n",
      "Validation loss decreased (0.186254 --> 0.183730).  Saving model ...\n",
      "[  8/100] train_loss: 0.17523 valid_loss: 0.18135 test_loss: 0.17130 \n",
      "Validation loss decreased (0.183730 --> 0.181353).  Saving model ...\n",
      "[  9/100] train_loss: 0.17008 valid_loss: 0.17885 test_loss: 0.16819 \n",
      "Validation loss decreased (0.181353 --> 0.178848).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16887 valid_loss: 0.17624 test_loss: 0.16472 \n",
      "Validation loss decreased (0.178848 --> 0.176242).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16525 valid_loss: 0.17366 test_loss: 0.16132 \n",
      "Validation loss decreased (0.176242 --> 0.173660).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16242 valid_loss: 0.16928 test_loss: 0.15736 \n",
      "Validation loss decreased (0.173660 --> 0.169276).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15749 valid_loss: 0.16615 test_loss: 0.15334 \n",
      "Validation loss decreased (0.169276 --> 0.166153).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15298 valid_loss: 0.16311 test_loss: 0.14926 \n",
      "Validation loss decreased (0.166153 --> 0.163106).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14798 valid_loss: 0.15889 test_loss: 0.14504 \n",
      "Validation loss decreased (0.163106 --> 0.158885).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14666 valid_loss: 0.15206 test_loss: 0.14032 \n",
      "Validation loss decreased (0.158885 --> 0.152056).  Saving model ...\n",
      "[ 17/100] train_loss: 0.14064 valid_loss: 0.14715 test_loss: 0.13626 \n",
      "Validation loss decreased (0.152056 --> 0.147151).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13485 valid_loss: 0.15068 test_loss: 0.13411 \n",
      "[ 19/100] train_loss: 0.13649 valid_loss: 0.14248 test_loss: 0.12881 \n",
      "Validation loss decreased (0.147151 --> 0.142475).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13180 valid_loss: 0.13518 test_loss: 0.12485 \n",
      "Validation loss decreased (0.142475 --> 0.135180).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12757 valid_loss: 0.13810 test_loss: 0.12214 \n",
      "[ 22/100] train_loss: 0.12769 valid_loss: 0.12697 test_loss: 0.11665 \n",
      "Validation loss decreased (0.135180 --> 0.126974).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11852 valid_loss: 0.12326 test_loss: 0.11365 \n",
      "Validation loss decreased (0.126974 --> 0.123264).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11149 valid_loss: 0.12616 test_loss: 0.11199 \n",
      "[ 25/100] train_loss: 0.11265 valid_loss: 0.11987 test_loss: 0.10793 \n",
      "Validation loss decreased (0.123264 --> 0.119866).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10630 valid_loss: 0.11291 test_loss: 0.10431 \n",
      "Validation loss decreased (0.119866 --> 0.112907).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10639 valid_loss: 0.11233 test_loss: 0.10156 \n",
      "Validation loss decreased (0.112907 --> 0.112335).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10782 valid_loss: 0.10705 test_loss: 0.09882 \n",
      "Validation loss decreased (0.112335 --> 0.107054).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10023 valid_loss: 0.10405 test_loss: 0.09748 \n",
      "Validation loss decreased (0.107054 --> 0.104048).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10055 valid_loss: 0.11052 test_loss: 0.09636 \n",
      "[ 31/100] train_loss: 0.09433 valid_loss: 0.10560 test_loss: 0.09309 \n",
      "[ 32/100] train_loss: 0.10035 valid_loss: 0.09649 test_loss: 0.09088 \n",
      "Validation loss decreased (0.104048 --> 0.096487).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09123 valid_loss: 0.09775 test_loss: 0.08885 \n",
      "[ 34/100] train_loss: 0.09143 valid_loss: 0.10282 test_loss: 0.08835 \n",
      "[ 35/100] train_loss: 0.08937 valid_loss: 0.09320 test_loss: 0.08532 \n",
      "Validation loss decreased (0.096487 --> 0.093202).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09023 valid_loss: 0.09294 test_loss: 0.08402 \n",
      "Validation loss decreased (0.093202 --> 0.092940).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09059 valid_loss: 0.09519 test_loss: 0.08333 \n",
      "[ 38/100] train_loss: 0.08899 valid_loss: 0.09236 test_loss: 0.08169 \n",
      "Validation loss decreased (0.092940 --> 0.092358).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08944 valid_loss: 0.08999 test_loss: 0.08041 \n",
      "Validation loss decreased (0.092358 --> 0.089993).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08755 valid_loss: 0.09759 test_loss: 0.08151 \n",
      "[ 41/100] train_loss: 0.08674 valid_loss: 0.08766 test_loss: 0.07840 \n",
      "Validation loss decreased (0.089993 --> 0.087659).  Saving model ...\n",
      "[ 42/100] train_loss: 0.08289 valid_loss: 0.08610 test_loss: 0.07806 \n",
      "Validation loss decreased (0.087659 --> 0.086097).  Saving model ...\n",
      "[ 43/100] train_loss: 0.08871 valid_loss: 0.08878 test_loss: 0.07686 \n",
      "[ 44/100] train_loss: 0.08432 valid_loss: 0.08697 test_loss: 0.07673 \n",
      "[ 45/100] train_loss: 0.08220 valid_loss: 0.08993 test_loss: 0.07590 \n",
      "[ 46/100] train_loss: 0.08197 valid_loss: 0.09231 test_loss: 0.07638 \n",
      "[ 47/100] train_loss: 0.08024 valid_loss: 0.08448 test_loss: 0.07416 \n",
      "Validation loss decreased (0.086097 --> 0.084483).  Saving model ...\n",
      "[ 48/100] train_loss: 0.08693 valid_loss: 0.08395 test_loss: 0.07394 \n",
      "Validation loss decreased (0.084483 --> 0.083945).  Saving model ...\n",
      "[ 49/100] train_loss: 0.07600 valid_loss: 0.08530 test_loss: 0.07321 \n",
      "[ 50/100] train_loss: 0.08822 valid_loss: 0.08888 test_loss: 0.07332 \n",
      "[ 51/100] train_loss: 0.08242 valid_loss: 0.08965 test_loss: 0.07318 \n",
      "[ 52/100] train_loss: 0.07980 valid_loss: 0.08269 test_loss: 0.07245 \n",
      "Validation loss decreased (0.083945 --> 0.082691).  Saving model ...\n",
      "[ 53/100] train_loss: 0.07924 valid_loss: 0.08752 test_loss: 0.07213 \n",
      "[ 54/100] train_loss: 0.07521 valid_loss: 0.08349 test_loss: 0.07136 \n",
      "[ 55/100] train_loss: 0.07681 valid_loss: 0.08167 test_loss: 0.07170 \n",
      "Validation loss decreased (0.082691 --> 0.081673).  Saving model ...\n",
      "[ 56/100] train_loss: 0.07847 valid_loss: 0.08483 test_loss: 0.07012 \n",
      "[ 57/100] train_loss: 0.07634 valid_loss: 0.09201 test_loss: 0.07222 \n",
      "[ 58/100] train_loss: 0.07330 valid_loss: 0.08419 test_loss: 0.06975 \n",
      "[ 59/100] train_loss: 0.07433 valid_loss: 0.08165 test_loss: 0.06926 \n",
      "Validation loss decreased (0.081673 --> 0.081650).  Saving model ...\n",
      "[ 60/100] train_loss: 0.07922 valid_loss: 0.08593 test_loss: 0.06960 \n",
      "[ 61/100] train_loss: 0.08049 valid_loss: 0.08207 test_loss: 0.06863 \n",
      "[ 62/100] train_loss: 0.07761 valid_loss: 0.08332 test_loss: 0.06849 \n",
      "[ 63/100] train_loss: 0.07837 valid_loss: 0.08570 test_loss: 0.06834 \n",
      "[ 64/100] train_loss: 0.08234 valid_loss: 0.08282 test_loss: 0.06770 \n",
      "[ 65/100] train_loss: 0.07350 valid_loss: 0.08196 test_loss: 0.06748 \n",
      "[ 66/100] train_loss: 0.07767 valid_loss: 0.08867 test_loss: 0.06846 \n",
      "[ 67/100] train_loss: 0.07662 valid_loss: 0.09068 test_loss: 0.06891 \n",
      "[ 68/100] train_loss: 0.07530 valid_loss: 0.08070 test_loss: 0.06618 \n",
      "Validation loss decreased (0.081650 --> 0.080702).  Saving model ...\n",
      "[ 69/100] train_loss: 0.07653 valid_loss: 0.08052 test_loss: 0.06691 \n",
      "Validation loss decreased (0.080702 --> 0.080515).  Saving model ...\n",
      "[ 70/100] train_loss: 0.07347 valid_loss: 0.08837 test_loss: 0.06820 \n",
      "[ 71/100] train_loss: 0.07288 valid_loss: 0.08321 test_loss: 0.06689 \n",
      "[ 72/100] train_loss: 0.07207 valid_loss: 0.07900 test_loss: 0.06614 \n",
      "Validation loss decreased (0.080515 --> 0.078998).  Saving model ...\n",
      "[ 73/100] train_loss: 0.06973 valid_loss: 0.08673 test_loss: 0.06639 \n",
      "[ 74/100] train_loss: 0.07469 valid_loss: 0.08509 test_loss: 0.06560 \n",
      "[ 75/100] train_loss: 0.07057 valid_loss: 0.08437 test_loss: 0.06555 \n",
      "[ 76/100] train_loss: 0.06418 valid_loss: 0.08265 test_loss: 0.06487 \n",
      "[ 77/100] train_loss: 0.06909 valid_loss: 0.07677 test_loss: 0.06440 \n",
      "Validation loss decreased (0.078998 --> 0.076770).  Saving model ...\n",
      "[ 78/100] train_loss: 0.06891 valid_loss: 0.08332 test_loss: 0.06455 \n",
      "[ 79/100] train_loss: 0.07189 valid_loss: 0.08991 test_loss: 0.06622 \n",
      "[ 80/100] train_loss: 0.06713 valid_loss: 0.07648 test_loss: 0.06389 \n",
      "Validation loss decreased (0.076770 --> 0.076476).  Saving model ...\n",
      "[ 81/100] train_loss: 0.07187 valid_loss: 0.08176 test_loss: 0.06410 \n",
      "[ 82/100] train_loss: 0.06893 valid_loss: 0.09310 test_loss: 0.06706 \n",
      "[ 83/100] train_loss: 0.07623 valid_loss: 0.07915 test_loss: 0.06311 \n",
      "[ 84/100] train_loss: 0.07478 valid_loss: 0.07386 test_loss: 0.06558 \n",
      "Validation loss decreased (0.076476 --> 0.073862).  Saving model ...\n",
      "[ 85/100] train_loss: 0.06998 valid_loss: 0.09119 test_loss: 0.06644 \n",
      "[ 86/100] train_loss: 0.06804 valid_loss: 0.09611 test_loss: 0.06834 \n",
      "[ 87/100] train_loss: 0.06736 valid_loss: 0.07397 test_loss: 0.06321 \n",
      "[ 88/100] train_loss: 0.06757 valid_loss: 0.08708 test_loss: 0.06410 \n",
      "[ 89/100] train_loss: 0.06956 valid_loss: 0.08717 test_loss: 0.06386 \n",
      "[ 90/100] train_loss: 0.06829 valid_loss: 0.08801 test_loss: 0.06420 \n",
      "[ 91/100] train_loss: 0.06999 valid_loss: 0.07663 test_loss: 0.06304 \n",
      "[ 92/100] train_loss: 0.07515 valid_loss: 0.10152 test_loss: 0.06915 \n",
      "[ 93/100] train_loss: 0.06899 valid_loss: 0.07966 test_loss: 0.06200 \n",
      "[ 94/100] train_loss: 0.06848 valid_loss: 0.07622 test_loss: 0.06163 \n",
      "[ 95/100] train_loss: 0.05993 valid_loss: 0.09414 test_loss: 0.06502 \n",
      "[ 96/100] train_loss: 0.07403 valid_loss: 0.07974 test_loss: 0.06133 \n",
      "[ 97/100] train_loss: 0.06304 valid_loss: 0.08126 test_loss: 0.06166 \n",
      "[ 98/100] train_loss: 0.06332 valid_loss: 0.08218 test_loss: 0.06142 \n",
      "[ 99/100] train_loss: 0.06509 valid_loss: 0.09378 test_loss: 0.06414 \n",
      "[100/100] train_loss: 0.07402 valid_loss: 0.08644 test_loss: 0.06227 \n",
      "TRAINING MODEL 15\n",
      "[  1/100] train_loss: 0.18550 valid_loss: 0.18899 test_loss: 0.18605 \n",
      "Validation loss decreased (inf --> 0.188993).  Saving model ...\n",
      "[  2/100] train_loss: 0.18381 valid_loss: 0.18849 test_loss: 0.18490 \n",
      "Validation loss decreased (0.188993 --> 0.188494).  Saving model ...\n",
      "[  3/100] train_loss: 0.18213 valid_loss: 0.18728 test_loss: 0.18312 \n",
      "Validation loss decreased (0.188494 --> 0.187280).  Saving model ...\n",
      "[  4/100] train_loss: 0.17912 valid_loss: 0.18520 test_loss: 0.18062 \n",
      "Validation loss decreased (0.187280 --> 0.185198).  Saving model ...\n",
      "[  5/100] train_loss: 0.17730 valid_loss: 0.18283 test_loss: 0.17748 \n",
      "Validation loss decreased (0.185198 --> 0.182835).  Saving model ...\n",
      "[  6/100] train_loss: 0.17550 valid_loss: 0.17972 test_loss: 0.17397 \n",
      "Validation loss decreased (0.182835 --> 0.179718).  Saving model ...\n",
      "[  7/100] train_loss: 0.17143 valid_loss: 0.17698 test_loss: 0.17050 \n",
      "Validation loss decreased (0.179718 --> 0.176978).  Saving model ...\n",
      "[  8/100] train_loss: 0.16934 valid_loss: 0.17443 test_loss: 0.16697 \n",
      "Validation loss decreased (0.176978 --> 0.174435).  Saving model ...\n",
      "[  9/100] train_loss: 0.16726 valid_loss: 0.17101 test_loss: 0.16352 \n",
      "Validation loss decreased (0.174435 --> 0.171009).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16315 valid_loss: 0.16744 test_loss: 0.16040 \n",
      "Validation loss decreased (0.171009 --> 0.167437).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16035 valid_loss: 0.16432 test_loss: 0.15656 \n",
      "Validation loss decreased (0.167437 --> 0.164320).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15790 valid_loss: 0.16111 test_loss: 0.15271 \n",
      "Validation loss decreased (0.164320 --> 0.161107).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15312 valid_loss: 0.15693 test_loss: 0.14903 \n",
      "Validation loss decreased (0.161107 --> 0.156934).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14909 valid_loss: 0.15351 test_loss: 0.14510 \n",
      "Validation loss decreased (0.156934 --> 0.153514).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14521 valid_loss: 0.15063 test_loss: 0.14133 \n",
      "Validation loss decreased (0.153514 --> 0.150631).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14199 valid_loss: 0.14600 test_loss: 0.13736 \n",
      "Validation loss decreased (0.150631 --> 0.146003).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13805 valid_loss: 0.14544 test_loss: 0.13460 \n",
      "Validation loss decreased (0.146003 --> 0.145435).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13583 valid_loss: 0.13961 test_loss: 0.13063 \n",
      "Validation loss decreased (0.145435 --> 0.139609).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13244 valid_loss: 0.13493 test_loss: 0.12818 \n",
      "Validation loss decreased (0.139609 --> 0.134928).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12567 valid_loss: 0.13030 test_loss: 0.12315 \n",
      "Validation loss decreased (0.134928 --> 0.130301).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12256 valid_loss: 0.13299 test_loss: 0.12057 \n",
      "[ 22/100] train_loss: 0.12245 valid_loss: 0.12165 test_loss: 0.11603 \n",
      "Validation loss decreased (0.130301 --> 0.121646).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11683 valid_loss: 0.12156 test_loss: 0.11476 \n",
      "Validation loss decreased (0.121646 --> 0.121562).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11271 valid_loss: 0.11899 test_loss: 0.11151 \n",
      "Validation loss decreased (0.121562 --> 0.118989).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11428 valid_loss: 0.11504 test_loss: 0.10710 \n",
      "Validation loss decreased (0.118989 --> 0.115038).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11085 valid_loss: 0.11319 test_loss: 0.10484 \n",
      "Validation loss decreased (0.115038 --> 0.113193).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10502 valid_loss: 0.11234 test_loss: 0.10262 \n",
      "Validation loss decreased (0.113193 --> 0.112344).  Saving model ...\n",
      "[ 28/100] train_loss: 0.09987 valid_loss: 0.11237 test_loss: 0.10044 \n",
      "[ 29/100] train_loss: 0.10123 valid_loss: 0.10242 test_loss: 0.09697 \n",
      "Validation loss decreased (0.112344 --> 0.102420).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10421 valid_loss: 0.10183 test_loss: 0.09517 \n",
      "Validation loss decreased (0.102420 --> 0.101831).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09442 valid_loss: 0.10655 test_loss: 0.09300 \n",
      "[ 32/100] train_loss: 0.10026 valid_loss: 0.09927 test_loss: 0.08947 \n",
      "Validation loss decreased (0.101831 --> 0.099266).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09104 valid_loss: 0.09766 test_loss: 0.08727 \n",
      "Validation loss decreased (0.099266 --> 0.097662).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09214 valid_loss: 0.09585 test_loss: 0.08632 \n",
      "Validation loss decreased (0.097662 --> 0.095851).  Saving model ...\n",
      "[ 35/100] train_loss: 0.08742 valid_loss: 0.10272 test_loss: 0.08728 \n",
      "[ 36/100] train_loss: 0.09448 valid_loss: 0.08891 test_loss: 0.08565 \n",
      "Validation loss decreased (0.095851 --> 0.088912).  Saving model ...\n",
      "[ 37/100] train_loss: 0.08719 valid_loss: 0.09254 test_loss: 0.08305 \n",
      "[ 38/100] train_loss: 0.08685 valid_loss: 0.09483 test_loss: 0.08213 \n",
      "[ 39/100] train_loss: 0.08500 valid_loss: 0.09535 test_loss: 0.08147 \n",
      "[ 40/100] train_loss: 0.08363 valid_loss: 0.09712 test_loss: 0.08138 \n",
      "[ 41/100] train_loss: 0.08127 valid_loss: 0.09135 test_loss: 0.07893 \n",
      "[ 42/100] train_loss: 0.08233 valid_loss: 0.09262 test_loss: 0.07837 \n",
      "[ 43/100] train_loss: 0.08542 valid_loss: 0.08959 test_loss: 0.07751 \n",
      "[ 44/100] train_loss: 0.08280 valid_loss: 0.09244 test_loss: 0.07793 \n",
      "[ 45/100] train_loss: 0.08276 valid_loss: 0.09262 test_loss: 0.07651 \n",
      "[ 46/100] train_loss: 0.08382 valid_loss: 0.08406 test_loss: 0.07456 \n",
      "Validation loss decreased (0.088912 --> 0.084062).  Saving model ...\n",
      "[ 47/100] train_loss: 0.08385 valid_loss: 0.08438 test_loss: 0.07454 \n",
      "[ 48/100] train_loss: 0.08126 valid_loss: 0.09688 test_loss: 0.07578 \n",
      "[ 49/100] train_loss: 0.08312 valid_loss: 0.08495 test_loss: 0.07258 \n",
      "[ 50/100] train_loss: 0.07648 valid_loss: 0.08595 test_loss: 0.07210 \n",
      "[ 51/100] train_loss: 0.08379 valid_loss: 0.08696 test_loss: 0.07185 \n",
      "[ 52/100] train_loss: 0.07884 valid_loss: 0.08613 test_loss: 0.07147 \n",
      "[ 53/100] train_loss: 0.07850 valid_loss: 0.08471 test_loss: 0.07144 \n",
      "[ 54/100] train_loss: 0.08376 valid_loss: 0.08816 test_loss: 0.07129 \n",
      "[ 55/100] train_loss: 0.07396 valid_loss: 0.08504 test_loss: 0.07065 \n",
      "[ 56/100] train_loss: 0.07864 valid_loss: 0.08656 test_loss: 0.06985 \n",
      "[ 57/100] train_loss: 0.07809 valid_loss: 0.08660 test_loss: 0.06967 \n",
      "[ 58/100] train_loss: 0.07868 valid_loss: 0.09085 test_loss: 0.07015 \n",
      "[ 59/100] train_loss: 0.07606 valid_loss: 0.08412 test_loss: 0.06902 \n",
      "[ 60/100] train_loss: 0.07596 valid_loss: 0.08782 test_loss: 0.06892 \n",
      "[ 61/100] train_loss: 0.07552 valid_loss: 0.08356 test_loss: 0.06810 \n",
      "Validation loss decreased (0.084062 --> 0.083564).  Saving model ...\n",
      "[ 62/100] train_loss: 0.07624 valid_loss: 0.08561 test_loss: 0.06827 \n",
      "[ 63/100] train_loss: 0.07513 valid_loss: 0.08634 test_loss: 0.06822 \n",
      "[ 64/100] train_loss: 0.07393 valid_loss: 0.08287 test_loss: 0.06748 \n",
      "Validation loss decreased (0.083564 --> 0.082874).  Saving model ...\n",
      "[ 65/100] train_loss: 0.07133 valid_loss: 0.08265 test_loss: 0.06736 \n",
      "Validation loss decreased (0.082874 --> 0.082652).  Saving model ...\n",
      "[ 66/100] train_loss: 0.07825 valid_loss: 0.09023 test_loss: 0.06839 \n",
      "[ 67/100] train_loss: 0.07604 valid_loss: 0.08363 test_loss: 0.06736 \n",
      "[ 68/100] train_loss: 0.07766 valid_loss: 0.08050 test_loss: 0.06895 \n",
      "Validation loss decreased (0.082652 --> 0.080495).  Saving model ...\n",
      "[ 69/100] train_loss: 0.07438 valid_loss: 0.08959 test_loss: 0.06758 \n",
      "[ 70/100] train_loss: 0.07510 valid_loss: 0.08414 test_loss: 0.06653 \n",
      "[ 71/100] train_loss: 0.07101 valid_loss: 0.08233 test_loss: 0.06648 \n",
      "[ 72/100] train_loss: 0.07129 valid_loss: 0.08300 test_loss: 0.06661 \n",
      "[ 73/100] train_loss: 0.07184 valid_loss: 0.08461 test_loss: 0.06629 \n",
      "[ 74/100] train_loss: 0.07940 valid_loss: 0.08347 test_loss: 0.06615 \n",
      "[ 75/100] train_loss: 0.07050 valid_loss: 0.09184 test_loss: 0.06713 \n",
      "[ 76/100] train_loss: 0.07407 valid_loss: 0.08101 test_loss: 0.06524 \n",
      "[ 77/100] train_loss: 0.08099 valid_loss: 0.08013 test_loss: 0.06582 \n",
      "Validation loss decreased (0.080495 --> 0.080128).  Saving model ...\n",
      "[ 78/100] train_loss: 0.07418 valid_loss: 0.08761 test_loss: 0.06599 \n",
      "[ 79/100] train_loss: 0.06705 valid_loss: 0.07979 test_loss: 0.06485 \n",
      "Validation loss decreased (0.080128 --> 0.079786).  Saving model ...\n",
      "[ 80/100] train_loss: 0.07454 valid_loss: 0.08125 test_loss: 0.06438 \n",
      "[ 81/100] train_loss: 0.07742 valid_loss: 0.08462 test_loss: 0.06447 \n",
      "[ 82/100] train_loss: 0.06630 valid_loss: 0.08071 test_loss: 0.06430 \n",
      "[ 83/100] train_loss: 0.07073 valid_loss: 0.08394 test_loss: 0.06399 \n",
      "[ 84/100] train_loss: 0.06814 valid_loss: 0.08294 test_loss: 0.06355 \n",
      "[ 85/100] train_loss: 0.06670 valid_loss: 0.07910 test_loss: 0.06455 \n",
      "Validation loss decreased (0.079786 --> 0.079099).  Saving model ...\n",
      "[ 86/100] train_loss: 0.07084 valid_loss: 0.07963 test_loss: 0.06467 \n",
      "[ 87/100] train_loss: 0.07341 valid_loss: 0.08202 test_loss: 0.06351 \n",
      "[ 88/100] train_loss: 0.07156 valid_loss: 0.08744 test_loss: 0.06383 \n",
      "[ 89/100] train_loss: 0.07595 valid_loss: 0.07723 test_loss: 0.06403 \n",
      "Validation loss decreased (0.079099 --> 0.077228).  Saving model ...\n",
      "[ 90/100] train_loss: 0.07169 valid_loss: 0.07718 test_loss: 0.06379 \n",
      "Validation loss decreased (0.077228 --> 0.077178).  Saving model ...\n",
      "[ 91/100] train_loss: 0.07338 valid_loss: 0.08641 test_loss: 0.06337 \n",
      "[ 92/100] train_loss: 0.07174 valid_loss: 0.07716 test_loss: 0.06391 \n",
      "Validation loss decreased (0.077178 --> 0.077158).  Saving model ...\n",
      "[ 93/100] train_loss: 0.06926 valid_loss: 0.07721 test_loss: 0.06370 \n",
      "[ 94/100] train_loss: 0.06764 valid_loss: 0.08089 test_loss: 0.06293 \n",
      "[ 95/100] train_loss: 0.07590 valid_loss: 0.07896 test_loss: 0.06300 \n",
      "[ 96/100] train_loss: 0.07471 valid_loss: 0.08343 test_loss: 0.06249 \n",
      "[ 97/100] train_loss: 0.06931 valid_loss: 0.07953 test_loss: 0.06300 \n",
      "[ 98/100] train_loss: 0.06428 valid_loss: 0.07499 test_loss: 0.06652 \n",
      "Validation loss decreased (0.077158 --> 0.074987).  Saving model ...\n",
      "[ 99/100] train_loss: 0.06011 valid_loss: 0.07948 test_loss: 0.06171 \n",
      "[100/100] train_loss: 0.06408 valid_loss: 0.08557 test_loss: 0.06171 \n",
      "TRAINING MODEL 16\n",
      "[  1/100] train_loss: 0.18507 valid_loss: 0.19917 test_loss: 0.18572 \n",
      "Validation loss decreased (inf --> 0.199168).  Saving model ...\n",
      "[  2/100] train_loss: 0.18356 valid_loss: 0.19781 test_loss: 0.18437 \n",
      "Validation loss decreased (0.199168 --> 0.197813).  Saving model ...\n",
      "[  3/100] train_loss: 0.18241 valid_loss: 0.19600 test_loss: 0.18253 \n",
      "Validation loss decreased (0.197813 --> 0.196003).  Saving model ...\n",
      "[  4/100] train_loss: 0.17985 valid_loss: 0.19389 test_loss: 0.18029 \n",
      "Validation loss decreased (0.196003 --> 0.193895).  Saving model ...\n",
      "[  5/100] train_loss: 0.17637 valid_loss: 0.19183 test_loss: 0.17759 \n",
      "Validation loss decreased (0.193895 --> 0.191826).  Saving model ...\n",
      "[  6/100] train_loss: 0.17601 valid_loss: 0.18971 test_loss: 0.17452 \n",
      "Validation loss decreased (0.191826 --> 0.189714).  Saving model ...\n",
      "[  7/100] train_loss: 0.17412 valid_loss: 0.18732 test_loss: 0.17092 \n",
      "Validation loss decreased (0.189714 --> 0.187320).  Saving model ...\n",
      "[  8/100] train_loss: 0.16839 valid_loss: 0.18484 test_loss: 0.16803 \n",
      "Validation loss decreased (0.187320 --> 0.184841).  Saving model ...\n",
      "[  9/100] train_loss: 0.16570 valid_loss: 0.18289 test_loss: 0.16500 \n",
      "Validation loss decreased (0.184841 --> 0.182893).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16535 valid_loss: 0.18131 test_loss: 0.16150 \n",
      "Validation loss decreased (0.182893 --> 0.181311).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16332 valid_loss: 0.17785 test_loss: 0.15919 \n",
      "Validation loss decreased (0.181311 --> 0.177850).  Saving model ...\n",
      "[ 12/100] train_loss: 0.16095 valid_loss: 0.17399 test_loss: 0.15549 \n",
      "Validation loss decreased (0.177850 --> 0.173988).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15602 valid_loss: 0.17077 test_loss: 0.15130 \n",
      "Validation loss decreased (0.173988 --> 0.170770).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15066 valid_loss: 0.16748 test_loss: 0.14739 \n",
      "Validation loss decreased (0.170770 --> 0.167475).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14785 valid_loss: 0.16299 test_loss: 0.14393 \n",
      "Validation loss decreased (0.167475 --> 0.162988).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14769 valid_loss: 0.15845 test_loss: 0.14015 \n",
      "Validation loss decreased (0.162988 --> 0.158448).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13885 valid_loss: 0.15762 test_loss: 0.13771 \n",
      "Validation loss decreased (0.158448 --> 0.157621).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13393 valid_loss: 0.15016 test_loss: 0.13288 \n",
      "Validation loss decreased (0.157621 --> 0.150162).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13190 valid_loss: 0.14720 test_loss: 0.12939 \n",
      "Validation loss decreased (0.150162 --> 0.147203).  Saving model ...\n",
      "[ 20/100] train_loss: 0.13092 valid_loss: 0.14382 test_loss: 0.12647 \n",
      "Validation loss decreased (0.147203 --> 0.143817).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12398 valid_loss: 0.14408 test_loss: 0.12388 \n",
      "[ 22/100] train_loss: 0.12091 valid_loss: 0.13432 test_loss: 0.11778 \n",
      "Validation loss decreased (0.143817 --> 0.134323).  Saving model ...\n",
      "[ 23/100] train_loss: 0.12059 valid_loss: 0.12870 test_loss: 0.11438 \n",
      "Validation loss decreased (0.134323 --> 0.128705).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11505 valid_loss: 0.12670 test_loss: 0.11118 \n",
      "Validation loss decreased (0.128705 --> 0.126696).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11329 valid_loss: 0.12230 test_loss: 0.10730 \n",
      "Validation loss decreased (0.126696 --> 0.122300).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11114 valid_loss: 0.12108 test_loss: 0.10501 \n",
      "Validation loss decreased (0.122300 --> 0.121080).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10955 valid_loss: 0.11636 test_loss: 0.10232 \n",
      "Validation loss decreased (0.121080 --> 0.116360).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10568 valid_loss: 0.11281 test_loss: 0.09923 \n",
      "Validation loss decreased (0.116360 --> 0.112814).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10234 valid_loss: 0.10719 test_loss: 0.09662 \n",
      "Validation loss decreased (0.112814 --> 0.107191).  Saving model ...\n",
      "[ 30/100] train_loss: 0.10550 valid_loss: 0.10515 test_loss: 0.09498 \n",
      "Validation loss decreased (0.107191 --> 0.105153).  Saving model ...\n",
      "[ 31/100] train_loss: 0.10405 valid_loss: 0.10942 test_loss: 0.09369 \n",
      "[ 32/100] train_loss: 0.09774 valid_loss: 0.10844 test_loss: 0.09176 \n",
      "[ 33/100] train_loss: 0.10116 valid_loss: 0.10091 test_loss: 0.08874 \n",
      "Validation loss decreased (0.105153 --> 0.100911).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09211 valid_loss: 0.09971 test_loss: 0.08732 \n",
      "Validation loss decreased (0.100911 --> 0.099705).  Saving model ...\n",
      "[ 35/100] train_loss: 0.08752 valid_loss: 0.10093 test_loss: 0.08636 \n",
      "[ 36/100] train_loss: 0.09346 valid_loss: 0.09657 test_loss: 0.08442 \n",
      "Validation loss decreased (0.099705 --> 0.096573).  Saving model ...\n",
      "[ 37/100] train_loss: 0.08818 valid_loss: 0.09349 test_loss: 0.08339 \n",
      "Validation loss decreased (0.096573 --> 0.093491).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08805 valid_loss: 0.09528 test_loss: 0.08250 \n",
      "[ 39/100] train_loss: 0.08533 valid_loss: 0.09375 test_loss: 0.08118 \n",
      "[ 40/100] train_loss: 0.08482 valid_loss: 0.08952 test_loss: 0.08022 \n",
      "Validation loss decreased (0.093491 --> 0.089517).  Saving model ...\n",
      "[ 41/100] train_loss: 0.09504 valid_loss: 0.09761 test_loss: 0.08034 \n",
      "[ 42/100] train_loss: 0.08431 valid_loss: 0.09608 test_loss: 0.07968 \n",
      "[ 43/100] train_loss: 0.08138 valid_loss: 0.09115 test_loss: 0.07783 \n",
      "[ 44/100] train_loss: 0.08100 valid_loss: 0.09036 test_loss: 0.07682 \n",
      "[ 45/100] train_loss: 0.07822 valid_loss: 0.09747 test_loss: 0.07774 \n",
      "[ 46/100] train_loss: 0.08541 valid_loss: 0.08728 test_loss: 0.07560 \n",
      "Validation loss decreased (0.089517 --> 0.087278).  Saving model ...\n",
      "[ 47/100] train_loss: 0.08995 valid_loss: 0.08598 test_loss: 0.07728 \n",
      "Validation loss decreased (0.087278 --> 0.085982).  Saving model ...\n",
      "[ 48/100] train_loss: 0.08051 valid_loss: 0.09238 test_loss: 0.07559 \n",
      "[ 49/100] train_loss: 0.07803 valid_loss: 0.09675 test_loss: 0.07578 \n",
      "[ 50/100] train_loss: 0.08227 valid_loss: 0.08933 test_loss: 0.07367 \n",
      "[ 51/100] train_loss: 0.08033 valid_loss: 0.08672 test_loss: 0.07321 \n",
      "[ 52/100] train_loss: 0.07807 valid_loss: 0.09344 test_loss: 0.07346 \n",
      "[ 53/100] train_loss: 0.07873 valid_loss: 0.08336 test_loss: 0.07242 \n",
      "Validation loss decreased (0.085982 --> 0.083363).  Saving model ...\n",
      "[ 54/100] train_loss: 0.08480 valid_loss: 0.08754 test_loss: 0.07202 \n",
      "[ 55/100] train_loss: 0.08137 valid_loss: 0.09842 test_loss: 0.07442 \n",
      "[ 56/100] train_loss: 0.08418 valid_loss: 0.09118 test_loss: 0.07181 \n",
      "[ 57/100] train_loss: 0.07282 valid_loss: 0.08595 test_loss: 0.07099 \n",
      "[ 58/100] train_loss: 0.08660 valid_loss: 0.09996 test_loss: 0.07392 \n",
      "[ 59/100] train_loss: 0.08493 valid_loss: 0.08967 test_loss: 0.07105 \n",
      "[ 60/100] train_loss: 0.07464 valid_loss: 0.08860 test_loss: 0.07024 \n",
      "[ 61/100] train_loss: 0.07887 valid_loss: 0.09150 test_loss: 0.07017 \n",
      "[ 62/100] train_loss: 0.08307 valid_loss: 0.08542 test_loss: 0.06929 \n",
      "[ 63/100] train_loss: 0.07799 valid_loss: 0.08556 test_loss: 0.06963 \n",
      "[ 64/100] train_loss: 0.06803 valid_loss: 0.09050 test_loss: 0.06949 \n",
      "[ 65/100] train_loss: 0.07518 valid_loss: 0.08888 test_loss: 0.06851 \n",
      "[ 66/100] train_loss: 0.08145 valid_loss: 0.08895 test_loss: 0.06849 \n",
      "[ 67/100] train_loss: 0.07547 valid_loss: 0.10029 test_loss: 0.07177 \n",
      "[ 68/100] train_loss: 0.06970 valid_loss: 0.08309 test_loss: 0.06822 \n",
      "Validation loss decreased (0.083363 --> 0.083087).  Saving model ...\n",
      "[ 69/100] train_loss: 0.07239 valid_loss: 0.08698 test_loss: 0.06766 \n",
      "[ 70/100] train_loss: 0.07442 valid_loss: 0.09589 test_loss: 0.06903 \n",
      "[ 71/100] train_loss: 0.07670 valid_loss: 0.08527 test_loss: 0.06717 \n",
      "[ 72/100] train_loss: 0.07091 valid_loss: 0.08519 test_loss: 0.06783 \n",
      "[ 73/100] train_loss: 0.07481 valid_loss: 0.08893 test_loss: 0.06770 \n",
      "[ 74/100] train_loss: 0.08031 valid_loss: 0.09484 test_loss: 0.06832 \n",
      "[ 75/100] train_loss: 0.07042 valid_loss: 0.08740 test_loss: 0.06645 \n",
      "[ 76/100] train_loss: 0.07342 valid_loss: 0.08091 test_loss: 0.06795 \n",
      "Validation loss decreased (0.083087 --> 0.080911).  Saving model ...\n",
      "[ 77/100] train_loss: 0.07500 valid_loss: 0.09589 test_loss: 0.06839 \n",
      "[ 78/100] train_loss: 0.07476 valid_loss: 0.09301 test_loss: 0.06677 \n",
      "[ 79/100] train_loss: 0.06736 valid_loss: 0.08051 test_loss: 0.06572 \n",
      "Validation loss decreased (0.080911 --> 0.080508).  Saving model ...\n",
      "[ 80/100] train_loss: 0.06570 valid_loss: 0.08663 test_loss: 0.06533 \n",
      "[ 81/100] train_loss: 0.07116 valid_loss: 0.09604 test_loss: 0.06707 \n",
      "[ 82/100] train_loss: 0.07097 valid_loss: 0.08635 test_loss: 0.06540 \n",
      "[ 83/100] train_loss: 0.06753 valid_loss: 0.08666 test_loss: 0.06504 \n",
      "[ 84/100] train_loss: 0.07144 valid_loss: 0.09317 test_loss: 0.06573 \n",
      "[ 85/100] train_loss: 0.06793 valid_loss: 0.09274 test_loss: 0.06564 \n",
      "[ 86/100] train_loss: 0.07390 valid_loss: 0.08753 test_loss: 0.06538 \n",
      "[ 87/100] train_loss: 0.07245 valid_loss: 0.09314 test_loss: 0.06633 \n",
      "[ 88/100] train_loss: 0.06298 valid_loss: 0.08482 test_loss: 0.06420 \n",
      "[ 89/100] train_loss: 0.06340 valid_loss: 0.08673 test_loss: 0.06368 \n",
      "[ 90/100] train_loss: 0.07409 valid_loss: 0.08486 test_loss: 0.06356 \n",
      "[ 91/100] train_loss: 0.06626 valid_loss: 0.09516 test_loss: 0.06516 \n",
      "[ 92/100] train_loss: 0.07545 valid_loss: 0.08390 test_loss: 0.06377 \n",
      "[ 93/100] train_loss: 0.07435 valid_loss: 0.08752 test_loss: 0.06408 \n",
      "[ 94/100] train_loss: 0.07191 valid_loss: 0.09863 test_loss: 0.06619 \n",
      "[ 95/100] train_loss: 0.06858 valid_loss: 0.09291 test_loss: 0.06434 \n",
      "[ 96/100] train_loss: 0.07129 valid_loss: 0.08037 test_loss: 0.06388 \n",
      "Validation loss decreased (0.080508 --> 0.080365).  Saving model ...\n",
      "[ 97/100] train_loss: 0.06844 valid_loss: 0.08737 test_loss: 0.06385 \n",
      "[ 98/100] train_loss: 0.07511 valid_loss: 0.10472 test_loss: 0.06859 \n",
      "[ 99/100] train_loss: 0.07184 valid_loss: 0.07972 test_loss: 0.06365 \n",
      "Validation loss decreased (0.080365 --> 0.079717).  Saving model ...\n",
      "[100/100] train_loss: 0.06401 valid_loss: 0.08267 test_loss: 0.06297 \n",
      "TRAINING MODEL 17\n",
      "[  1/100] train_loss: 0.18682 valid_loss: 0.19079 test_loss: 0.18627 \n",
      "Validation loss decreased (inf --> 0.190793).  Saving model ...\n",
      "[  2/100] train_loss: 0.18277 valid_loss: 0.18965 test_loss: 0.18481 \n",
      "Validation loss decreased (0.190793 --> 0.189653).  Saving model ...\n",
      "[  3/100] train_loss: 0.18230 valid_loss: 0.18793 test_loss: 0.18271 \n",
      "Validation loss decreased (0.189653 --> 0.187932).  Saving model ...\n",
      "[  4/100] train_loss: 0.17931 valid_loss: 0.18581 test_loss: 0.17994 \n",
      "Validation loss decreased (0.187932 --> 0.185811).  Saving model ...\n",
      "[  5/100] train_loss: 0.17730 valid_loss: 0.18337 test_loss: 0.17704 \n",
      "Validation loss decreased (0.185811 --> 0.183373).  Saving model ...\n",
      "[  6/100] train_loss: 0.17528 valid_loss: 0.18116 test_loss: 0.17387 \n",
      "Validation loss decreased (0.183373 --> 0.181163).  Saving model ...\n",
      "[  7/100] train_loss: 0.17185 valid_loss: 0.17831 test_loss: 0.17045 \n",
      "Validation loss decreased (0.181163 --> 0.178312).  Saving model ...\n",
      "[  8/100] train_loss: 0.16950 valid_loss: 0.17592 test_loss: 0.16726 \n",
      "Validation loss decreased (0.178312 --> 0.175917).  Saving model ...\n",
      "[  9/100] train_loss: 0.16624 valid_loss: 0.17381 test_loss: 0.16476 \n",
      "Validation loss decreased (0.175917 --> 0.173814).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16414 valid_loss: 0.17176 test_loss: 0.16166 \n",
      "Validation loss decreased (0.173814 --> 0.171756).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16201 valid_loss: 0.16930 test_loss: 0.15826 \n",
      "Validation loss decreased (0.171756 --> 0.169298).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15709 valid_loss: 0.16553 test_loss: 0.15443 \n",
      "Validation loss decreased (0.169298 --> 0.165530).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15467 valid_loss: 0.16277 test_loss: 0.15136 \n",
      "Validation loss decreased (0.165530 --> 0.162766).  Saving model ...\n",
      "[ 14/100] train_loss: 0.15179 valid_loss: 0.15965 test_loss: 0.14782 \n",
      "Validation loss decreased (0.162766 --> 0.159650).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14958 valid_loss: 0.15560 test_loss: 0.14448 \n",
      "Validation loss decreased (0.159650 --> 0.155596).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14404 valid_loss: 0.15305 test_loss: 0.14056 \n",
      "Validation loss decreased (0.155596 --> 0.153048).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13916 valid_loss: 0.14682 test_loss: 0.13559 \n",
      "Validation loss decreased (0.153048 --> 0.146825).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13559 valid_loss: 0.14249 test_loss: 0.13211 \n",
      "Validation loss decreased (0.146825 --> 0.142491).  Saving model ...\n",
      "[ 19/100] train_loss: 0.13522 valid_loss: 0.13991 test_loss: 0.12777 \n",
      "Validation loss decreased (0.142491 --> 0.139914).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12951 valid_loss: 0.13636 test_loss: 0.12411 \n",
      "Validation loss decreased (0.139914 --> 0.136361).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12796 valid_loss: 0.13385 test_loss: 0.12207 \n",
      "Validation loss decreased (0.136361 --> 0.133852).  Saving model ...\n",
      "[ 22/100] train_loss: 0.12146 valid_loss: 0.13205 test_loss: 0.11874 \n",
      "Validation loss decreased (0.133852 --> 0.132054).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11867 valid_loss: 0.12590 test_loss: 0.11431 \n",
      "Validation loss decreased (0.132054 --> 0.125902).  Saving model ...\n",
      "[ 24/100] train_loss: 0.10931 valid_loss: 0.12143 test_loss: 0.11108 \n",
      "Validation loss decreased (0.125902 --> 0.121432).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11067 valid_loss: 0.11908 test_loss: 0.10744 \n",
      "Validation loss decreased (0.121432 --> 0.119084).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10708 valid_loss: 0.11790 test_loss: 0.10450 \n",
      "Validation loss decreased (0.119084 --> 0.117905).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10263 valid_loss: 0.11374 test_loss: 0.10150 \n",
      "Validation loss decreased (0.117905 --> 0.113739).  Saving model ...\n",
      "[ 28/100] train_loss: 0.10086 valid_loss: 0.10950 test_loss: 0.09858 \n",
      "Validation loss decreased (0.113739 --> 0.109496).  Saving model ...\n",
      "[ 29/100] train_loss: 0.10205 valid_loss: 0.10555 test_loss: 0.09668 \n",
      "Validation loss decreased (0.109496 --> 0.105548).  Saving model ...\n",
      "[ 30/100] train_loss: 0.09764 valid_loss: 0.10508 test_loss: 0.09398 \n",
      "Validation loss decreased (0.105548 --> 0.105079).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09634 valid_loss: 0.10582 test_loss: 0.09192 \n",
      "[ 32/100] train_loss: 0.09471 valid_loss: 0.10117 test_loss: 0.08928 \n",
      "Validation loss decreased (0.105079 --> 0.101172).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09531 valid_loss: 0.09803 test_loss: 0.08759 \n",
      "Validation loss decreased (0.101172 --> 0.098033).  Saving model ...\n",
      "[ 34/100] train_loss: 0.09220 valid_loss: 0.10107 test_loss: 0.08661 \n",
      "[ 35/100] train_loss: 0.08298 valid_loss: 0.09881 test_loss: 0.08483 \n",
      "[ 36/100] train_loss: 0.09691 valid_loss: 0.09368 test_loss: 0.08287 \n",
      "Validation loss decreased (0.098033 --> 0.093676).  Saving model ...\n",
      "[ 37/100] train_loss: 0.09174 valid_loss: 0.09647 test_loss: 0.08252 \n",
      "[ 38/100] train_loss: 0.09072 valid_loss: 0.09542 test_loss: 0.08130 \n",
      "[ 39/100] train_loss: 0.08192 valid_loss: 0.09143 test_loss: 0.07939 \n",
      "Validation loss decreased (0.093676 --> 0.091426).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08205 valid_loss: 0.08969 test_loss: 0.07832 \n",
      "Validation loss decreased (0.091426 --> 0.089694).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08140 valid_loss: 0.09295 test_loss: 0.07799 \n",
      "[ 42/100] train_loss: 0.08674 valid_loss: 0.09475 test_loss: 0.07775 \n",
      "[ 43/100] train_loss: 0.09150 valid_loss: 0.09458 test_loss: 0.07752 \n",
      "[ 44/100] train_loss: 0.08272 valid_loss: 0.08833 test_loss: 0.07566 \n",
      "Validation loss decreased (0.089694 --> 0.088328).  Saving model ...\n",
      "[ 45/100] train_loss: 0.08364 valid_loss: 0.08553 test_loss: 0.07517 \n",
      "Validation loss decreased (0.088328 --> 0.085525).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08162 valid_loss: 0.08894 test_loss: 0.07411 \n",
      "[ 47/100] train_loss: 0.07648 valid_loss: 0.08944 test_loss: 0.07338 \n",
      "[ 48/100] train_loss: 0.08524 valid_loss: 0.08663 test_loss: 0.07285 \n",
      "[ 49/100] train_loss: 0.07700 valid_loss: 0.09554 test_loss: 0.07440 \n",
      "[ 50/100] train_loss: 0.07851 valid_loss: 0.08744 test_loss: 0.07176 \n",
      "[ 51/100] train_loss: 0.07743 valid_loss: 0.08241 test_loss: 0.07216 \n",
      "Validation loss decreased (0.085525 --> 0.082409).  Saving model ...\n",
      "[ 52/100] train_loss: 0.08551 valid_loss: 0.08986 test_loss: 0.07222 \n",
      "[ 53/100] train_loss: 0.08276 valid_loss: 0.08897 test_loss: 0.07227 \n",
      "[ 54/100] train_loss: 0.08472 valid_loss: 0.08501 test_loss: 0.07351 \n",
      "[ 55/100] train_loss: 0.07832 valid_loss: 0.09362 test_loss: 0.07183 \n",
      "[ 56/100] train_loss: 0.07799 valid_loss: 0.09298 test_loss: 0.07067 \n",
      "[ 57/100] train_loss: 0.07926 valid_loss: 0.08178 test_loss: 0.06995 \n",
      "Validation loss decreased (0.082409 --> 0.081781).  Saving model ...\n",
      "[ 58/100] train_loss: 0.07590 valid_loss: 0.08947 test_loss: 0.06935 \n",
      "[ 59/100] train_loss: 0.08473 valid_loss: 0.08552 test_loss: 0.06961 \n",
      "[ 60/100] train_loss: 0.07753 valid_loss: 0.09523 test_loss: 0.07091 \n",
      "[ 61/100] train_loss: 0.07336 valid_loss: 0.08321 test_loss: 0.06784 \n",
      "[ 62/100] train_loss: 0.07587 valid_loss: 0.08481 test_loss: 0.06828 \n",
      "[ 63/100] train_loss: 0.07156 valid_loss: 0.08690 test_loss: 0.06870 \n",
      "[ 64/100] train_loss: 0.07061 valid_loss: 0.08784 test_loss: 0.06811 \n",
      "[ 65/100] train_loss: 0.08207 valid_loss: 0.08946 test_loss: 0.06801 \n",
      "[ 66/100] train_loss: 0.07754 valid_loss: 0.08685 test_loss: 0.06758 \n",
      "[ 67/100] train_loss: 0.07278 valid_loss: 0.08367 test_loss: 0.06683 \n",
      "[ 68/100] train_loss: 0.07012 valid_loss: 0.08547 test_loss: 0.06614 \n",
      "[ 69/100] train_loss: 0.06884 valid_loss: 0.08836 test_loss: 0.06634 \n",
      "[ 70/100] train_loss: 0.07368 valid_loss: 0.08569 test_loss: 0.06572 \n",
      "[ 71/100] train_loss: 0.08140 valid_loss: 0.08623 test_loss: 0.06602 \n",
      "[ 72/100] train_loss: 0.07599 valid_loss: 0.08809 test_loss: 0.06683 \n",
      "[ 73/100] train_loss: 0.07344 valid_loss: 0.08929 test_loss: 0.06724 \n",
      "[ 74/100] train_loss: 0.07381 valid_loss: 0.08810 test_loss: 0.06655 \n",
      "[ 75/100] train_loss: 0.07096 valid_loss: 0.08382 test_loss: 0.06561 \n",
      "[ 76/100] train_loss: 0.07594 valid_loss: 0.09284 test_loss: 0.06648 \n",
      "[ 77/100] train_loss: 0.07453 valid_loss: 0.08850 test_loss: 0.06532 \n",
      "[ 78/100] train_loss: 0.06871 valid_loss: 0.08443 test_loss: 0.06460 \n",
      "[ 79/100] train_loss: 0.07063 valid_loss: 0.08848 test_loss: 0.06467 \n",
      "[ 80/100] train_loss: 0.06581 valid_loss: 0.08613 test_loss: 0.06440 \n",
      "[ 81/100] train_loss: 0.06639 valid_loss: 0.08868 test_loss: 0.06469 \n",
      "[ 82/100] train_loss: 0.07788 valid_loss: 0.08591 test_loss: 0.06473 \n",
      "[ 83/100] train_loss: 0.07393 valid_loss: 0.09075 test_loss: 0.06542 \n",
      "[ 84/100] train_loss: 0.06783 valid_loss: 0.09003 test_loss: 0.06449 \n",
      "[ 85/100] train_loss: 0.07223 valid_loss: 0.08056 test_loss: 0.06294 \n",
      "Validation loss decreased (0.081781 --> 0.080557).  Saving model ...\n",
      "[ 86/100] train_loss: 0.07284 valid_loss: 0.09196 test_loss: 0.06490 \n",
      "[ 87/100] train_loss: 0.06308 valid_loss: 0.09223 test_loss: 0.06503 \n",
      "[ 88/100] train_loss: 0.06387 valid_loss: 0.08119 test_loss: 0.06280 \n",
      "[ 89/100] train_loss: 0.06251 valid_loss: 0.08632 test_loss: 0.06284 \n",
      "[ 90/100] train_loss: 0.06921 valid_loss: 0.08995 test_loss: 0.06389 \n",
      "[ 91/100] train_loss: 0.06728 valid_loss: 0.08139 test_loss: 0.06373 \n",
      "[ 92/100] train_loss: 0.06634 valid_loss: 0.08747 test_loss: 0.06314 \n",
      "[ 93/100] train_loss: 0.07012 valid_loss: 0.09040 test_loss: 0.06334 \n",
      "[ 94/100] train_loss: 0.06604 valid_loss: 0.08543 test_loss: 0.06226 \n",
      "[ 95/100] train_loss: 0.06282 valid_loss: 0.09361 test_loss: 0.06411 \n",
      "[ 96/100] train_loss: 0.07532 valid_loss: 0.08826 test_loss: 0.06240 \n",
      "[ 97/100] train_loss: 0.06870 valid_loss: 0.08394 test_loss: 0.06177 \n",
      "[ 98/100] train_loss: 0.06672 valid_loss: 0.08070 test_loss: 0.06092 \n",
      "[ 99/100] train_loss: 0.07453 valid_loss: 0.10247 test_loss: 0.06691 \n",
      "[100/100] train_loss: 0.07432 valid_loss: 0.09378 test_loss: 0.06430 \n",
      "TRAINING MODEL 18\n",
      "[  1/100] train_loss: 0.18501 valid_loss: 0.19607 test_loss: 0.18524 \n",
      "Validation loss decreased (inf --> 0.196072).  Saving model ...\n",
      "[  2/100] train_loss: 0.18042 valid_loss: 0.19435 test_loss: 0.18361 \n",
      "Validation loss decreased (0.196072 --> 0.194352).  Saving model ...\n",
      "[  3/100] train_loss: 0.18000 valid_loss: 0.19220 test_loss: 0.18138 \n",
      "Validation loss decreased (0.194352 --> 0.192199).  Saving model ...\n",
      "[  4/100] train_loss: 0.17804 valid_loss: 0.18946 test_loss: 0.17851 \n",
      "Validation loss decreased (0.192199 --> 0.189460).  Saving model ...\n",
      "[  5/100] train_loss: 0.17746 valid_loss: 0.18648 test_loss: 0.17526 \n",
      "Validation loss decreased (0.189460 --> 0.186478).  Saving model ...\n",
      "[  6/100] train_loss: 0.17152 valid_loss: 0.18447 test_loss: 0.17207 \n",
      "Validation loss decreased (0.186478 --> 0.184466).  Saving model ...\n",
      "[  7/100] train_loss: 0.16995 valid_loss: 0.18257 test_loss: 0.16858 \n",
      "Validation loss decreased (0.184466 --> 0.182571).  Saving model ...\n",
      "[  8/100] train_loss: 0.16534 valid_loss: 0.18005 test_loss: 0.16558 \n",
      "Validation loss decreased (0.182571 --> 0.180046).  Saving model ...\n",
      "[  9/100] train_loss: 0.16373 valid_loss: 0.17648 test_loss: 0.16234 \n",
      "Validation loss decreased (0.180046 --> 0.176477).  Saving model ...\n",
      "[ 10/100] train_loss: 0.15972 valid_loss: 0.17343 test_loss: 0.15885 \n",
      "Validation loss decreased (0.176477 --> 0.173428).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15931 valid_loss: 0.17034 test_loss: 0.15524 \n",
      "Validation loss decreased (0.173428 --> 0.170345).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15438 valid_loss: 0.16791 test_loss: 0.15152 \n",
      "Validation loss decreased (0.170345 --> 0.167905).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15033 valid_loss: 0.16353 test_loss: 0.14800 \n",
      "Validation loss decreased (0.167905 --> 0.163526).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14313 valid_loss: 0.16108 test_loss: 0.14461 \n",
      "Validation loss decreased (0.163526 --> 0.161083).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14178 valid_loss: 0.15798 test_loss: 0.14035 \n",
      "Validation loss decreased (0.161083 --> 0.157983).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13987 valid_loss: 0.14952 test_loss: 0.13534 \n",
      "Validation loss decreased (0.157983 --> 0.149524).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13740 valid_loss: 0.14791 test_loss: 0.13098 \n",
      "Validation loss decreased (0.149524 --> 0.147905).  Saving model ...\n",
      "[ 18/100] train_loss: 0.12800 valid_loss: 0.15088 test_loss: 0.12888 \n",
      "[ 19/100] train_loss: 0.12985 valid_loss: 0.14094 test_loss: 0.12341 \n",
      "Validation loss decreased (0.147905 --> 0.140942).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12590 valid_loss: 0.13342 test_loss: 0.12005 \n",
      "Validation loss decreased (0.140942 --> 0.133416).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12017 valid_loss: 0.13229 test_loss: 0.11759 \n",
      "Validation loss decreased (0.133416 --> 0.132293).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11821 valid_loss: 0.13039 test_loss: 0.11478 \n",
      "Validation loss decreased (0.132293 --> 0.130391).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11419 valid_loss: 0.13146 test_loss: 0.11183 \n",
      "[ 24/100] train_loss: 0.11237 valid_loss: 0.12699 test_loss: 0.11040 \n",
      "Validation loss decreased (0.130391 --> 0.126990).  Saving model ...\n",
      "[ 25/100] train_loss: 0.10833 valid_loss: 0.12261 test_loss: 0.10810 \n",
      "Validation loss decreased (0.126990 --> 0.122608).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10691 valid_loss: 0.11993 test_loss: 0.10527 \n",
      "Validation loss decreased (0.122608 --> 0.119929).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10615 valid_loss: 0.12000 test_loss: 0.10078 \n",
      "[ 28/100] train_loss: 0.10664 valid_loss: 0.11177 test_loss: 0.09718 \n",
      "Validation loss decreased (0.119929 --> 0.111769).  Saving model ...\n",
      "[ 29/100] train_loss: 0.09772 valid_loss: 0.11273 test_loss: 0.09533 \n",
      "[ 30/100] train_loss: 0.09415 valid_loss: 0.11527 test_loss: 0.09411 \n",
      "[ 31/100] train_loss: 0.09195 valid_loss: 0.10524 test_loss: 0.08979 \n",
      "Validation loss decreased (0.111769 --> 0.105238).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09225 valid_loss: 0.09933 test_loss: 0.08820 \n",
      "Validation loss decreased (0.105238 --> 0.099331).  Saving model ...\n",
      "[ 33/100] train_loss: 0.09693 valid_loss: 0.10052 test_loss: 0.08694 \n",
      "[ 34/100] train_loss: 0.09031 valid_loss: 0.10235 test_loss: 0.08579 \n",
      "[ 35/100] train_loss: 0.08624 valid_loss: 0.09971 test_loss: 0.08410 \n",
      "[ 36/100] train_loss: 0.08610 valid_loss: 0.09970 test_loss: 0.08297 \n",
      "[ 37/100] train_loss: 0.08471 valid_loss: 0.09512 test_loss: 0.08141 \n",
      "Validation loss decreased (0.099331 --> 0.095116).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08643 valid_loss: 0.09054 test_loss: 0.08122 \n",
      "Validation loss decreased (0.095116 --> 0.090541).  Saving model ...\n",
      "[ 39/100] train_loss: 0.08415 valid_loss: 0.09251 test_loss: 0.08031 \n",
      "[ 40/100] train_loss: 0.07935 valid_loss: 0.09752 test_loss: 0.07950 \n",
      "[ 41/100] train_loss: 0.08854 valid_loss: 0.09511 test_loss: 0.07810 \n",
      "[ 42/100] train_loss: 0.08211 valid_loss: 0.09354 test_loss: 0.07730 \n",
      "[ 43/100] train_loss: 0.08154 valid_loss: 0.09421 test_loss: 0.07695 \n",
      "[ 44/100] train_loss: 0.08039 valid_loss: 0.09094 test_loss: 0.07567 \n",
      "[ 45/100] train_loss: 0.08206 valid_loss: 0.09040 test_loss: 0.07492 \n",
      "Validation loss decreased (0.090541 --> 0.090399).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08720 valid_loss: 0.08823 test_loss: 0.07464 \n",
      "Validation loss decreased (0.090399 --> 0.088229).  Saving model ...\n",
      "[ 47/100] train_loss: 0.08202 valid_loss: 0.09435 test_loss: 0.07543 \n",
      "[ 48/100] train_loss: 0.08251 valid_loss: 0.09040 test_loss: 0.07373 \n",
      "[ 49/100] train_loss: 0.08025 valid_loss: 0.08541 test_loss: 0.07344 \n",
      "Validation loss decreased (0.088229 --> 0.085409).  Saving model ...\n",
      "[ 50/100] train_loss: 0.08418 valid_loss: 0.08507 test_loss: 0.07233 \n",
      "Validation loss decreased (0.085409 --> 0.085071).  Saving model ...\n",
      "[ 51/100] train_loss: 0.07925 valid_loss: 0.08931 test_loss: 0.07246 \n",
      "[ 52/100] train_loss: 0.08095 valid_loss: 0.09337 test_loss: 0.07323 \n",
      "[ 53/100] train_loss: 0.08275 valid_loss: 0.08656 test_loss: 0.07216 \n",
      "[ 54/100] train_loss: 0.08360 valid_loss: 0.08414 test_loss: 0.07187 \n",
      "Validation loss decreased (0.085071 --> 0.084142).  Saving model ...\n",
      "[ 55/100] train_loss: 0.08333 valid_loss: 0.09059 test_loss: 0.07083 \n",
      "[ 56/100] train_loss: 0.07491 valid_loss: 0.08937 test_loss: 0.07040 \n",
      "[ 57/100] train_loss: 0.07275 valid_loss: 0.08670 test_loss: 0.06964 \n",
      "[ 58/100] train_loss: 0.07433 valid_loss: 0.08291 test_loss: 0.07023 \n",
      "Validation loss decreased (0.084142 --> 0.082911).  Saving model ...\n",
      "[ 59/100] train_loss: 0.07976 valid_loss: 0.08953 test_loss: 0.07085 \n",
      "[ 60/100] train_loss: 0.07787 valid_loss: 0.09476 test_loss: 0.07178 \n",
      "[ 61/100] train_loss: 0.08459 valid_loss: 0.08401 test_loss: 0.06910 \n",
      "[ 62/100] train_loss: 0.07365 valid_loss: 0.08212 test_loss: 0.06933 \n",
      "Validation loss decreased (0.082911 --> 0.082122).  Saving model ...\n",
      "[ 63/100] train_loss: 0.07480 valid_loss: 0.09099 test_loss: 0.06922 \n",
      "[ 64/100] train_loss: 0.07559 valid_loss: 0.08906 test_loss: 0.06877 \n",
      "[ 65/100] train_loss: 0.07792 valid_loss: 0.08291 test_loss: 0.06800 \n",
      "[ 66/100] train_loss: 0.07414 valid_loss: 0.08645 test_loss: 0.06810 \n",
      "[ 67/100] train_loss: 0.07041 valid_loss: 0.08478 test_loss: 0.06762 \n",
      "[ 68/100] train_loss: 0.07245 valid_loss: 0.08092 test_loss: 0.06763 \n",
      "Validation loss decreased (0.082122 --> 0.080921).  Saving model ...\n",
      "[ 69/100] train_loss: 0.07738 valid_loss: 0.08392 test_loss: 0.06775 \n",
      "[ 70/100] train_loss: 0.07429 valid_loss: 0.09201 test_loss: 0.06888 \n",
      "[ 71/100] train_loss: 0.07274 valid_loss: 0.08320 test_loss: 0.06597 \n",
      "[ 72/100] train_loss: 0.07399 valid_loss: 0.08765 test_loss: 0.06653 \n",
      "[ 73/100] train_loss: 0.07249 valid_loss: 0.08038 test_loss: 0.06744 \n",
      "Validation loss decreased (0.080921 --> 0.080384).  Saving model ...\n",
      "[ 74/100] train_loss: 0.07511 valid_loss: 0.08182 test_loss: 0.06706 \n",
      "[ 75/100] train_loss: 0.08033 valid_loss: 0.08376 test_loss: 0.06634 \n",
      "[ 76/100] train_loss: 0.07389 valid_loss: 0.08296 test_loss: 0.06597 \n",
      "[ 77/100] train_loss: 0.07689 valid_loss: 0.08644 test_loss: 0.06603 \n",
      "[ 78/100] train_loss: 0.06848 valid_loss: 0.08592 test_loss: 0.06617 \n",
      "[ 79/100] train_loss: 0.07007 valid_loss: 0.08316 test_loss: 0.06567 \n",
      "[ 80/100] train_loss: 0.07517 valid_loss: 0.07894 test_loss: 0.06676 \n",
      "Validation loss decreased (0.080384 --> 0.078938).  Saving model ...\n",
      "[ 81/100] train_loss: 0.06625 valid_loss: 0.08324 test_loss: 0.06527 \n",
      "[ 82/100] train_loss: 0.07677 valid_loss: 0.08807 test_loss: 0.06528 \n",
      "[ 83/100] train_loss: 0.07193 valid_loss: 0.08372 test_loss: 0.06484 \n",
      "[ 84/100] train_loss: 0.07408 valid_loss: 0.08274 test_loss: 0.06490 \n",
      "[ 85/100] train_loss: 0.07428 valid_loss: 0.07844 test_loss: 0.06713 \n",
      "Validation loss decreased (0.078938 --> 0.078441).  Saving model ...\n",
      "[ 86/100] train_loss: 0.07686 valid_loss: 0.08880 test_loss: 0.06553 \n",
      "[ 87/100] train_loss: 0.07740 valid_loss: 0.08702 test_loss: 0.06520 \n",
      "[ 88/100] train_loss: 0.07356 valid_loss: 0.07750 test_loss: 0.06787 \n",
      "Validation loss decreased (0.078441 --> 0.077504).  Saving model ...\n",
      "[ 89/100] train_loss: 0.06815 valid_loss: 0.08185 test_loss: 0.06434 \n",
      "[ 90/100] train_loss: 0.07181 valid_loss: 0.09705 test_loss: 0.06804 \n",
      "[ 91/100] train_loss: 0.07063 valid_loss: 0.07991 test_loss: 0.06284 \n",
      "[ 92/100] train_loss: 0.06738 valid_loss: 0.07553 test_loss: 0.06483 \n",
      "Validation loss decreased (0.077504 --> 0.075531).  Saving model ...\n",
      "[ 93/100] train_loss: 0.06898 valid_loss: 0.08457 test_loss: 0.06428 \n",
      "[ 94/100] train_loss: 0.06376 valid_loss: 0.08353 test_loss: 0.06347 \n",
      "[ 95/100] train_loss: 0.07030 valid_loss: 0.07703 test_loss: 0.06406 \n",
      "[ 96/100] train_loss: 0.07025 valid_loss: 0.08286 test_loss: 0.06398 \n",
      "[ 97/100] train_loss: 0.06531 valid_loss: 0.09061 test_loss: 0.06509 \n",
      "[ 98/100] train_loss: 0.06296 valid_loss: 0.08934 test_loss: 0.06395 \n",
      "[ 99/100] train_loss: 0.06852 valid_loss: 0.07637 test_loss: 0.06313 \n",
      "[100/100] train_loss: 0.07296 valid_loss: 0.07741 test_loss: 0.06534 \n",
      "TRAINING MODEL 19\n",
      "[  1/100] train_loss: 0.18453 valid_loss: 0.19640 test_loss: 0.18662 \n",
      "Validation loss decreased (inf --> 0.196403).  Saving model ...\n",
      "[  2/100] train_loss: 0.18326 valid_loss: 0.19517 test_loss: 0.18530 \n",
      "Validation loss decreased (0.196403 --> 0.195175).  Saving model ...\n",
      "[  3/100] train_loss: 0.18132 valid_loss: 0.19308 test_loss: 0.18329 \n",
      "Validation loss decreased (0.195175 --> 0.193077).  Saving model ...\n",
      "[  4/100] train_loss: 0.17911 valid_loss: 0.19028 test_loss: 0.18043 \n",
      "Validation loss decreased (0.193077 --> 0.190277).  Saving model ...\n",
      "[  5/100] train_loss: 0.17697 valid_loss: 0.18719 test_loss: 0.17714 \n",
      "Validation loss decreased (0.190277 --> 0.187191).  Saving model ...\n",
      "[  6/100] train_loss: 0.17498 valid_loss: 0.18394 test_loss: 0.17351 \n",
      "Validation loss decreased (0.187191 --> 0.183941).  Saving model ...\n",
      "[  7/100] train_loss: 0.17291 valid_loss: 0.18092 test_loss: 0.17003 \n",
      "Validation loss decreased (0.183941 --> 0.180924).  Saving model ...\n",
      "[  8/100] train_loss: 0.16925 valid_loss: 0.17823 test_loss: 0.16665 \n",
      "Validation loss decreased (0.180924 --> 0.178228).  Saving model ...\n",
      "[  9/100] train_loss: 0.16707 valid_loss: 0.17570 test_loss: 0.16338 \n",
      "Validation loss decreased (0.178228 --> 0.175702).  Saving model ...\n",
      "[ 10/100] train_loss: 0.16238 valid_loss: 0.17254 test_loss: 0.16034 \n",
      "Validation loss decreased (0.175702 --> 0.172544).  Saving model ...\n",
      "[ 11/100] train_loss: 0.16037 valid_loss: 0.16891 test_loss: 0.15677 \n",
      "Validation loss decreased (0.172544 --> 0.168908).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15779 valid_loss: 0.16542 test_loss: 0.15273 \n",
      "Validation loss decreased (0.168908 --> 0.165418).  Saving model ...\n",
      "[ 13/100] train_loss: 0.15176 valid_loss: 0.16251 test_loss: 0.14883 \n",
      "Validation loss decreased (0.165418 --> 0.162512).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14731 valid_loss: 0.15816 test_loss: 0.14483 \n",
      "Validation loss decreased (0.162512 --> 0.158157).  Saving model ...\n",
      "[ 15/100] train_loss: 0.14752 valid_loss: 0.15477 test_loss: 0.14096 \n",
      "Validation loss decreased (0.158157 --> 0.154775).  Saving model ...\n",
      "[ 16/100] train_loss: 0.14128 valid_loss: 0.15164 test_loss: 0.13800 \n",
      "Validation loss decreased (0.154775 --> 0.151638).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13839 valid_loss: 0.15037 test_loss: 0.13437 \n",
      "Validation loss decreased (0.151638 --> 0.150366).  Saving model ...\n",
      "[ 18/100] train_loss: 0.13320 valid_loss: 0.14422 test_loss: 0.13065 \n",
      "Validation loss decreased (0.150366 --> 0.144216).  Saving model ...\n",
      "[ 19/100] train_loss: 0.12890 valid_loss: 0.13988 test_loss: 0.12803 \n",
      "Validation loss decreased (0.144216 --> 0.139878).  Saving model ...\n",
      "[ 20/100] train_loss: 0.12705 valid_loss: 0.13954 test_loss: 0.12377 \n",
      "Validation loss decreased (0.139878 --> 0.139544).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12292 valid_loss: 0.13403 test_loss: 0.12017 \n",
      "Validation loss decreased (0.139544 --> 0.134029).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11958 valid_loss: 0.12711 test_loss: 0.11688 \n",
      "Validation loss decreased (0.134029 --> 0.127106).  Saving model ...\n",
      "[ 23/100] train_loss: 0.11896 valid_loss: 0.12383 test_loss: 0.11232 \n",
      "Validation loss decreased (0.127106 --> 0.123833).  Saving model ...\n",
      "[ 24/100] train_loss: 0.11031 valid_loss: 0.12629 test_loss: 0.11005 \n",
      "[ 25/100] train_loss: 0.10705 valid_loss: 0.11958 test_loss: 0.10631 \n",
      "Validation loss decreased (0.123833 --> 0.119578).  Saving model ...\n",
      "[ 26/100] train_loss: 0.11008 valid_loss: 0.11434 test_loss: 0.10440 \n",
      "Validation loss decreased (0.119578 --> 0.114341).  Saving model ...\n",
      "[ 27/100] train_loss: 0.11529 valid_loss: 0.11259 test_loss: 0.10101 \n",
      "Validation loss decreased (0.114341 --> 0.112593).  Saving model ...\n",
      "[ 28/100] train_loss: 0.09998 valid_loss: 0.11690 test_loss: 0.09926 \n",
      "[ 29/100] train_loss: 0.10543 valid_loss: 0.10738 test_loss: 0.09586 \n",
      "Validation loss decreased (0.112593 --> 0.107383).  Saving model ...\n",
      "[ 30/100] train_loss: 0.09886 valid_loss: 0.10560 test_loss: 0.09407 \n",
      "Validation loss decreased (0.107383 --> 0.105599).  Saving model ...\n",
      "[ 31/100] train_loss: 0.09447 valid_loss: 0.10399 test_loss: 0.09183 \n",
      "Validation loss decreased (0.105599 --> 0.103993).  Saving model ...\n",
      "[ 32/100] train_loss: 0.09446 valid_loss: 0.10394 test_loss: 0.08994 \n",
      "Validation loss decreased (0.103993 --> 0.103944).  Saving model ...\n",
      "[ 33/100] train_loss: 0.08897 valid_loss: 0.10268 test_loss: 0.08817 \n",
      "Validation loss decreased (0.103944 --> 0.102676).  Saving model ...\n",
      "[ 34/100] train_loss: 0.08804 valid_loss: 0.09949 test_loss: 0.08605 \n",
      "Validation loss decreased (0.102676 --> 0.099493).  Saving model ...\n",
      "[ 35/100] train_loss: 0.09080 valid_loss: 0.09693 test_loss: 0.08441 \n",
      "Validation loss decreased (0.099493 --> 0.096932).  Saving model ...\n",
      "[ 36/100] train_loss: 0.09052 valid_loss: 0.10011 test_loss: 0.08371 \n",
      "[ 37/100] train_loss: 0.08237 valid_loss: 0.09469 test_loss: 0.08181 \n",
      "Validation loss decreased (0.096932 --> 0.094691).  Saving model ...\n",
      "[ 38/100] train_loss: 0.08469 valid_loss: 0.09526 test_loss: 0.08085 \n",
      "[ 39/100] train_loss: 0.08993 valid_loss: 0.09461 test_loss: 0.07989 \n",
      "Validation loss decreased (0.094691 --> 0.094607).  Saving model ...\n",
      "[ 40/100] train_loss: 0.08469 valid_loss: 0.09350 test_loss: 0.07919 \n",
      "Validation loss decreased (0.094607 --> 0.093496).  Saving model ...\n",
      "[ 41/100] train_loss: 0.08333 valid_loss: 0.09527 test_loss: 0.07858 \n",
      "[ 42/100] train_loss: 0.08602 valid_loss: 0.09112 test_loss: 0.07703 \n",
      "Validation loss decreased (0.093496 --> 0.091123).  Saving model ...\n",
      "[ 43/100] train_loss: 0.07929 valid_loss: 0.09235 test_loss: 0.07680 \n",
      "[ 44/100] train_loss: 0.08385 valid_loss: 0.09129 test_loss: 0.07624 \n",
      "[ 45/100] train_loss: 0.08695 valid_loss: 0.08818 test_loss: 0.07606 \n",
      "Validation loss decreased (0.091123 --> 0.088177).  Saving model ...\n",
      "[ 46/100] train_loss: 0.08263 valid_loss: 0.10022 test_loss: 0.07807 \n",
      "[ 47/100] train_loss: 0.09037 valid_loss: 0.08683 test_loss: 0.07445 \n",
      "Validation loss decreased (0.088177 --> 0.086834).  Saving model ...\n",
      "[ 48/100] train_loss: 0.07885 valid_loss: 0.08484 test_loss: 0.07594 \n",
      "Validation loss decreased (0.086834 --> 0.084835).  Saving model ...\n",
      "[ 49/100] train_loss: 0.08274 valid_loss: 0.08976 test_loss: 0.07357 \n",
      "[ 50/100] train_loss: 0.08148 valid_loss: 0.09562 test_loss: 0.07472 \n",
      "[ 51/100] train_loss: 0.08056 valid_loss: 0.08321 test_loss: 0.07250 \n",
      "Validation loss decreased (0.084835 --> 0.083209).  Saving model ...\n",
      "[ 52/100] train_loss: 0.08395 valid_loss: 0.08359 test_loss: 0.07355 \n",
      "[ 53/100] train_loss: 0.07895 valid_loss: 0.09117 test_loss: 0.07257 \n",
      "[ 54/100] train_loss: 0.07699 valid_loss: 0.09192 test_loss: 0.07213 \n",
      "[ 55/100] train_loss: 0.08010 valid_loss: 0.08812 test_loss: 0.07060 \n",
      "[ 56/100] train_loss: 0.07232 valid_loss: 0.08199 test_loss: 0.07012 \n",
      "Validation loss decreased (0.083209 --> 0.081986).  Saving model ...\n",
      "[ 57/100] train_loss: 0.07695 valid_loss: 0.08172 test_loss: 0.07083 \n",
      "Validation loss decreased (0.081986 --> 0.081721).  Saving model ...\n",
      "[ 58/100] train_loss: 0.07525 valid_loss: 0.08754 test_loss: 0.06982 \n",
      "[ 59/100] train_loss: 0.08423 valid_loss: 0.08718 test_loss: 0.06964 \n",
      "[ 60/100] train_loss: 0.07646 valid_loss: 0.08504 test_loss: 0.06893 \n",
      "[ 61/100] train_loss: 0.08456 valid_loss: 0.08684 test_loss: 0.06893 \n",
      "[ 62/100] train_loss: 0.07265 valid_loss: 0.08437 test_loss: 0.06845 \n",
      "[ 63/100] train_loss: 0.06748 valid_loss: 0.08171 test_loss: 0.06783 \n",
      "Validation loss decreased (0.081721 --> 0.081709).  Saving model ...\n",
      "[ 64/100] train_loss: 0.08362 valid_loss: 0.08629 test_loss: 0.06795 \n",
      "[ 65/100] train_loss: 0.07455 valid_loss: 0.09349 test_loss: 0.07045 \n",
      "[ 66/100] train_loss: 0.08108 valid_loss: 0.09094 test_loss: 0.06925 \n",
      "[ 67/100] train_loss: 0.07343 valid_loss: 0.08297 test_loss: 0.06827 \n",
      "[ 68/100] train_loss: 0.07452 valid_loss: 0.08351 test_loss: 0.06753 \n",
      "[ 69/100] train_loss: 0.07148 valid_loss: 0.08519 test_loss: 0.06716 \n",
      "[ 70/100] train_loss: 0.07734 valid_loss: 0.09020 test_loss: 0.06777 \n",
      "[ 71/100] train_loss: 0.07009 valid_loss: 0.08036 test_loss: 0.06633 \n",
      "Validation loss decreased (0.081709 --> 0.080364).  Saving model ...\n",
      "[ 72/100] train_loss: 0.07708 valid_loss: 0.08457 test_loss: 0.06641 \n",
      "[ 73/100] train_loss: 0.07009 valid_loss: 0.08680 test_loss: 0.06622 \n",
      "[ 74/100] train_loss: 0.07541 valid_loss: 0.08711 test_loss: 0.06586 \n",
      "[ 75/100] train_loss: 0.06919 valid_loss: 0.08501 test_loss: 0.06541 \n",
      "[ 76/100] train_loss: 0.06909 valid_loss: 0.08608 test_loss: 0.06528 \n",
      "[ 77/100] train_loss: 0.07240 valid_loss: 0.08642 test_loss: 0.06501 \n",
      "[ 78/100] train_loss: 0.07941 valid_loss: 0.08632 test_loss: 0.06493 \n",
      "[ 79/100] train_loss: 0.08110 valid_loss: 0.08283 test_loss: 0.06463 \n",
      "[ 80/100] train_loss: 0.07281 valid_loss: 0.08526 test_loss: 0.06434 \n",
      "[ 81/100] train_loss: 0.07564 valid_loss: 0.08943 test_loss: 0.06499 \n",
      "[ 82/100] train_loss: 0.07490 valid_loss: 0.08048 test_loss: 0.06358 \n",
      "[ 83/100] train_loss: 0.06741 valid_loss: 0.08079 test_loss: 0.06384 \n",
      "[ 84/100] train_loss: 0.06586 valid_loss: 0.08403 test_loss: 0.06389 \n",
      "[ 85/100] train_loss: 0.07283 valid_loss: 0.09000 test_loss: 0.06486 \n",
      "[ 86/100] train_loss: 0.07438 valid_loss: 0.08599 test_loss: 0.06393 \n",
      "[ 87/100] train_loss: 0.07305 valid_loss: 0.07973 test_loss: 0.06374 \n",
      "Validation loss decreased (0.080364 --> 0.079729).  Saving model ...\n",
      "[ 88/100] train_loss: 0.06996 valid_loss: 0.08956 test_loss: 0.06492 \n",
      "[ 89/100] train_loss: 0.07261 valid_loss: 0.08854 test_loss: 0.06438 \n",
      "[ 90/100] train_loss: 0.06173 valid_loss: 0.08222 test_loss: 0.06322 \n",
      "[ 91/100] train_loss: 0.07237 valid_loss: 0.08666 test_loss: 0.06342 \n",
      "[ 92/100] train_loss: 0.06795 valid_loss: 0.08761 test_loss: 0.06338 \n",
      "[ 93/100] train_loss: 0.06525 valid_loss: 0.07967 test_loss: 0.06227 \n",
      "Validation loss decreased (0.079729 --> 0.079667).  Saving model ...\n",
      "[ 94/100] train_loss: 0.06106 valid_loss: 0.08855 test_loss: 0.06325 \n",
      "[ 95/100] train_loss: 0.06603 valid_loss: 0.08343 test_loss: 0.06255 \n",
      "[ 96/100] train_loss: 0.07268 valid_loss: 0.07898 test_loss: 0.06263 \n",
      "Validation loss decreased (0.079667 --> 0.078985).  Saving model ...\n",
      "[ 97/100] train_loss: 0.06590 valid_loss: 0.08250 test_loss: 0.06230 \n",
      "[ 98/100] train_loss: 0.07443 valid_loss: 0.09547 test_loss: 0.06481 \n",
      "[ 99/100] train_loss: 0.06343 valid_loss: 0.08155 test_loss: 0.06142 \n",
      "[100/100] train_loss: 0.07000 valid_loss: 0.08482 test_loss: 0.06177 \n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "n_epochs = Epochs\n",
    "\n",
    "train_loader = dl_train_unseen\n",
    "valid_loader = dl_valid_unseen\n",
    "test_loader = dl_test_unseen\n",
    "\n",
    "#i = 0\n",
    "for i in range(jb):\n",
    "    print('TRAINING MODEL %d' %i)\n",
    "    # Instantiate the model\n",
    "    model = TTRNet(1,4,32).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1.E-5)\n",
    "    criterion = FocalLoss(pos_weight=torch.tensor([1.23,1.5,1,1]), gamma=2)\n",
    "    fn = './unseen/UKDALE_unseen_%d.pth' %i\n",
    "    model, train_loss, valid_loss, test_loss = train_model(model, batch_size, n_epochs, fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYrzbsFJEThv"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(valid_loss)\n",
    "plt.plot(test_loss)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "MD1z2vUwyZIH",
    "outputId": "42c5af30-c246-4805-8ce7-2a7fbe79176c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TTRNet(\n",
       "  (embed): DataEmbedding(\n",
       "    (value_embedding): TokenEmbedding(\n",
       "      (tokenConv): Conv1d(1, 256, kernel_size=(3,), stride=(1,), padding=(1,), padding_mode=circular)\n",
       "    )\n",
       "    (position_embedding): PositionalEmbedding()\n",
       "    (temporal_embedding): TemporalEmbedding(\n",
       "      (hour_embed): FixedEmbedding(\n",
       "        (emb): Embedding(24, 256)\n",
       "      )\n",
       "      (weekday_embed): FixedEmbedding(\n",
       "        (emb): Embedding(7, 256)\n",
       "      )\n",
       "      (day_embed): FixedEmbedding(\n",
       "        (emb): Embedding(32, 256)\n",
       "      )\n",
       "      (month_embed): FixedEmbedding(\n",
       "        (emb): Embedding(13, 256)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer1): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer2): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer3): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder_layer4): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool1): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(5,), stride=(5,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool2): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(10,), stride=(10,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool3): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(20,), stride=(20,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (tpool4): TemporalPooling(\n",
       "    (pool): AvgPool1d(kernel_size=(30,), stride=(30,), padding=(0,))\n",
       "    (conv): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder1): Encoder(\n",
       "    (conv): Conv1d(1, 32, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder2): Encoder(\n",
       "    (conv): Conv1d(32, 64, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder3): Encoder(\n",
       "    (conv): Conv1d(64, 128, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (encoder4): Encoder(\n",
       "    (conv): Conv1d(128, 256, kernel_size=(3,), stride=(1,), bias=False)\n",
       "    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv): ConvTranspose1d(512, 32, kernel_size=(8,), stride=(8,), bias=False)\n",
       "    (bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (activation): Conv1d(32, 4, kernel_size=(1,), stride=(1,))\n",
       "  (re): RethinkNet(\n",
       "    (input_layer): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (rnn): LSTM(128, 128)\n",
       "    (dec): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TTRNet(1,4,32).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "HuXotkW1yd-b",
    "outputId": "49ef336e-dab6-470f-ab7c-7ed83a4c4162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./seen/UKDALE_seen_0.pth\n",
      "./seen/UKDALE_seen_1.pth\n",
      "./seen/UKDALE_seen_2.pth\n",
      "./seen/UKDALE_seen_3.pth\n",
      "./seen/UKDALE_seen_4.pth\n",
      "./seen/UKDALE_seen_5.pth\n",
      "./seen/UKDALE_seen_6.pth\n",
      "./seen/UKDALE_seen_7.pth\n",
      "./seen/UKDALE_seen_8.pth\n",
      "./seen/UKDALE_seen_9.pth\n",
      "./seen/UKDALE_seen_10.pth\n",
      "./seen/UKDALE_seen_11.pth\n",
      "./seen/UKDALE_seen_12.pth\n",
      "./seen/UKDALE_seen_13.pth\n",
      "./seen/UKDALE_seen_14.pth\n",
      "./seen/UKDALE_seen_15.pth\n",
      "./seen/UKDALE_seen_16.pth\n",
      "./seen/UKDALE_seen_17.pth\n",
      "./seen/UKDALE_seen_18.pth\n",
      "./seen/UKDALE_seen_19.pth\n",
      "\n",
      "AHU0\n",
      "F1 score  : 0.892 (0.817, 0.944)\n",
      "Precision : 0.986 (0.964, 1.000)\n",
      "Recall    : 0.819 (0.691, 0.912)\n",
      "Accuracy  : 0.938 (0.901, 0.965)\n",
      "MCC       : 0.859 (0.777, 0.920)\n",
      "MAE       : 185.192 (121.699, 270.676)\n",
      "SAE       : -0.148 (-0.292, -0.037)\n",
      "\n",
      "AHU1\n",
      "F1 score  : 0.951 (0.865, 0.996)\n",
      "Precision : 0.978 (0.884, 1.000)\n",
      "Recall    : 0.928 (0.836, 0.996)\n",
      "Accuracy  : 0.979 (0.948, 0.998)\n",
      "MCC       : 0.940 (0.845, 0.995)\n",
      "MAE       : 142.766 (108.088, 220.303)\n",
      "SAE       : -0.059 (-0.174, 0.071)\n",
      "\n",
      "AHU2\n",
      "F1 score  : 0.992 (0.991, 0.993)\n",
      "Precision : 0.998 (0.997, 0.999)\n",
      "Recall    : 0.986 (0.983, 0.989)\n",
      "Accuracy  : 0.994 (0.993, 0.995)\n",
      "MCC       : 0.988 (0.985, 0.990)\n",
      "MAE       : 80.417 (77.928, 83.348)\n",
      "SAE       : -0.057 (-0.061, -0.053)\n",
      "\n",
      "AHU5\n",
      "F1 score  : 0.991 (0.983, 0.994)\n",
      "Precision : 0.990 (0.973, 0.995)\n",
      "Recall    : 0.992 (0.987, 0.994)\n",
      "Accuracy  : 0.994 (0.988, 0.996)\n",
      "MCC       : 0.986 (0.974, 0.991)\n",
      "MAE       : 380.095 (365.314, 438.533)\n",
      "SAE       : 0.070 (0.060, 0.090)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(4):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.6\n",
    "for i in range(jb):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_seen_%d.pth' %i\n",
    "    filename = './seen/UKDALE_seen_%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(4):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[0], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        #pm = (ds_appliance[0][APPLIANCE[a]] * \n",
    "        #      ds_status[0][APPLIANCE[a]]).sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        pm = ds_appliance[0][APPLIANCE[a]].sum() / ds_status[0][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_test[0], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "np.save('seen.npy', scores) \n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "colab_type": "code",
    "id": "oYjnWxnQyURS",
    "outputId": "1f4f491b-4271-426e-ee56-c739248a2a9d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./unseen/UKDALE_unseen_0.pth\n",
      "./unseen/UKDALE_unseen_1.pth\n",
      "./unseen/UKDALE_unseen_2.pth\n",
      "./unseen/UKDALE_unseen_3.pth\n",
      "./unseen/UKDALE_unseen_4.pth\n",
      "./unseen/UKDALE_unseen_5.pth\n",
      "./unseen/UKDALE_unseen_6.pth\n",
      "./unseen/UKDALE_unseen_7.pth\n",
      "./unseen/UKDALE_unseen_8.pth\n",
      "./unseen/UKDALE_unseen_9.pth\n",
      "./unseen/UKDALE_unseen_10.pth\n",
      "./unseen/UKDALE_unseen_11.pth\n",
      "./unseen/UKDALE_unseen_12.pth\n",
      "./unseen/UKDALE_unseen_13.pth\n",
      "./unseen/UKDALE_unseen_14.pth\n",
      "./unseen/UKDALE_unseen_15.pth\n",
      "./unseen/UKDALE_unseen_16.pth\n",
      "./unseen/UKDALE_unseen_17.pth\n",
      "./unseen/UKDALE_unseen_18.pth\n",
      "./unseen/UKDALE_unseen_19.pth\n",
      "\n",
      "AHU0\n",
      "F1 score  : 0.864 (0.854, 0.874)\n",
      "Precision : 0.910 (0.877, 0.926)\n",
      "Recall    : 0.823 (0.791, 0.862)\n",
      "Accuracy  : 0.883 (0.877, 0.889)\n",
      "MCC       : 0.765 (0.754, 0.776)\n",
      "MAE       : 359.177 (345.369, 371.955)\n",
      "SAE       : -0.095 (-0.148, -0.015)\n",
      "\n",
      "AHU1\n",
      "F1 score  : 0.827 (0.812, 0.843)\n",
      "Precision : 0.871 (0.829, 0.916)\n",
      "Recall    : 0.788 (0.743, 0.829)\n",
      "Accuracy  : 0.867 (0.857, 0.880)\n",
      "MCC       : 0.722 (0.701, 0.749)\n",
      "MAE       : 515.570 (470.778, 551.563)\n",
      "SAE       : -0.094 (-0.169, 0.001)\n",
      "\n",
      "AHU2\n",
      "F1 score  : 0.940 (0.938, 0.942)\n",
      "Precision : 0.955 (0.951, 0.960)\n",
      "Recall    : 0.926 (0.917, 0.931)\n",
      "Accuracy  : 0.941 (0.940, 0.943)\n",
      "MCC       : 0.883 (0.880, 0.887)\n",
      "MAE       : 266.097 (260.464, 271.350)\n",
      "SAE       : -0.030 (-0.045, -0.021)\n",
      "\n",
      "AHU5\n",
      "F1 score  : 0.983 (0.982, 0.985)\n",
      "Precision : 0.989 (0.983, 0.995)\n",
      "Recall    : 0.978 (0.971, 0.985)\n",
      "Accuracy  : 0.984 (0.982, 0.985)\n",
      "MCC       : 0.967 (0.964, 0.971)\n",
      "MAE       : 592.146 (577.865, 617.021)\n",
      "SAE       : -0.012 (-0.024, -0.001)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for a in range(4):\n",
    "    scores[a] = {}\n",
    "    scores[a]['F1'] = []\n",
    "    scores[a]['Precision'] = []\n",
    "    scores[a]['Recall'] = []\n",
    "    scores[a]['Accuracy'] = []\n",
    "    scores[a]['MCC'] = []\n",
    "    scores[a]['MAE'] = []\n",
    "    scores[a]['SAE'] = []\n",
    "\n",
    "thr = 0.6\n",
    "\n",
    "for i in range(jb):\n",
    "    #filename = '/content/gdrive/My Drive/NILM/UKDALE_unseen_%d.pth' %i\n",
    "    filename = './unseen/UKDALE_unseen_%d.pth' %i\n",
    "    print(filename)\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    for a in range(4):\n",
    "        #x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        #pm = p_true.sum() / s_true.sum()\n",
    "        pm = ds_appliance[1][APPLIANCE[a]].sum() / ds_status[1][APPLIANCE[a]].sum() / MAX_POWER\n",
    "        x_true, p_true, s_true, s_hat = evaluate_activation(model, dl_house_total[1], a)\n",
    "        s_hat = get_status(s_hat, thr, MIN_OFF[a], MIN_ON[a])\n",
    "        p_hat = pm * s_hat\n",
    "        scores[a]['F1'].append(f1_score(s_true, s_hat))\n",
    "        scores[a]['Precision'].append(precision_score(s_true, s_hat))\n",
    "        scores[a]['Recall'].append(recall_score(s_true, s_hat))\n",
    "        scores[a]['Accuracy'].append(accuracy_score(s_true, s_hat))\n",
    "        scores[a]['MCC'].append(matthews_corrcoef(s_true, s_hat))\n",
    "        scores[a]['MAE'].append(mean_absolute_error(p_true, p_hat)*MAX_POWER)\n",
    "        scores[a]['SAE'].append((p_hat.sum() - p_true.sum()) / p_true.sum())\n",
    "np.save('unseen.npy', scores) \n",
    "for i,a in enumerate(APPLIANCE):\n",
    "    print()\n",
    "    print(a)\n",
    "    print('F1 score  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['F1']), sorted(scores[i]['F1'])[1], sorted(scores[i]['F1'])[18]))\n",
    "    print('Precision : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Precision']), sorted(scores[i]['Precision'])[1], sorted(scores[i]['Precision'])[18]))\n",
    "    print('Recall    : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Recall']), sorted(scores[i]['Recall'])[1], sorted(scores[i]['Recall'])[18]))\n",
    "    print('Accuracy  : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['Accuracy']), sorted(scores[i]['Accuracy'])[1], sorted(scores[i]['Accuracy'])[18]))\n",
    "    print('MCC       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MCC']), sorted(scores[i]['MCC'])[1], sorted(scores[i]['MCC'])[18]))\n",
    "    print('MAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['MAE']), sorted(scores[i]['MAE'])[1], sorted(scores[i]['MAE'])[18]))\n",
    "    print('SAE       : %.3f (%.3f, %.3f)' %(np.mean(scores[i]['SAE']), sorted(scores[i]['SAE'])[1], sorted(scores[i]['SAE'])[18]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TPNILM_UKDALE_run.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
